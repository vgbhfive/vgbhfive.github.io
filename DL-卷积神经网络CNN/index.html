<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="G-QBK8PCQC9B">
  <meta name="baidu-site-verification" content="codeva-K7aZhBcBPm">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.vgbhfive.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="简介在之前的文章中主要介绍了神经网络的基本原理、 TensorFlow 使用和基本的全连接层模型，而本节正如题目所言：卷积神经网络。">
<meta property="og:type" content="article">
<meta property="og:title" content="DL-卷积神经网络CNN">
<meta property="og:url" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/index.html">
<meta property="og:site_name" content="Vgbhfive&#39;s Blog">
<meta property="og:description" content="简介在之前的文章中主要介绍了神经网络的基本原理、 TensorFlow 使用和基本的全连接层模型，而本节正如题目所言：卷积神经网络。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/warning.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/jubu.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_1.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_2.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_3.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_4.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_5.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_6.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/buchang.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/tianchong.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/tidu.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/max_pooling.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/dildated.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/deliver.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/alexnet_network.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/vgg_network.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/goo_ince_network.jpg">
<meta property="og:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/goo_network.jpg">
<meta property="article:published_time" content="2024-06-06T14:16:04.000Z">
<meta property="article:modified_time" content="2024-06-19T15:39:36.788Z">
<meta property="article:author" content="vgbhfive">
<meta property="article:tag" content="DL">
<meta property="article:tag" content="TensorFlow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/warning.jpg">

<link rel="canonical" href="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>DL-卷积神经网络CNN | Vgbhfive's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Vgbhfive's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Vgbhfive's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-pictures">

    <a href="/pictures/" rel="section"><i class="fa fa-th fa-fw"></i>Pictures</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/vgbhfive" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.vgbhfive.com/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
      <meta itemprop="name" content="vgbhfive">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vgbhfive's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DL-卷积神经网络CNN
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-06 22:16:04" itemprop="dateCreated datePublished" datetime="2024-06-06T22:16:04+08:00">2024-06-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-06-19 23:39:36" itemprop="dateModified" datetime="2024-06-19T23:39:36+08:00">2024-06-19</time>
              </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在之前的文章中主要介绍了<strong>神经网络的基本原理</strong>、 <strong><code>TensorFlow</code> 使用</strong>和<strong>基本的全连接层模型</strong>，而本节正如题目所言：<strong>卷积神经网络</strong>。</p>
<span id="more"></span>

<h4 id="全连接层的问题"><a href="#全连接层的问题" class="headerlink" title="全连接层的问题"></a>全连接层的问题</h4><p>首先分析下全连接层的问题，通过 <code>TensorFlow</code> 快速搭建一个网络模型，添加四个 <code>Dense</code> 层，使用 <code>Sequential</code> 容器封装为一个网络对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">10</span>),</span><br><span class="line">])</span><br><span class="line">model.build(input_shape=[<span class="number">4</span>, <span class="number">784</span>])</span><br><span class="line">model.summary() <span class="comment"># 335114 个参数</span></span><br></pre></td></tr></table></figure>

<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/warning.jpg" alt="warning"></p>
<p>在该网络模型中，对于输入节点数为 <code>n</code>，输出节点数为 <code>m</code>，则对于每一次的全连接层来说计算的参数量为 <code>n * m + m</code>。以同样的方式计算剩余层的参数量，则最终的总参数量大约为 <code>34</code> 万个。而在实际中，网络的计算过程中还需要<em>缓存计算图模型</em>、<em>梯度信息</em>、<em>输入和输出中间结果</em>等，其中关于梯度计算等占用资源非常多！</p>
<hr>

<h3 id="基础原理"><a href="#基础原理" class="headerlink" title="基础原理"></a>基础原理</h3><h4 id="局部相关性"><a href="#局部相关性" class="headerlink" title="局部相关性"></a>局部相关性</h4><p>在实际应用中存在着大量以位置或距离作为重要性分布衡量标准的数据。以 <code>2D</code> 图片为例，实心网格线中心所在的像素点为参考点，其周边欧氏距离小于或等于 <code>k/2^(1/2)</code> 的像素点以矩阵网格展示，其网格线内的像素点重要性较高，而外部的像素点重要性较低。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/jubu.jpg" alt="jubu"></p>
<p>这种以距离的重要性分布假设特性被称为<strong>局部相关性</strong>，其仅关心距离自己较近部分的节点，而忽略较远部分的节点。</p>
<h4 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h4><p>在卷积神经网络（<code>CNN</code>）中，<strong>权值共享</strong>是指在网络的不同位置使用相同的权重参数。这种权值共享的机制使得 <code>CNN</code> 能够有效地处理图像等具有局部相关性的数据。通过权值共享，网络可以学习到不同位置的特征，而不需要为每个位置都学习独立的参数，从而减少需要学习的参数数量，提高网络的参数效率。</p>
<h4 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h4><p>在 <code>CNN</code> 的卷积层中<strong>卷积核（filter）</strong>通过<strong>滑动窗口</strong>的方式在输入数据上进行<strong>卷积</strong>操作。如果使用权值共享，那么无论卷积核在输入数据的哪个位置进行卷积，都会使用相同的卷积核参数。这样，在不同位置学习到的特征将是共享的，而不同位置之间的参数是共享的。这种共享使得CNN能够更好地捕捉到输入数据的局部特征，从而提高了网络的性能和泛化能力。</p>
<p><small><em>离散卷积</em>操作是指简化的“局部连接层”，对于 <code>k * k</code> 范围内的所有像素，采用权值累乘相加的方式提取信息，每个输出节点表示对应感受野区域的特征信息。</small></p>
<h4 id="输入和卷积核"><a href="#输入和卷积核" class="headerlink" title="输入和卷积核"></a>输入和卷积核</h4><p>卷积神经网络通过充分利用<strong>局部相关性</strong>和<strong>权值共享</strong>思想，极大地减少了网络的参数量，从以提高训练效率，以此推动实现更大的深层网络。</p>
<p>下面主要介绍卷积神经网络层的具体计算流程。</p>
<p><small>以 <code>2D</code> 图片为例，卷积层输入高、宽分别用 <code>h, w</code> 来表示，通道数为 $c_{in}$ 的输入特征图 <code>X</code>，在 $c_{out}$ 个高、宽均为 <code>k</code>，通道数为 $c_{in}$ 的卷积核作用下，生成高、宽分别为 $h^*, w^*$，通道数为 $c_{out}$ 的特征图输出。</small></p>
<h5 id="单通道输入和单卷积核"><a href="#单通道输入和单卷积核" class="headerlink" title="单通道输入和单卷积核"></a>单通道输入和单卷积核</h5><p>首先讨论单通道输入 $c_{in}&#x3D;1$，单个卷积核 $c_{out}&#x3D;1$ 的情况。以输入 <code>X</code> 为 <code>5*5</code> 的矩阵，卷积核为 <code>3*3</code> 的矩阵为例，如下图所示：</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_1.jpg" alt="juanji_1"></p>
<p>与卷积核相同大小的感受野首先移动到输入 <code>X</code> 的最左上方，选中输入 <code>X</code> 中 <code>3*3</code> 的感受野区域，与卷积核对应元素<strong>相乘</strong>，得到运算结果 <code>3*3</code> 的矩阵后，该矩阵中的所有元素全部<strong>相加</strong>，然后写入输出矩阵第一行、第一列的位置上。</p>
<p>$$ -1 - 1 + 0 - 1 + 2 + 6 + 0 - 2 + 4 &#x3D; 7$$</p>
<p>在完成第一个感受野区域计算后，感受野窗口向右移动一个<strong>步长单位（<code>Strides</code>）</strong>，即可得到下一个感受野，按照相同的计算方式与卷积核相乘累加写入输出矩阵第一行、第二个位置。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_2.jpg" alt="juanji_2"></p>
<p>按照上述方法每次感受野区域向右移动一个步长单位，若超出边界时，则向下移动一个步长单位并回到行首，直至感受野区域移动到最右边、最下边的位置。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_3.jpg" alt="juanji_3"></p>
<p>至此就完成了单通道输入、单个卷积核的运算流程。</p>
<h5 id="多通道输入和单卷积核"><a href="#多通道输入和单卷积核" class="headerlink" title="多通道输入和单卷积核"></a>多通道输入和单卷积核</h5><p>彩色图片的多通道输入的卷积层较为常见，其包含 <code>R/G/B</code> 三个通道，每个通道上的数据分别表示 <code>RGB</code> 色彩的强度。</p>
<p>在多通道输入的情况下，卷积核的通道数将与输入 <code>X</code> 的通道数量一致，卷积核的第 <code>i</code> 个通道和输入 <code>X</code> 的第 <code>i</code> 个通道相互运算，由此得到第 <code>i</code> 个中间矩阵，此时就可以视为单通道输入与单卷积核的情况，所有通道的中间矩阵对应值再次相加即可作为最终输出。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_4.jpg" alt="juanji_4"></p>
<p>初始状态下每个通道的感受野区域同步在对应通道的最左边、最上边位置，每个通道上感受野区域元素与卷积核对应通道上的矩阵相乘累加，分别得到三个通道上的输出，再将中间变量相加得到最终值，写入输出矩阵的第一行、第一列的位置。</p>
<p>感受野区域同步在输入 <code>X</code> 的通道上向右移动一个步长单位，再次计算感受野区域与卷积核对应通道上的矩阵相乘<strong>累加</strong>，得到中间变量后再全部相加得到最终值，并写入对应位置。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_5.jpg" alt="juanji_5"></p>
<p>依次按照上述方式完成所有的计算，直至移动到输入 <code>X</code> 最下边、最右边的感受野区域，此时就完成了全部的卷积运算，得到一个 <code>3*3</code> 的输出矩阵。</p>
<h5 id="多通道输入和多卷积核"><a href="#多通道输入和多卷积核" class="headerlink" title="多通道输入和多卷积核"></a>多通道输入和多卷积核</h5><p>一般情况下一个卷积核只能完成某种逻辑的特征提取，而当需要同时提取多种特征时，则可以通过增加多个卷积核来得到多种特征，以提高神经网络的表达能力，这就是<strong>多通道输入、多卷积核</strong>的情况。</p>
<p>下面以三通道输入、两卷积核的卷积层为例。第一个卷积核与输入 <code>X</code> 运算得到输出 <code>O</code> 的第一个通道，第二个卷积核与输入 <code>X</code> 运算得到输出 <code>O</code> 的第二个通道，多个输出的通道拼接在一起后得到最终的输出 <code>O</code>。每个卷积核大小 <code>k</code>、步长 <code>s</code>、填充设定等都必须一致，这样最终才能保证输出通道的统一，从而满足拼接的条件。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/juanji_6.jpg" alt="juanji_6"></p>
<h4 id="步长和填充"><a href="#步长和填充" class="headerlink" title="步长和填充"></a>步长和填充</h4><h5 id="步长"><a href="#步长" class="headerlink" title="步长"></a>步长</h5><p>感受野区域密度的控制一般是通过<strong>移动步长（<code>Strides</code>）</strong>实现。而步长是指感受野窗口每次移动的长度单位，对于 <code>2D</code> 图片来说，分为向右和向下方向的移动长度。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/buchang.jpg" alt="buchang"></p>
<p>通过设定步长 <code>s</code>，可以有效地控制信息密度的提取，当步长较小时感受野以较小幅度移动窗口，可以提取更多的特征信息，由此输出的张量尺寸也更大；当步长较大时感受野以较大幅度移动窗口，减少了计算代价，过滤冗余信息，输出的张量尺寸也更小。</p>
<h5 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h5><p>在经过卷积运算之后的输出 <code>O</code> 的高宽一般都会小于输入 <code>X</code> 的高宽，即使步长 <code>s=1</code> 其输出 <code>O</code> 的高宽也会略小于输入 <code>X</code> 的高宽。因此为了输出 <code>O</code> 的高宽能与输入 <code>X</code> 的高宽相等，可以通过在原输入 <code>X</code> 的高宽维度上进行填充（<code>Padding</code>）若干个无效元素，以此得到增大的输入 $X^*$。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/tianchong.jpg" alt="tianchong"></p>
<p>在高&#x2F;行的上（<code>Top</code>）、下（<code>Bottom</code>）方向，宽&#x2F;列的左（<code>Left</code>）、右（<code>Right</code>）方向均可以进行不定数量的填充操作，填充的值默认为 <code>0</code>，不过也可以填充自定义值。</p>
<hr>

<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><h4 id="卷积层实现"><a href="#卷积层实现" class="headerlink" title="卷积层实现"></a>卷积层实现</h4><p>在 <code>TensorFlow</code> 中可以通过<strong>自定义权值</strong>的底层实现方式搭建神经网络，也可以直接调用现成的<strong>卷积层类</strong>的高层实现方式快速搭建神经网络。</p>
<h5 id="自定义权值"><a href="#自定义权值" class="headerlink" title="自定义权值"></a>自定义权值</h5><p>在 <code>TensorFlow</code> 中通过 <code>tf.nn.conv2d()</code> 函数可以实现 <code>2D</code> 卷积运算。<code>tf.nn.conv2d()</code> 基于输入 $x:[b,h,w,c_{in}]$ 和卷积核 $w:[k,k,c_{in},c_{out}]$ 进行卷积运算，得到输出 $o:[b,h^*,w^*,c_{out}]$，其中 $c_{in}$ 表示输入通道数，$c_{out}$ 表示卷积核数量，也是特征图的输出通道数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>]) <span class="comment"># 模拟输入，高宽为5，通道数为3</span></span><br><span class="line">w = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># 根据 [k, k, cin, cout] 格式创建 W 张量，4个3*3大小的卷积核</span></span><br><span class="line">out = tf.nn.conv2d(x, w, strides=<span class="number">1</span>, padding=[[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">out.shape <span class="comment"># 输出张量的shape</span></span><br></pre></td></tr></table></figure>

<p><small>其中 <code>padding</code> 的参数格式为：<code>padding=[[0,0],[上,下],[左,右],[0,0]]</code></small></p>
<p>特别地通过设置 <code>padding=&#39;SAME&#39;, strides=1</code> 可以得到输入、输出同样大小的卷积层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>]) <span class="comment"># 模拟输入，高宽为5，通道数为3</span></span><br><span class="line">w = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># 根据 [k, k, cin, cout] 格式创建 W 张量，4个3*3大小的卷积核</span></span><br><span class="line">out = tf.nn.conv2d(x, w, strides=<span class="number">3</span>, padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">out.shape <span class="comment"># 输出张量的shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当 strides&gt;1，padding=&#x27;SAME&#x27; 时，输出的卷积层高宽将缩小 1/strides 倍</span></span><br></pre></td></tr></table></figure>

<p>卷积层和全连接层一样均可以设置偏置向量，但 <code>tf.nn.conv2d()</code> 函数没有实现偏置向量计算，不过可以手动添加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = tf.zeros(<span class="number">4</span>)</span><br><span class="line">out = out + b</span><br><span class="line">out</span><br></pre></td></tr></table></figure>

<h5 id="卷积层类"><a href="#卷积层类" class="headerlink" title="卷积层类"></a>卷积层类</h5><p>通过卷积层类 <code>layers.Conv2D</code> 则可以不需要手动定义卷积核 <code>W</code> 和偏置张量 <code>B</code>，直接调用类实例即可完成卷积层的前向计算。</p>
<p>在新建卷积层类时，只需要指定<strong>卷积核数量参数 <code>filters</code><strong>、</strong>卷积核大小 <code>kernel_size</code><strong>、</strong>步长 <code>strides</code><strong>、</strong>填充 <code>padding</code></strong> 等即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer = layers.Conv2D(<span class="number">4</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">3</span>, padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果遇到高宽不等，步长行列方向不一致，kernel_size 参数设计为 (k_h,k_w)，strides 参数设计为 (s_h,s_w)</span></span><br><span class="line">layer_tmp = layers.Conv2D(<span class="number">4</span>, kernel_size=(<span class="number">3</span>,<span class="number">4</span>), strides=(<span class="number">2</span>,<span class="number">1</span>), padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在创建完成后，通过调用实例 <code>__call__()</code> 函数即可完成前向计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>]) <span class="comment"># 模拟输入，高宽为5，通道数为3</span></span><br><span class="line">out = layer(x)</span><br><span class="line">out.shape, layer.trainable_variable <span class="comment"># 返回输出张量的 shape 和所有待优化张量列表</span></span><br></pre></td></tr></table></figure>

<h4 id="梯度传播"><a href="#梯度传播" class="headerlink" title="梯度传播"></a>梯度传播</h4><p>前面了解了如何通过感受野区域移动的方式实现离散卷积计算，那这其中的梯度又是如何计算的呢？</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/tidu.jpg" alt="tidu"></p>
<p>可以得出输出张量 <code>O</code> 的表达式：<br>$$ o_{00} &#x3D; x_{00}w_{00} + x_{01}w_{01} + x_{10}w_{10}+ x_{11}w_{11} + b $$</p>
<p>以计算 $w_{00}$ 为例，通过<strong>链式法则</strong>分解：<br>$$ {\frac {\delta L} {\delta_{w_{00}}}} &#x3D; sum_{i \in [00, 01, 10, 11]} {\frac {\delta L} {\delta_{o_i}}} {\frac {\delta_{o_i}} {\delta_{w_{00}}}} $$</p>
<p>其中 ${\frac {\delta L} {\delta_{o_i}}}$ 可以由误差函数推导而得，则 ${\frac {\delta_{o_i}} {\delta_{w_{00}}}}$：<br>$$ {\frac {\delta_{o_{00}}} {\delta_{w_{00}}}} &#x3D; {\frac {\delta (x_{00}w_{00} + x_{01}w_{01} + x_{10}w_{10}+ x_{11}w_{11} + b)} {w_{00}}} &#x3D; x_{00} $$</p>
<p>由上述可以看出通过循环移动感受野区域的方式并没有改变网络层的可导性，同时梯度的计算也并不复杂，只是当网络层数增加后人工推导梯度将很难进行。</p>
<h4 id="卷积表示"><a href="#卷积表示" class="headerlink" title="卷积表示"></a>卷积表示</h4><hr>

<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>卷积层可以通过 <code>strides</code> 的大小来动态调整特征图的高宽成倍缩小，从而降低参数量。除此之外还有一种专门的网络层可以实现尺寸缩减功能，那就是<strong>池化层（<code>Pooling Layer</code>）</strong>。</p>
<p>池化层同样基于<strong>局部相关性</strong>的思想，通过从局部相关的一组元素中进行采样或信息聚合，从而得到新的元素值。目前共有两种类型：</p>
<ul>
<li><strong>最大池化层（<code>Max Pooling</code>）</strong>：从相关元素集中提取最大的一个元素值。</li>
<li><strong>平均池化层（<code>Average Pooling</code>）</strong>：从相关元素集中计算平均值并返回。</li>
</ul>
<p>以 <code>5*5</code> 的输入 <code>X</code> 的最大池化层为例，其中感受野窗口大小 <code>k=2</code>，步长 <code>s=1</code>。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/max_pooling.jpg" alt="max_pooling"></p>
<p>取第一个感受野区域元素集合 <code>[1,-1,-1,-2]</code>，其中的最大值为 <code>1</code>，写入对应的位置。之后的以此类推，可得到最终的输出 <code>O</code>。</p>
<hr>

<h3 id="BN-层"><a href="#BN-层" class="headerlink" title="BN 层"></a><code>BN</code> 层</h3><p>卷积神经网络的出现使得网络参数量大大减低，使得几十层的神经网络成为现实，然而在实际中网络训练中，网络层数的增加使得训练变得很不稳定，有时会出现长时间不收敛的情况，同时网络对于超参数的轻微变会非常敏感。</p>
<p>为解决上述问题就提出来一种<strong>参数标准化（<code>Normalize</code>）</strong>的方法，基于参数标准化设计了 <strong><code>Batch Normalization (BN)</code> 层</strong>。<code>BN</code> 层使得网络的超参数设定更加自由，其被广泛应用于各种深度网络模型中，卷积层、<code>BN</code> 层、<code>ReLU</code> 层、池化层等成为网络模型的标准基础模块，通过堆叠 <code>Conv-BN-ReLU-Pooling</code> 方式可以获得不错的模型性能。</p>
<p>说到这里那就看看 <code>BN</code> 层进行参数标准化后有哪些变化？<br>在之前了解过 <code>Sigmoid</code> 激活函数和其梯度分布，当输入值 <code>x</code> 无限大时，其导数就会变得很小，趋近于 <code>0</code>，此时就会出现<strong>梯度弥散现象</strong>。而为了避免 <code>Sigmoid</code> 函数出现梯度弥散现象，就需要将函数输入 <code>x</code> 映射到 <code>0</code> 附近的一段较小区间内，通过标准化重映射后此处的导数不至于过小，而从减少<em>梯度弥散现象</em>。</p>
<p>举个例子，现在有两个输入参数的线性模型：<br>$$ L &#x3D; a &#x3D; w_1 * x_1 + w_2 * x_2 + b $$</p>
<p>考虑不同输入分布的优化情况：</p>
<ul>
<li>$x_1 \in [1,10], x_2 \in [1,10]$</li>
<li>$x_1 \in [1,10], x_2 \in [100,1000]$</li>
</ul>
<p>根据求导公式可得：<br>$$ {\frac {\delta L} {\delta w_1}} &#x3D; x_1 $$<br>$$ {\frac {\delta L} {\delta w_2}} &#x3D; x_2 $$</p>
<p>当 $x_1, x_2$ 的输入数据范围大致相当时，其偏导数基本一致；而当 $x_1, x_2$ 的输入数据范围相差较大时，例如 $x_1 &lt;&lt; x_2$ 时，则会出现 ${\frac {\delta L} {\delta w_1}} &lt;&lt; {\frac {\delta L} {\delta w_2}}$，此时 $w_2$ 的更新会出现<em>梯度弥散现象</em>。</p>
<p>那数据标准化操作如何实现？将数据 <code>x</code> 映射到 $\hat x$：<br>$$ \hat x &#x3D; {\frac {x - \mu_r} {\sqrt {\sigma_{r^2} + \epsilon}}} $$</p>
<p><small>$\mu_r, \sigma_{r^2}$ 来自统计的数据的均值和方差，$\epsilon$ 是为了防止出现除 <code>0</code> 错误而设置的较小数字。</small></p>
<p>在基于 <code>Batch</code> 的训练阶段，计算 <code>Batch</code> 内部的均值 $\mu_B$ 和方差 $\sigma_{B^2}$，其可以被视为近似于 $\mu_r, \sigma_{r^2}$：<br>$$ \mu_B &#x3D; {\frac {1} {m}} sum_{i&#x3D;1}^m x_i $$<br>$$ \sigma_{B^2} &#x3D; {\frac {1} {m}} sum_{i&#x3D;1}^m (x_i - \mu_B)^2 $$</p>
<h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p><code>BN</code> 层的输入标记为 <code>x</code>，输出标记为 $\hat x$，分别就<strong>训练阶段</strong>和<strong>测试阶段</strong>说明前向传播过程：</p>
<ul>
<li><strong>训练阶段</strong><br> 首先计算当前 <code>Batch</code> 的 $\mu_B, \sigma_{B^2}$，根据<br> $$ \hat x_{train} &#x3D; {\frac {x_{train} - \mu_B} {\sqrt {\sigma_{B^2} + \epsilon}}} * \gamma + \beta $$<br> 计算 <code>BN</code> 层输出。<br> 同时按照<br> $$\mu_r \leftarrow momentum * \mu_r + (1 - momentum) * \mu_B$$<br> $$ \sigma_{r^2} \leftarrow momentum * \sigma_{r^2} + (1 - momentum) * \sigma_{B^2} $$<br> 迭代更新全局训练参数的统计值 $\mu_r, \sigma_{r^2}$，其中 $momentum$ 是设置的一个超参数，其用于确定 $\mu_r, \sigma_{r^2}$ 的更新幅度（类似于学习率）。在 <code>TensorFlow</code> 中 $momentum$ 的默认设置为 <code>0.99</code>。</li>
<li><strong>测试阶段</strong><br> <code>BN</code> 层根据<br> $$ \hat x_{test} &#x3D; {\frac {x_{test} - \mu_B} {\sqrt {\sigma_{B^2} + \epsilon}}} * \gamma + \beta  $$<br> 计算输出 $\hat x_{test}$，其中 $\mu_r, \sigma_{r^2}, \gamma, \beta$ 来自于训练阶段统计或优化的结果，在测试阶段直接使用即可并不会更新这些参数。</li>
</ul>
<h4 id="反向更新"><a href="#反向更新" class="headerlink" title="反向更新"></a>反向更新</h4><p>在训练模式下的反向更新阶段中，反向传播算法根据损失 <code>L</code>求解梯度 ${\frac {\delta L} {\delta \gamma}}, {\frac {\delta L} {\delta \beta}}$，并按梯度更新法则自动优化 $\gamma, \beta$ 参数。</p>
<p>以 <code>shape</code> 为 <code>[100,32,32,3]</code> 的输入为例，在通道轴 <code>c</code> 上面的均值计算如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">100</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>]) <span class="comment"># 构造输入</span></span><br><span class="line">x = tf.reshape(x, [-<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 将其他唯独合并，仅保留通道维度</span></span><br><span class="line">ub = tf.reduce_mean(x, axis=<span class="number">0</span>) <span class="comment"># 计算其他维度的均值</span></span><br><span class="line">ub.shape <span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<p>通道有 <code>c</code> 个通道数，则有 <code>c</code> 个均值产生。</p>
<p>除了在 <code>c</code> 轴上统计数据 $\mu_B, \sigma_{B^2}$ 的方式，也很容易将其推广到其他维度的计算均值的方式：</p>
<ul>
<li><strong><code>Layer Norm</code></strong> ：统计每个样本的所有特征的均值和方差。</li>
<li><strong><code>Instance Norm</code></strong> ：统计每个样本的每个通道上特征的均值和方差。</li>
<li><strong><code>Group Norm</code></strong> ：将 <code>c</code> 通道分为若干个组，统计每个样本的通道组内特征的均值和方差。</li>
</ul>
<h4 id="BN-层实现"><a href="#BN-层实现" class="headerlink" title="BN 层实现"></a><code>BN</code> 层实现</h4><p>在 <code>TensorFlow</code> 中通过 <code>layers.BatchNormalization()</code> 类可以非常方便地实现 <code>BN</code> 层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer = layers.BatchNormalization()</span><br></pre></td></tr></table></figure>
<p><small>与全连接层、卷积层不同，由于 <code>BN</code> 层在训练阶段和测试阶段行为不同，需要通过设置 <code>training</code> 标志来区分<strong>训练模式</strong>或<strong>测试模式</strong>。</small></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([</span><br><span class="line">    layers.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>),</span><br><span class="line">    layers.BatchNormalization(), <span class="comment"># 插入 BN 层</span></span><br><span class="line">    layers.MaxPooling2D(pooling_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">    layers.ReLU(),</span><br><span class="line">    layers.Conv2D(<span class="number">6</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>),</span><br><span class="line">    layers.BatchNormalization(), <span class="comment"># 插入 BN 层</span></span><br><span class="line">    layers.MaxPooling2D(pooling_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">    layers.ReLU(),</span><br><span class="line">    layers.Flatten(),</span><br><span class="line">    layers.Dense(<span class="number">120</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">84</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">10</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    x = tf.expand_dims(x, axis=<span class="number">3</span>)</span><br><span class="line">    out = network(x, training=<span class="literal">True</span>) <span class="comment"># 训练阶段需要设置 training=True</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> db_test:</span><br><span class="line">        x = tf.expand_dims(x, axis=<span class="number">3</span>)</span><br><span class="line">        out = network(x, training=<span class="literal">False</span>) <span class="comment"># 测试阶段需要设置 training=False</span></span><br></pre></td></tr></table></figure>

<hr>

<h3 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><p>激活层又名<strong>非线性映射层（<code>non-linearity mapping</code>）</strong>，其主要是通过激活函数增加整个网络的的非线性能力（表达能力或抽象能力）。</p>
<p>激活函数中最经典当属 <strong><code>Sigmoid</code> 函数</strong>和 <strong><code>ReLU</code> 函数</strong>。</p>
<p><small>激活函数在之前已经说过，这里就不过多赘述，忘记的可以看看之前的内容。</small></p>
<hr>

<h3 id="变种卷积层"><a href="#变种卷积层" class="headerlink" title="变种卷积层"></a>变种卷积层</h3><h4 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h4><p>普通的卷积层为了减少网络的参数量，通常卷积核使用较小的 <code>1*1</code> 或 <code>3*3</code> 感受野。由于小卷积核提取网络特征时的感受野区域有限，增大感受野区域又会使得网络的参数量计算增大，因此其在实际中需要权衡比较。</p>
<p><strong>空洞卷积（<code>Dilated/Atrous Convolution</code>）</strong>就较好的解决了该问题，空洞卷积是在原有卷积的基础上增加一个 <code>Dilated Rate</code> 参数，用于控制感受野区域的采样步长。<br>当感受野的采样步长 <code>Dilated Rate</code> 为 <code>1</code> 时，每个感受野采样点之间的距离为 <code>1</code>，此时空洞卷积就与普通的卷积一致；当 <code>Dilated Rate</code> 为 <code>2</code> 时，感受野每两个单元采样一个点，每个采样格子之间的距离为 <code>2</code>；尽管 <code>Dilated Rate</code> 的增大会使得感受野区域增大，但实际中参与运算的点数仍未保持不变。</p>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/dildated.jpg" alt="dildated"><br><small>上述图片中的 <code>Dilated Rate</code> 为 <code>2</code>。</small></p>
<p>在 <code>TensorFlow</code> 中可以通过设置 <code>layers.Conv2D()</code> 类中 <code>dilation_rate</code> 参数来选择使用普通卷积还是空洞卷积。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">1</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">1</span>])</span><br><span class="line">layer = layers.Conv2D(<span class="number">1</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, dilation_rate=<span class="number">2</span>) <span class="comment"># dilation_rate=1 时为普通卷积，dilation_rate&gt;1 时为空洞卷积</span></span><br><span class="line">out = layer(x)</span><br><span class="line">out.shape <span class="comment"># TensorShape([1, 3, 3, 1])</span></span><br></pre></td></tr></table></figure>

<h4 id="分离卷积"><a href="#分离卷积" class="headerlink" title="分离卷积"></a>分离卷积</h4><p>普通卷积在对多通道输入进行运算时，卷积核的每个通道与输入的每个通道分别进行卷积运算，得到多通道的特征图，之后对应元素相加产生单个卷积核的最终输出。而<strong>深度可分离卷积（<code>Depth-wise Separable Convolution</code>）</strong>的计算流程则不同，卷积核的每个通道与输入的每个通道进行卷积运算，得到多个通道的中间特征，这些多通道的中间特征接下来进行多个 <code>1*1</code> 卷积核的普通卷积运算，得到多个高宽不变的输出，这些输出在通道轴上进行拼接，从而产生最终的分离卷积层输出。</p>
<p>分离卷积层包含两次卷积运算：</p>
<ul>
<li>第一步卷积运算是单个卷积核。</li>
<li>第二步卷积运算包含多个卷积核。</li>
</ul>
<p><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/deliver.jpg" alt="deliver"></p>
<p>分离卷积有什么优势呢？一个很明显的优势就是分离卷积的参数计算量约是普通卷积的 <code>1/3</code>。以上述为例：</p>
<ul>
<li>普通卷积的参数量：<br> <code>3 * 3 * 3 * 4 = 108</code></li>
<li>分离卷积的参数量：<br> <code>3 * 3 * 3 * 1 + 1 * 1 * 3 * 4 = 39</code></li>
</ul>
<p>分离卷积使用更少的参数量却也能完成普通卷积同样的输入输出尺寸变换，其在计算敏感的网络中应用很多。</p>
<hr>

<h3 id="经典卷积网络"><a href="#经典卷积网络" class="headerlink" title="经典卷积网络"></a>经典卷积网络</h3><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a><code>AlexNet</code></h4><p><code>AlexNet</code> 网络特点：</p>
<ul>
<li>层数达到了较深的 <code>8</code> 层。</li>
<li>采用了 <code>ReLU</code> 激活函数，之前的神经网络大多采用 <code>Sigmoid</code> 激活函数，计算相对复杂且容易出现梯度弥散现象。</li>
<li>引用 <code>Dropout</code> 层，其极大提高模型的泛化能力，防止过拟合。</li>
</ul>
<p><code>AlexNet</code> 网络结构<br><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/alexnet_network.jpg" alt="alexnet_network"></p>
<h4 id="VGG-系列"><a href="#VGG-系列" class="headerlink" title="VGG 系列"></a><code>VGG</code> 系列</h4><p><code>VGG</code> 系列网络特点：</p>
<ul>
<li>层数提升至 <code>19</code> 层。</li>
<li>全部采用更小的 <code>3*3</code> 卷积核，相比 <code>AlexNet</code> 参数量更少，计算代价更低。</li>
<li>采用更小的池化层 <code>2*2</code> 窗口和 <code>s=2</code> 步长。</li>
</ul>
<p><code>VGG</code> 系列网络结构<br><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/vgg_network.jpg" alt="vgg_network"></p>
<h4 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a><code>GoogleNet</code></h4><p><code>GoogleNet</code> 网络采用模块化设计思想，通过堆叠大量 <strong><code>Inception</code> 模块</strong>，以此形成复杂的网络结构。<br><code>Inception</code> 模块输入为 <code>X</code>，通过 <code>4</code> 个子网络得到 <code>4</code> 个网络输出，在通道轴上进行拼接合并，最终形成 <code>Inception</code> 模块的输出。<br><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/goo_ince_network.jpg" alt="goo_ince_network"></p>
<p><code>GoogleNet</code> 网络特点：</p>
<ul>
<li>引入 <code>Inception</code> 模块。</li>
<li>使用 <code>1*1</code> 卷积核进行降维及映射处理。</li>
<li>添加两个辅助分类器帮助训练。</li>
<li>放弃使用全连接层，使用平均池化层。</li>
</ul>
<p><code>GoogleNet</code> 网络结构<br><img src="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/goo_network.jpg" alt="goo_network"></p>
<hr>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>一入深度学习深似海，回首机器学习在招手。😭</p>
<hr>

<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a target="_blank" rel="noopener" href="https://cs231n.github.io/">https://cs231n.github.io/</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Lb411b7BS">卷积神经网络</a><br><a target="_blank" rel="noopener" href="https://liuruiyang98.github.io/posts/2021/08/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-5-GoogLeNet%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.html">深度学习之图像分类（五）– GoogLeNet网络结构</a><br><a target="_blank" rel="noopener" href="https://space.bilibili.com/18161609/channel/collectiondetail?sid=48290">经典卷积神经网络 bilibili 课程</a></p>
<hr>

<h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习《TensorFlow深度学习》所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DL/" rel="tag"># DL</a>
              <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/DL-TensorFlow%E5%9F%BA%E7%A1%80%E5%AE%9E%E8%B7%B5/" rel="prev" title="DL-TensorFlow基础实践">
      <i class="fa fa-chevron-left"></i> DL-TensorFlow基础实践
    </a></div>
      <div class="post-nav-item">
    <a href="/DL-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AE%9E%E8%B7%B5/" rel="next" title="DL-卷积神经网络CNN实践">
      DL-卷积神经网络CNN实践 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.</span> <span class="nav-text">全连接层的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">基础原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="nav-number">2.1.</span> <span class="nav-text">局部相关性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB"><span class="nav-number">2.2.</span> <span class="nav-text">权值共享</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97"><span class="nav-number">2.3.</span> <span class="nav-text">卷积计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="nav-number">2.4.</span> <span class="nav-text">输入和卷积核</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%95%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%95%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="nav-number">2.4.1.</span> <span class="nav-text">单通道输入和单卷积核</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%95%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="nav-number">2.4.2.</span> <span class="nav-text">多通道输入和单卷积核</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%A4%9A%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="nav-number">2.4.3.</span> <span class="nav-text">多通道输入和多卷积核</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%95%BF%E5%92%8C%E5%A1%AB%E5%85%85"><span class="nav-number">2.5.</span> <span class="nav-text">步长和填充</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A5%E9%95%BF"><span class="nav-number">2.5.1.</span> <span class="nav-text">步长</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A1%AB%E5%85%85"><span class="nav-number">2.5.2.</span> <span class="nav-text">填充</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">3.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.1.</span> <span class="nav-text">卷积层实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9D%83%E5%80%BC"><span class="nav-number">3.1.1.</span> <span class="nav-text">自定义权值</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%B1%BB"><span class="nav-number">3.1.2.</span> <span class="nav-text">卷积层类</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%BC%A0%E6%92%AD"><span class="nav-number">3.2.</span> <span class="nav-text">梯度传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E8%A1%A8%E7%A4%BA"><span class="nav-number">3.3.</span> <span class="nav-text">卷积表示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">4.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BN-%E5%B1%82"><span class="nav-number">5.</span> <span class="nav-text">BN 层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">5.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E6%9B%B4%E6%96%B0"><span class="nav-number">5.2.</span> <span class="nav-text">反向更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.3.</span> <span class="nav-text">BN 层实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%B1%82"><span class="nav-number">6.</span> <span class="nav-text">激活层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E7%A7%8D%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">7.</span> <span class="nav-text">变种卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF"><span class="nav-number">7.1.</span> <span class="nav-text">空洞卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF"><span class="nav-number">7.2.</span> <span class="nav-text">分离卷积</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">8.</span> <span class="nav-text">经典卷积网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AlexNet"><span class="nav-number">8.1.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG-%E7%B3%BB%E5%88%97"><span class="nav-number">8.2.</span> <span class="nav-text">VGG 系列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GoogleNet"><span class="nav-number">8.3.</span> <span class="nav-text">GoogleNet</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">9.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E7%94%A8"><span class="nav-number">10.</span> <span class="nav-text">引用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E5%A4%87%E6%B3%A8"><span class="nav-number">11.</span> <span class="nav-text">个人备注</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="vgbhfive"
      src="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
  <p class="site-author-name" itemprop="name">vgbhfive</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">148</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/vgbhfive" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;vgbhfive" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:vgbhfive@foxmail.com" title="E-Mail → mailto:vgbhfive@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://vgbhfive.com/" title="Web-Site → https:&#x2F;&#x2F;vgbhfive.com" rel="noopener" target="_blank"><i class="fab fa-chrome fa-fw"></i>Web-Site</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">陕ICP备20002937号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 2016 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">vgbhfive</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '2ff0dea213e4c7c0bbcc',
      clientSecret: '7f3d808240b513b00a1dbf20d725809acc316b67',
      repo        : 'vgbhfive.github.io',
      owner       : 'vgbhfive',
      admin       : ['vgbhfive'],
      id          : 'e8c1adc1ba6a63b2dbd0e826b65820a7',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
