<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="G-QBK8PCQC9B">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.vgbhfive.cn","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="引入Spark 是用于处理大数据的集群计算框架 ，与其他大多数数据处理框架不同之处在于 Spark 没有以 MapReduce 作为执行引擎，而是使用它自己的分布式运行环境在集群上执行工作。另外 Spark 与 Hadoop 又紧密集成，Spark 可以在 YARN 上运行，并支持 Hadoop 文件格式及其存储后端（例如 HDFS）。 Spark 最突出的表现在于其能将 作业与作业之间的大规模的">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop-Spark">
<meta property="og:url" content="https://blog.vgbhfive.cn/Hadoop-Spark/index.html">
<meta property="og:site_name" content="Vgbhfive&#39;s Blog">
<meta property="og:description" content="引入Spark 是用于处理大数据的集群计算框架 ，与其他大多数数据处理框架不同之处在于 Spark 没有以 MapReduce 作为执行引擎，而是使用它自己的分布式运行环境在集群上执行工作。另外 Spark 与 Hadoop 又紧密集成，Spark 可以在 YARN 上运行，并支持 Hadoop 文件格式及其存储后端（例如 HDFS）。 Spark 最突出的表现在于其能将 作业与作业之间的大规模的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2023/04/17/OyXb2IrHGMF3z5J.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/17/BzWSR42FvDLtMql.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/z1HoGR53NuTWdbI.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/A4U8BWLM21ix9a5.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/sMCpmWhGg5qeiV1.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/897iHPxjcUebNE5.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/S5PKNrgB8GFZHRd.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/a6jyR9tQhSOGAEz.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/yilF4BnUgvVhQTC.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/uevImbGLT7N9rJK.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/04/10/cUXY3alpo6qZmrn.jpg">
<meta property="article:published_time" content="2023-03-12T03:06:30.000Z">
<meta property="article:modified_time" content="2023-04-18T14:06:05.388Z">
<meta property="article:author" content="vgbhfive">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/04/17/OyXb2IrHGMF3z5J.jpg">

<link rel="canonical" href="https://blog.vgbhfive.cn/Hadoop-Spark/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Hadoop-Spark | Vgbhfive's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Vgbhfive's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Vgbhfive's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-pictures">

    <a href="/pictures/" rel="section"><i class="fa fa-th fa-fw"></i>Pictures</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.vgbhfive.cn/Hadoop-Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
      <meta itemprop="name" content="vgbhfive">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vgbhfive's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hadoop-Spark
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-12 11:06:30" itemprop="dateCreated datePublished" datetime="2023-03-12T11:06:30+08:00">2023-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-18 22:06:05" itemprop="dateModified" datetime="2023-04-18T22:06:05+08:00">2023-04-18</time>
              </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p><code>Spark</code> 是用于<strong>处理大数据的集群计算框架</strong> ，与其他大多数数据处理框架不同之处在于 <code>Spark</code> 没有以 <code>MapReduce</code> 作为执行引擎，而是使用它自己的<strong>分布式运行环境</strong>在集群上执行工作。另外 <code>Spark</code> 与 <code>Hadoop</code> 又紧密集成，<code>Spark</code> 可以在 <code>YARN</code> 上运行，并支持 <code>Hadoop</code> 文件格式及其存储后端（例如 <code>HDFS</code>）。</p>
<p><code>Spark</code> 最突出的表现在于其能将 <strong>作业与作业之间的大规模的工作数据集存储在内存中</strong>。这种能力使得在性能上远超 <code>MapReduce</code> 好几个数量级，原因就在于 <code>MapReduce</code> 数据都是从磁盘上加载。根据 <code>Spark</code> 的处理模型有两类应用获益最大，分别是 <strong>迭代算法（即对一个数据集重复应用某个函数，直至满足退出条件）</strong>和 <strong>交互式分析（用户向数据集发出一系列专用的探索性查询）</strong> 。<br>另外 <code>Spark</code> 还因为其具有的 <strong><code>DAG</code> 引擎</strong>更具吸引力，原因在于 <code>DAG</code> 引擎可以处理任意操作流水线，并为用户将其转化为单个任务。</p>
<span id="more"></span>

<hr>

<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>从官方页面下载一个稳定版本的 <code>Spark</code> 二进制发行包（选择与当前使用 <code>Hadoop</code> 匹配的版本），然后在合适的位置解压文件包，并将 <code>Spark</code> 的解压路径添加到 <code>PATH</code> 中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tar -xzvf spark-x.y.z.tgz</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> PATH=~/spark-x.y.z/bin:<span class="variable">$PATH</span></span></span><br></pre></td></tr></table></figure>

<h4 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h4><p><code>Spark</code> 像 <code>MapReduce</code> 一样也有 <strong>作业（<code>job</code>）</strong>的概念，只不过 <code>Spark</code> 的作业相比 <code>MapReduce</code> 更加通用，因为 <code>Spark</code> 作业可以由任意的<strong>多阶段（<code>stages</code>）有向无环图（<code>DAG</code>）</strong>构成，其中每个阶段大致相当于 <code>MapReduce</code> 中的 <code>map</code> 阶段或 <code>reduce</code> 阶段。<br>这些阶段又被 <code>Spark</code> 运行环境分解为多个任务（<code>tash</code>），任务并行运行在分布式集群中的 <code>RDD</code> 分区上，类似于 <code>MapReduce</code> 中的任务。</p>
<p><code>Spark</code> 作业始终运行在应用（<code>application</code>）上下文中，它提供了 <code>RDD</code> 分组以及共享变量。一个应用可以串行或并行地运行多个作业，并为这些作业提供访问由同一应用的先前作业所缓存的 <code>RDD</code> 的机制。</p>
<h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a><code>RDD</code></h4><p><code>RDD</code> 又称<strong>弹性分布式数据集（<code>Resilient Distributed Dataset</code>，简称 <code>RDD</code>）</strong>是 <code>Spark</code> 最核心的概念，他是在<strong>集群中跨多个机器分区存储的一个只读的对象集合</strong> ，在 <code>Spark</code> 应用中首先会加载一个或多个 <code>RDD</code>，他们作为输入通过一系列转换得到一组目标 <code>RDD</code>，然后对这些目标 <code>RDD</code> 执行一个动作。<br><small>弹性分布式数据集中的“弹性”是指 <code>Spark</code> 可以通过重新安排计算来自动重建丢失的分区。</small></p>
<p><code>RDD</code> 是 <code>Spark</code> 最基本的抽象，<code>RDD</code> 关联着三个至关重要的属性：</p>
<ul>
<li>依赖关系。</li>
<li>分区（包括一些位置信息）</li>
<li>计算函数：<code>Partition =&gt; Iterator[T]</code></li>
</ul>
<p>这三大属性对于简单的 <code>RDD</code> 是不可或缺的，其他更高层的功能也都是基于这套模型构建的。首先，依赖关系列表会告诉 <code>Spark</code> 如何从必要的输入构建 <code>RDD</code>。其次，分区允许 <code>Spark</code> 将工作以分区为单位，分配到多个执行器上并行计算。最后，每个 <code>RDD</code> 都是一个计算函数 <code>compute</code>，可用于生成 <code>RDD</code> 所表示数据的 <code>Iterator[T]</code> 对象。</p>
<p>创建 <code>RDD</code> 共有三种方式：</p>
<ul>
<li>来自一个内存中的对象集合（也称为并行化一个集合）。适用于对少量的输入数据进行并行的 <code>CPU</code> 密集型计算。</li>
<li>使用外部存储器（例如 <code>HDFS</code>）中的数据集。创建一个外部数据集的引用。</li>
<li>对现有的 <code>RDD</code> 进行转换。</li>
</ul>
<h4 id="窄依赖和宽依赖"><a href="#窄依赖和宽依赖" class="headerlink" title="窄依赖和宽依赖"></a>窄依赖和宽依赖</h4><p>在对每一个 <code>RDD</code> 操作时，都会得到一个新的 <code>RDD</code>，那么前后的两个 <code>RDD</code> 就有了某种联系，即新的 <code>child RDD</code> 会依赖旧的 <code>parent RDD</code>。目前这些依赖关系被分为： <strong>窄依赖（<code>NarrowDependency</code>）</strong>和 <strong>宽依赖（<code>ShuffleDependency</code>）</strong>。</p>
<ol>
<li><p><strong>窄依赖</strong><br>官方解释为：<code>child RDD</code> 中的每个分区都依赖 <code>parent RDD</code> 中的一小部分分区。</p>
<p> <img src="https://s2.loli.net/2023/04/17/OyXb2IrHGMF3z5J.jpg" alt="hadoop_spark_10.jpg"></p>
<p> 上图包括了有关窄依赖的各种依赖情况：</p>
<ul>
<li><strong>一对一依赖</strong>：<code>child RDD</code> 中的每个分区都只依赖 <code>parent RDD</code> 中的一个分区，并且 <code>child RDD</code> 的分区数和 <code>parent RDD</code> 的分区数相同。属于这种依赖关系的转换算子有 <code>map()</code>、<code>flatMap()</code>、<code>filter()</code> 等。</li>
<li><strong>范围依赖</strong>：<code>child RDD</code> 和 <code>parent RDD</code> 的分区经过划分，每个范围内的父子 <code>RDD</code> 的分区都为一一对应的关系。属于这种依赖关系的转换算子有 <code>union()</code> 等。</li>
<li><strong>窄依赖</strong>：窄依赖可以理解为一对一依赖和范围依赖的组合使用。属于这种依赖关系的转换算子有 <code>join()</code>、<code>cartesian()</code>、<code>cogroup()</code> 等。</li>
</ul>
</li>
<li><p><strong>宽依赖</strong><br>宽依赖官方解释为需要两个 <code>shuffle</code> 的两个 <code>stage</code> 的依赖。</p>
<p> <img src="https://s2.loli.net/2023/04/17/BzWSR42FvDLtMql.jpg" alt="hadoop_spark_11.jpg"></p>
<p> <code>child RDD</code> 的一个分区依赖的是 <code>parent RDD</code> 中各个分区的某一部分，即 <code>child RDD</code> 的两个分区分别只依赖 <code>parent RDD</code> 中的部分，而计算出某个部分的过程，以及 <code>child RDD</code> 分别读取某个部分的过程（<code>shuffle write/shuffle read</code>），此过程正是 <code>shuffle</code> 开销所在。</p>
</li>
</ol>
<h4 id="转换和动作"><a href="#转换和动作" class="headerlink" title="转换和动作"></a>转换和动作</h4><p><code>Spark</code> 对 <code>RDD</code> 提供了两大操作：<strong>转换（<code>transformation</code>）</strong> 和 <strong>动作（<code>action</code>）</strong> 。转换是从现有 <code>RDD</code> 生成新的 <code>RDD</code>，而动作则触发对 <code>RDD</code> 的计算并对计算结果执行某种操作，要么返回给用户，要么保存到外部存储器中。</p>
<p>加载 <code>RDD</code> 或者执行转换并不会立即触发任何数据处理的操作，只不过是创建了一个计划，只有当对 <code>RDD</code> 执行某个动作时才会触发真正的计算。<br><small>如果想判断一个操作是转换还是动作，可以通过观察其返回类型：如果返回的类型是 <code>RDD</code>，那么他是一个转换否则就是一个动作。</small></p>
<p>在 <code>Spark</code>库中包含了丰富的操作，包含映射、分组、聚合、重新分区、采样、连接 <code>RDD</code> 以及把 <code>RDD</code> 作为集合来处理的各种转换，同时还包括将 <code>RDD</code> 物化为集合；对 <code>RDD</code> 进行统计数据的计算；从一个 <code>RDD</code> 中采样固定数量的元素；以及将 <code>RDD</code> 保存到外部存储器等各种动作。</p>
<h4 id="Lineage-机制"><a href="#Lineage-机制" class="headerlink" title="Lineage 机制"></a><code>Lineage</code> 机制</h4><p>相比其他系统的细颗粒度的内存数据更新级别的备份或者 <code>LOG</code> 机制，<code>RDD</code> 的 <strong><code>Lineage</code> 记录的是粗颗粒度的特定数据的 <code>Transformation</code> 操作</strong>（如 <code>filter</code>、<code>map</code>、<code>join</code> 等）行为。<br>当某个 <code>RDD</code> 的部分分区数据丢失时，它可以通过 <code>Lineage</code> 记录获取足够的信息来重新运算和恢复丢失的数据分区，该记录的内容就是前面提到的 <code>RDD</code> 之间的依赖关系。</p>
<h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p><code>Spark</code> 可以在跨集群的内存中<strong>缓存数据</strong> ，也就意味着对数据集所做的任何计算都会非常快。相比较而言，<code>MapReduce</code> 在执行另一个计算时必须从磁盘重新加载输入数据集，即使他们可以使用中间数据集作为输入，但也无法摆脱始终从磁盘加载数据的事实，也必然影响其执行速度。<br><small>被缓存的 <code>RDD</code> 只能由同一应用的作业来读取。同理，应用终止时，作业所缓存的 <code>RDD</code> 都会被销毁，除非这些 <code>RDD</code> 已经被持久化保存，否则无法访问。</small></p>
<p>默认的持久化级别共分为两类： <strong><code>MEMORY_ONLY</code></strong> 是默认持久化级别，使用对象在内存中的常规表示方式； <strong><code>MEMORY_ONLY_SER</code></strong> 是一种更加紧凑的表示方法，通过把分区中的元素序列化为字节数组来实现的。<br><code>MEMORY_ONLY_SER</code> 相比 <code>MEMORY_ONLY</code> 多了一笔 <code>CPU</code> 的开销，但若是生成的序列化 <code>RDD</code> 分区的大小适合被保存在内存中，而默认的持久化方式无法做到，那就说明额外的开销是值得。另外 <code>MEMORY_ONLY_SER</code> 还可以减少垃圾回收的压力，因为每个 <code>RDD</code> 被存储为一个字节数组而不是大量的对象。</p>
<p>默认情况下，<code>RDD</code> 分区的序列化使用的是 <code>Kryo</code> 序列化方法，通过压缩序列化分区可以进一步节省空间，而这通常是更好的选择。<br><small>将 <code>spark.rdd.compress</code> 属性设置为 <code>true</code>，并且可选地设置 <code>spark.io.compression.codec</code> 属性。</small></p>
<h4 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h4><p>在使用 <code>Spark</code> 的序列化时，需要从两个方面来考虑：</p>
<ul>
<li><strong>数据序列化</strong></li>
<li><strong>函数序列化（闭包函数）</strong></li>
</ul>
<h5 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h5><p>数据序列化在默认情况下，<code>Spark</code> 在通过网络将数据从一个 <code>executor</code> 发送到另一个 <code>executor</code> 时，或者以序列化的形式缓存（持久化）数据时，所使用的都是 <code>Java</code> 序列化机制。<br>使用 <code>Kryo</code> 序列化机制对于大多数 <code>Spark</code> 应用都是更好的选择，<code>Kryo</code> 是一个高效的通用 <code>Java</code> 序列化库，要想使用 <code>Kryo</code> 序列化机制，需要在应用中的 <code>SparkConf</code> 中设置 <code>spark.serializer</code> 属性，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br></pre></td></tr></table></figure>

<p><strong><code>Kryo</code></strong> 不要求被序列化的类实现某个特性的的接口，因此如果旧的对象需要使用 <code>Kryo</code> 序列化也是可以的，在配置启用 <code>Kryo</code> 序列化之后就可以使用了，不过话虽如此，若是使用前可以在 <code>Kryo</code> 中对这些类进行注册，那么就可以提高其性能。这是因为 <code>Kryo</code> 需要写被序列化对象的类的引用，如果已经引用已经注册的类，那么引用标识就只是一个整数，否则就是完整的类名。<br>在 <code>Kryo</code> 注册类很简单，创建一个 <code>KryoRegistrator</code> 子类，重写 <code>registerClasses()</code> 方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomKryoRegistrator</span> <span class="keyword">extends</span> <span class="title class_">KryoRegistrator</span> &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">registerClasses</span><span class="params">(Kryo kryo)</span> &#123;</span><br><span class="line">      kryo.register(Object.class);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后在 <code>driver</code> 应用中将 <code>spark.kryo.registrator</code> 属性设置为你的 <code>KryoRegistrator</code> 实现的完全限定类名：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;spark.kryo.registrator&quot;, &quot;CustomKryoRegistrator&quot;)</span><br></pre></td></tr></table></figure>

<h5 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h5><p>通常函数的序列化都会<strong>谨守本分</strong>，对于 <code>Spark</code> 来说，即使在本地模式下，也需要序列化函数，假若引入一个不可序列化的函数，那么应该在开发期间就应该发现。</p>
<h4 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h4><p><code>Spark</code> 应用可能会使用一些不属于 <code>RDD</code> 的数据，这些数据会被作为闭包函数的一部分被序列化后传递给下一个动作，这可以保证应用正常执行，但使用广播变量可以跟高效的完成相同的工作。</p>
<h5 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h5><p><strong>广播变量（<code>broadcast variable</code>）</strong>在经过序列化后被发送给各个 <code>executor</code>，然后缓存在那里，以便后期任务可以在需要时访问它。它与常规变量不同，常规变量是作为闭包函数的一部分被序列化的，因此他们在每个任务中都要通过网络被传输一次。<br>广播变量的作用类似于 <code>MapReduce</code> 中的分布式缓存，两者的不同之处在于 <code>Spark</code> 将数据保存到内存中，只有在内存耗尽时才会溢出到磁盘上。</p>
<h5 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h5><p><strong>累加器（<code>accumulator</code>）</strong>是在任务中只能对它做加法的共享变量，类似于 <code>MapReduce</code> 中的计数器。当作业完成后 <code>driver</code> 程序可以检索累加器的最终值。</p>
<hr>

<h3 id="作业运行机制"><a href="#作业运行机制" class="headerlink" title="作业运行机制"></a>作业运行机制</h3><p>在 <code>Spark</code> 作业的最高层，他有两个独立的实体： <strong><code>driver</code></strong> 和 <strong><code>executor</code></strong> 。<code>driver</code> 负责托管应用（<code>SparkContext</code>）并为作业调度任务。<code>executor</code> 专属于应用，他在应用运行期间运行，并执行该应用的任务。通常 <code>driver</code> 作为一个不由集群管理器（<code>cluster manager</code>）管理的客户端来运行，而 <code>executor</code> 则运行在集群的计算机上。 </p>
<h4 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h4><p>当对 <code>RDD</code> 执行一个动作时，会自动提交一个 <code>Spark</code> 作业。从内部看导致对 <code>SparkContext</code> 调用 <code>runJob()</code> 方法，然后将调用传递给作为 <code>driver</code> 的一部分运行的调度程序。调度程序由两部分组成： <strong><code>DAG</code> 调用程序</strong> 和 <strong>任务调度程序</strong> 。<code>DAG</code> 调度程序把作业分解为若干阶段，并由这些阶段组成一个 <code>DAG</code>。任务调度程序则负责把每个阶段中的任务提交到集群。</p>
<p><img src="https://s2.loli.net/2023/04/10/z1HoGR53NuTWdbI.jpg" alt="hadoop_spark_1.jpg"></p>
<h4 id="DAG-构建"><a href="#DAG-构建" class="headerlink" title="DAG 构建"></a><code>DAG</code> 构建</h4><p>要想了解一个作业如何被划分为阶段，首先需要了解在阶段中运行的任务的类型。有两种类型的任务： <strong><code>shuffle map</code> 任务</strong> 和 <strong><code>result</code> 任务</strong> ，从任务类型的名称可以看出 <code>Spark</code> 会怎样处理任务的输出。</p>
<ul>
<li><code>shuffle map</code> 任务。<br> 顾名思义 <code>shuffle map</code> 任务类似于 <code>MapReduce</code> 中 <code>shuffle</code> 的 <code>map</code> 端部分。每个 <code>shuffle map</code> 任务在一个 <code>RDD</code> 分区上运行计算，并根据分区函数把输出写入到一组新的分区中，以允许在后面的阶段中取用（后面的阶段可能由 <code>shuffle map</code> 任务组成，也可能由 <code>result</code> 任务组成），<code>shuffle map</code> 任务运行在除最终阶段之外的其他所有阶段中。</li>
<li><code>result</code> 任务。<br> <code>result</code> 任务运行在最终阶段，并将结果返回给用户程序。每个 <code>result</code> 任务在他自己的 <code>RDD</code> 分区上运行计算，然后把结果发送回 <code>driver</code>，再由 <code>driver</code> 将每个分区的计算结果汇集成最终结果。</li>
</ul>
<p>最简单的 <code>Spark</code> 作业不需要使用 <code>shuffle</code>，因此它只有一个由 <code>result</code> 任务构成阶段，就像是 <code>MapReduce</code> 中仅有 <code>map</code> 一样。而比较复杂的作业要涉及到分组操作，并且要求一个或多个 <code>shuffle</code> 阶段。</p>
<p>如果 <code>RDD</code> 已经被同一应用（<code>SparkContext</code>）中先前的作业持久化保存，那么 <code>DAG</code> 调度程序将会省掉一些任务，不会再创建一些阶段来重新计算（或者它的父 <code>RDD</code>）。</p>
<p><code>DAG</code> 调度程序负责将一个阶段分解为若干任务以提交给任务调度程序。另外 <code>DAG</code> 调度程序会为每个任务赋予一个位置偏好（<code>placement preference</code>），以允许任务调度程序充分利用数据本地化（<code>data locality</code>）。</p>
<h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><p>当任务集合被发送到任务调度程序后，任务调度程序用该应用运行的 <code>executor</code> 的列表，在斟酌位置偏好的同时构建任务到 <code>executor</code> 的映射。接着任务调度程序将任务分配给具有内核的 <code>executor</code>，并且在 <code>executor</code> 完成运行任务时继续分配更多的任务，直到任务集合全部完成。默认情况下，每个任务到分配一个内核，不过也可以通过设置 <code>spark.task.cpus</code> 来更改。<br><small>任务调度程序在为某个 <code>executor</code> 分配任务时，首先分配的是进程本地化（<code>process-local</code>）任务，再分配节点本地（<code>node-local</code>）任务，然后分配机架本地（<code>rack-local</code>）任务，最后分配任意（非本地）任务或者推测任务（<code>speculative task</code>）。</small></p>
<p>这些被分配的任务通过调度程序后端启动。调度程序后端向 <code>executor</code> 后端发送远程启动任务的消息，以告知 <code>executor</code> 开始运行任务。</p>
<p>当任务成功完成或者失败时，<code>executor</code> 都会向 <code>driver</code> 发送状态更新信息。如果失败任务调度程序将在另一个 <code>executor</code> 上重新提交任务。若是启用推测任务（默认情况下不启用），它还会为运行缓慢的任务启动推测任务。</p>
<h4 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h4><p><code>executor</code> 首先确保任务的 <code>JAR</code> 包和文件依赖关系都是最新的，<code>executor</code> 在本地高速缓存中保留了先前任务已使用的所有依赖，因此只有在更新的情况下才会重新下载。接下来由于任务代码是以启动任务消息的一部分而发送的序列化字节，因此需要反序列化任务代码（包括用户自己的函数）。最后执行任务代码，不过需要注意的是因为运行任务在于 <code>executor</code> 相同的 <code>JVM</code> 中，因此任务的启动没有进程开销。</p>
<p>任务可以向 <code>driver</code> 返回执行结果，这些执行结果被序列化并发送到 <code>executor</code> 后端，然后以状态更新消息的形式返回 <code>driver</code>。<code>shuffle map</code> 任务返回的是一些可以让下一个阶段检索其输出分区的消息，而 <code>result</code> 任务则返回其运行的分区的结果值，<code>driver</code> 将这些结果值收集起来，并把最终结果返回给用户的程序。</p>
<h4 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h4><p><code>Spark</code> 如何依靠 <code>executor</code> 来运行构成 <code>Spark</code> 作业的任务，负责管理 <code>executor</code> 生命周期的是集群管理器（<code>cluster manager</code>），同时 <code>Spark</code> 提供了多种具有不同特性的集群管理器：</p>
<ul>
<li>本地模式<br> 在使用本地模式时，有一个 <code>executor</code> 与 <code>driver</code> 运行在同一个 <code>JVM</code> 中。此模式对于测试或运行小规模作业非常有用。</li>
<li>独立模式<br> 独立模式的集群管理器是一个简单的分布式实现，它运行了一个 <code>master</code> 以及一个或多个 <code>worker</code>。当 <code>Spark</code> 应用启动时，<code>master</code> 要求 <code>worker</code> 代表应用生成多个 <code>executor</code> 进程，这种模式的主 <code>URL</code> 为 <code>spark://host:port</code>。</li>
<li><strong><code>Mesos</code> 模式</strong><br> <code>Apache Mesos</code> 是一个通用的集群资源管理器，它允许根据组织策略在不同的应用之间细化资源共享。默认情况下（细粒度模式）每个 <code>Spark</code> 任务被当作是一个 <code>Mesos</code> 任务运行，这样做可以更有效地使用集群资源，但是以额外的进程启动开销为代价。在粗粒度模式下 <code>executor</code> 在进程中运行任务，因此在 <code>Spark</code> 应用运行期间的集群资源由 <code>executor</code> 进程来掌管，这种模式的主 <code>URL</code> 为 <code>mesos://host:port</code>。</li>
<li><strong><code>YARN</code> 模式</strong><br> <code>YARN</code> 是 <code>Hadoop</code> 中使用的资源管理器，每个运行的 <code>Spark</code> 应用对应于一个 <code>YARN</code> 应用实例，每个 <code>executor</code> 在自己的 <code>YARN</code> 容器中运行，这种模式的主 <code>URL</code> 为 <code>yarn-client</code> 或 <code>yarn-cluster</code>。<br> <small><code>YARN</code> 是唯一一个能够与 <code>Hadoop</code> 的 <code>Kerberos</code> 安全机制集成的集群管理器。</small></li>
</ul>
<h5 id="运行在-YARN-上的-Spark"><a href="#运行在-YARN-上的-Spark" class="headerlink" title="运行在 YARN 上的 Spark"></a>运行在 <code>YARN</code> 上的 <code>Spark</code></h5><p>在 <code>YARN</code> 上运行 <code>Spark</code> 提供了与其他 <code>Hadoop</code> 组件最紧密的集成，为了在 <code>YARN</code> 上运行，<code>Spark</code> 提供了两种部署模式： <strong><code>YARN</code> 客户端模式</strong>和 <strong><code>YARN</code> 集群模式</strong>。<code>YARN</code> 客户端模式的 <code>driver</code> 在客户端运行，而 <code>YARN</code> 集群模式的 <code>driver</code> 在 <code>YARN</code> 的 <code>application master</code> 集群上运行。</p>
<p>对于具有任何交互式组件的程序都必须使用 <code>YARN</code> 客户端模式，在交互式组件上的任何调试都是立即可见的。<br>另一方面 <code>YARN</code> 集群模式适用于生成作业（<code>production job</code>），因为整个应用在集群上运行，这样更易于保留日志文件（包括来自 <code>driver</code> 的日志文件）以供稍后检查。如果 <code>application master</code> 出现故障，<code>YARN</code> 还可以尝试重新运行该应用。</p>
<ol>
<li><p><code>YARN</code> 客户端模式<br>在 <code>YARN</code> 客户端模式下，当 <code>driver</code> 构建新的 <code>SparkContext</code> 实例时就会启动与 <code>YARN</code> 之间的交互，该 <code>SparkContext</code> 向 <code>YARN</code> 资源管理器提交一个 <code>YARN</code> 应用，而 <code>YARN</code> 资源管理器则启动集群节点管理器上的 <code>YARN</code> 容器，并在其中运行一个名为 <code>SparkExecutorLauncher</code> 的 <code>application master</code>。该 <code>ExecutorLauncher</code> 的工作是启动 <code>YARN</code> 容器中的 <code>executor</code>，为了做到这一点 <code>ExecutorLauncher</code> 要向资源管理器请求资源，然后启动 <code>ExecutorBackend</code> 进程作为分配给它的容器。</p>
<p><img src="https://s2.loli.net/2023/04/10/A4U8BWLM21ix9a5.jpg" alt="hadoop_spark_2.jpg"></p>
<p>每个 <code>executor</code> 在启动时都会连接回 <code>SparkContext</code>，并注册自身。因此这就向 <code>SparkContext</code> 提供了关于可用于运行任务的 <code>executor</code> 的数量及其位置的信息，之后这些信息会被用在任务的位置偏好策路中。</p>
<p><code>YARN</code> 资源管理器的地址并没有在主 <code>URL</code> 中指定(这与使用独立模式或 <code>Mesos</code> 模式的集群管理器不同)，而是从 <code>HADOOP_CONF_DIR</code> 环境变量指定的目录中的 <code>Hadoop</code> 配置中选取。</p>
</li>
<li><p><code>YARN</code> 集群模式<br>在 <code>YARN</code> 集群模式下，用户的 <code>driver</code> 程序在 <code>YARN</code> 的 <code>application master</code> 进程中运行，<code>spark-submit</code> 客户端将会启动 <code>YARN</code> 应用，但是它不会运行任何用户代码。剩余过程与客户端模式相同，除了 <code>application master</code> 在为 <code>executor</code> 分配资源之前先启动 <code>driver</code> 程序外。</p>
<p><img src="https://s2.loli.net/2023/04/10/sMCpmWhGg5qeiV1.jpg" alt="hadoop_spark_3.jpg"></p>
</li>
</ol>
<p>在这两种 <code>YARN</code> 模式下，<code>executor</code> 都是在还没有任何本地数据位置信息之前先启动的，因此最终有可能会导致 <code>executor</code> 与存有作业所希望访同文件的 <code>datanode</code> 不在一起。而这些对于交互式会话是可以接受的，特别是因为会话开始之前可能开不知道需要访问哪些数据集。但是对于生成作业来说情况并非如此，所以 <code>Spark</code> 提供了一种方法，可以在 <code>YARN</code> 群集模式下运行时提供一些有关位置的提示，以提高数据本地性。</p>
<p><code>sparkContext</code> 构造函数可以使用第二个参数来传递一个优选位置，该优选位置是利用 <code>InputFormatInfo</code> 辅助类根据输人格式和路径计算得到的。因此当向资源管理器请求分配时 <code>application master</code> 需要用到这个优选位置。</p>
<hr>

<h3 id="数据结构化-DataFrame"><a href="#数据结构化-DataFrame" class="headerlink" title="数据结构化 DataFrame"></a>数据结构化 <code>DataFrame</code></h3><p><code>Spark</code> 的 <strong><code>DataFrame</code> 是结构化的、有格式的</strong> ，且支持一些特定的操作，就像分布式内存中的表那样，每列都有名字，有表结构定义，每列都有特定的数据类型：整数、字符串型、数组、映射表、实数、日期、时间戳等。另外 <code>DataFrame</code> 中的数据是不可变的，<code>Spark</code> 记录着所有转化操作的血缘关系。可以添加列或者改变已有列的名字和数据类型，这些操作都会创建新的 <code>DataFrame</code> ，原有的 <code>DataFrame</code> 则会保留。在 <code>DataFrame</code> 中，一列与其名字和对应的 <code>Spark</code> 数据类型都在表结构中定义。</p>
<h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p>基本数据类型：</p>
<ul>
<li><code>ByteType</code></li>
<li><code>ShortType</code></li>
<li><code>IntegerType</code></li>
<li><code>LongType</code></li>
<li><code>FloatType</code></li>
<li><code>DoubleType</code></li>
<li><code>StringType</code></li>
<li><code>BooleanType</code></li>
<li><code>DecimalType</code></li>
</ul>
<p>复杂数据类型：</p>
<ul>
<li><code>BinaryType</code></li>
<li><code>TimestampType</code></li>
<li><code>DateType</code></li>
<li><code>ArrayType</code></li>
<li><code>MapType</code></li>
<li><code>StructType</code></li>
<li><code>StructField</code></li>
</ul>
<h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><p><code>Spark</code> 中的表结构为 <code>DataFrame</code> 定义了各列的名字和对应的数据类型。从外部数据源读取结构化数据时，表结构就会派上用场。相较于在读取数据时确定数据结构，提前定义表结构有如下优点：</p>
<ul>
<li>可以避免 <code>Spark</code> <strong>推断数据类型的额外开销</strong>。</li>
<li>可以防止 <code>Spark</code> 为决定表结构而单独创建一个作业来从数据文件读取很大一部分内容，对于较大的数据文件而言，其<strong>耗时相当长</strong> 。</li>
<li>可以<strong>尽早发现数据与表结构不匹配</strong> 。</li>
</ul>
<p>定义表结构有两种方式：</p>
<ul>
<li>编程的方式。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">schema = StructType([StructField(<span class="string">&quot;author&quot;</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">   StructField(<span class="string">&quot;title&quot;</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">   StructField(<span class="string">&quot;pages&quot;</span>, IntegerType(), <span class="literal">False</span>)])</span><br></pre></td></tr></table></figure></li>
<li>使用数据定义语言（<code>data definition language, DDL</code>）。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schema = &quot;author STRING, title STRING, pages INT&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="行与列"><a href="#行与列" class="headerlink" title="行与列"></a>行与列</h4><p><code>DataFrame</code> 中的具名列与 <code>Pandas</code> 中的 <code>DataFrame</code> 对象的具名列，以及关系数据库表的列的概念上是类似的：描述的都是一种字段。<br><small><code>Spark</code> 的文档对列有 <code>col</code> 和 <code>Column</code> 两种表示。<code>Column</code> 是列对象的类名，而 <code>col()</code> 是标准的内建函数，返回一个 <code>Column</code> 对象。</small><br><code>DataFrame</code> 中的 <code>Column</code> 对象不能单独存在，在一条记录中，每一列都是行的一部分，所有的行共同组成整个 <code>DataFrame</code>。</p>
<p><code>Spark</code> 中的行是用 <code>Row</code> 对象来表示的，它包含一列或多列，各列既可以是相同的类型，也可以是不同的类型。由于 <code>Row</code> 是 <code>Spark</code> 中的对象，表示一系列字段的有序集合，因此可以在编程中很容易的实例化 <code>Row</code> 对象，并用自 <code>0</code> 开始的下标访问该对象的各字段。</p>
<h4 id="表与视图"><a href="#表与视图" class="headerlink" title="表与视图"></a>表与视图</h4><p>表存放数据，<code>Spark</code> 中的每张表都关联有相应的元数据，而这些元数据是表及其数据的一些信息，包括表结构、描述、表名、数据库名、列名、分区、实际数据所在的物理位置等，这些全都存放在中心化的元数据库中。<br><code>Spark</code> 没有专门的元数据库，<strong>默认使用 <code>Apache Hive</code> 的元数据库来保存表的所有数据</strong> ，仓库路径位于 <code>/user/hive/warehouse</code>。如果想要想要修改默认路径，可以修改 <code>Spark</code> 配置变量 <code>spark.sql.warehouse.dir</code> 为别的路径，这个路径既可以是本地路径，也可以是外部的分布式存储。</p>
<h5 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h5><p><code>Spark</code> 允许创建两种表：</p>
<ul>
<li><strong>有管理表</strong><br><code>Spark</code> 既管理元数据，也管理文件存储上的数据。这里的文件存储可以理解为本地文件系统或 <code>HDFS</code>，也可以是外部的对象存储系统。</li>
<li><strong>无管理表</strong><br><code>Spark</code> 只管理元数据，需要自行管理外部数据源中的数据。</li>
</ul>
<h5 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h5><ol>
<li><p>创建数据库和表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE demo_db;</span><br><span class="line">USE demo_db;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> demo_table_1 (<span class="type">date</span> STRING, delay <span class="type">INT</span>, distance <span class="type">INT</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> demo_table_2 (<span class="type">date</span> STRING, delay <span class="type">INT</span>, distance <span class="type">INT</span>) <span class="keyword">USING</span> csv OPTIONS (PATH <span class="string">&#x27;/data/learing/data.csv&#x27;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>新增视图</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> REPLACE TEMP <span class="keyword">VIEW</span> [global_temp.]test_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="type">date</span>, delay, distance <span class="keyword">FROM</span> demo_table_1 <span class="keyword">WHERE</span> distance <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>缓存 <code>SQL</code> 表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CACHE [LAZY] <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span></span><br><span class="line">UNCACHE <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h4><h5 id="DataFrameReader"><a href="#DataFrameReader" class="headerlink" title="DataFrameReader"></a><code>DataFrameReader</code></h5><p><code>DataFrameReader</code> 是从数据源读取数据到 <code>DataFrame</code> 所用到的核心结构。用法有固定的格式和推荐的使用模式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrameReader.format(args).option(&quot;key&quot;, &quot;value&quot;).schema(args).load()</span><br></pre></td></tr></table></figure>
<p>这种将一串方法串联起来使用的模式在 <code>Spark</code> 中很常见，可读性也不错。</p>
<p>需要注意的是只能通过 <code>SparkSession</code> 实例访问 <code>DataFrameReader</code>，也就是说不能自行创建 <code>DataFrameReader</code> 实例，获取该实例的方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.read // 返回 DataFrameReader 从静态数据源读取 DataFrame</span><br><span class="line">SparkSession.readStream // 返回的实例用于读取流式数据源</span><br></pre></td></tr></table></figure>

<p><code>DataFrameReader</code> 的公有方法如下：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>format()</code></td>
<td><code>&quot;parquet&quot;</code>、 <code>&quot;csv&quot;</code>、 <code>&quot;txt&quot;</code>、 <code>&quot;json&quot;</code>、 <code>&quot;orc&quot;</code>、 <code>&quot;avro&quot;</code></td>
<td>如果不指定方法的格式，则使用 <code>spark.sql.sources.default</code> 所指定的默认格式</td>
</tr>
<tr>
<td><code>option()</code></td>
<td><code>(&quot;mode&quot;, [PERMISSIVE FAILFAST DROPMALFORMAD])</code> 、 <code>(&quot;inferSchema&quot;, [true false])</code>、<code>(&quot;path&quot;, &quot;path_file_data_source&quot;)</code></td>
<td>一系列键值对，<code>Spark</code> 文档中解释了不同模式下的对应行为</td>
</tr>
<tr>
<td><code>schema()</code></td>
<td><code>DDL</code> 字符串或 <code>StructType</code> 对象</td>
<td>对于 <code>JSON</code> 或者 <code>CSV</code> 格式，可以使用 <code>option()</code> 方法自行推断表结构</td>
</tr>
<tr>
<td><code>load()</code></td>
<td><code>/path/source</code></td>
<td>要读取的数据源路径</td>
</tr>
</tbody></table>
<p><small>从静态的 <code>Parquet</code> 数据源读取数据不需要提供数据结构，因为 <code>Parquet</code> 文件的元数据通常包含表结构信息。不过对于流式数据源，表结构信息是需要提供的。</small></p>
<h5 id="DataFrameWriter"><a href="#DataFrameWriter" class="headerlink" title="DataFrameWriter"></a><code>DataFrameWriter</code></h5><p><code>DataFrameWriter</code> 是 <code>DataFrameReader</code> 的反面，将数据保存或写入特定的数据源。与 <code>DataFrameReader</code> 不同，<code>DataFrameWriter</code> 需要从保存的 <code>DataFrame</code> 获取，推荐的使用模式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataFrameWriter.format(args).option(args).bucketBy(args).partitionBy(args).save(path)</span><br><span class="line">DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)</span><br></pre></td></tr></table></figure>

<p><code>DataFrameWriter</code> 的公有方法如下：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>format()</code></td>
<td><code>&quot;parquet&quot;</code>、 <code>&quot;csv&quot;</code>、 <code>&quot;txt&quot;</code>、 <code>&quot;json&quot;</code>、 <code>&quot;orc&quot;</code>、 <code>&quot;avro&quot;</code></td>
<td>如果不指定方法的格式，则使用 <code>spark.sql.sources.default</code> 所指定的默认格式</td>
</tr>
<tr>
<td><code>option()</code></td>
<td><code>(&quot;mode&quot;, append [overwrite ignore error])</code>、 <code>(&quot;path&quot;, &quot;path_to_write_to&quot;)</code></td>
<td>一系列键值对，<code>Spark</code> 文档中解释了不同模式下的对应行为</td>
</tr>
<tr>
<td><code>bucketBy()</code></td>
<td><code>(numBuckets, col, ..., coln)</code></td>
<td>按桶写入时，指定桶数量和分桶所依据字段的名字列表</td>
</tr>
<tr>
<td><code>save()</code></td>
<td><code>&quot;/path/source&quot;</code></td>
<td>写入的路径</td>
</tr>
<tr>
<td><code>saveAsTable()</code></td>
<td><code>&quot;table_name&quot;</code></td>
<td>写入的表名</td>
</tr>
</tbody></table>
<h5 id="文件类型"><a href="#文件类型" class="headerlink" title="文件类型"></a>文件类型</h5><ol>
<li><p><strong><code>Parquet</code></strong><br><code>Spark</code> 的默认数据源，很多大数据框架和平台都支持，它是一种开源的列式存储文件格式，提供多种 <code>I/O</code> 优化措施（比如压缩，以节省存储空间，支持快速访问数据列）。</p>
</li>
<li><p><code>JSON</code><br><code>JSON</code> 的全称为 <code>JavaScript Object Notation</code>，它 是一种常见的数据格式。<code>JSON</code> 有两种表示格式：单行模式和多行模式。</p>
</li>
<li><p><code>CSV</code><br><code>CSV</code> 格式应用非常官方，是一种将所有数据字段用逗号隔开的文本文件格式。在这些用逗号隔开的字段中，每行表示一条记录。（这里的逗号分隔符号是可以被修改的）</p>
</li>
<li><p><strong><code>Avro</code></strong><br><code>Avro</code> 格式有很多优点，包括直接映射 <code>JSON</code>、快速且高效、支持多种编程语言。</p>
</li>
<li><p><strong><code>ORC</code></strong><br>作为另一种优化后的列式存储文件格式，<code>Spark</code> 支持 <code>ORC</code> 的向量化读。向量化读通常会成块（）读入数据，而不俗一次读一行，同时操作会串起来，降低扫描、过滤、聚合、连接等集中操作时的 <code>CPU</code> 使用率。</p>
</li>
</ol>
<h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><p>复杂数据类型由简单数据类型组合而成，而实际则是经常直接操作复杂数据类型，操作复杂数据类型的方式有以下两种:</p>
<ul>
<li>将嵌套的结构打散到多行，调用某个函数，然后重建嵌套结构。</li>
<li>构建用户自定义函数。</li>
</ul>
<p>这两种方式都有助于以表格格式处理问题，一般会涉及到 <code>get_json_object()</code>、<code>from_json()</code>、<code>to_json()</code>、<code>explode()</code> 和 <code>selectExpr()</code> 等工具函数。</p>
<h5 id="打散再重组"><a href="#打散再重组" class="headerlink" title="打散再重组"></a>打散再重组</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT id, collect_list(value + 1) AS values</span><br><span class="line">FROM (</span><br><span class="line">   SELECT id, EXPLODE(values) AS value</span><br><span class="line">   FROM table</span><br><span class="line">) x</span><br><span class="line">GROUP BY id</span><br></pre></td></tr></table></figure>
<p>上述的嵌套的 <code>SQL</code> 语句中，先执行 <code>EXPLODE(values)</code>，会为每一个 <code>value</code> 创建新的一行（包括 <code>id</code> 字段）。<code>collect_list()</code> 返回的是未去重的对象列表，由于 <code>GROUP BY</code> 语句会触发数据混洗操作，因此重新组合的数组顺序和原数组不一定相同。 </p>
<h5 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h5><p>要想自行上述等价的任务，也可以创建 <code>UDF</code>，用 <code>map()</code> 迭代各个元素并执行额外的操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val = plusOneInt = (values: Array[Int] =&gt; &#123;</span><br><span class="line">   values.map(value =&gt; value + 1)</span><br><span class="line">&#125;)</span><br><span class="line">spark.udf.register(&quot;plusOneInt&quot;, plusOneInt)</span><br></pre></td></tr></table></figure>
<p>然后可以在 <code>Spark SQL</code> 中使用这个 <code>UDF</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;SELECT id, plusOneInt(values) AS values FROM table&quot;).show()</span><br></pre></td></tr></table></figure>
<p>由于没有顺序问题，这种方法比使用 <code>explode()</code> 和 <code>collect_list()</code> 好一些，但序列化和反序列化过程本身开销很大。</p>
<h5 id="复杂类型的内建函数"><a href="#复杂类型的内建函数" class="headerlink" title="复杂类型的内建函数"></a>复杂类型的内建函数</h5><p><code>Spark</code> 专门为复杂数据类型准备的内建函数，完整列表可以参考官方文档。</p>
<h5 id="高阶函数-1"><a href="#高阶函数-1" class="headerlink" title="高阶函数"></a>高阶函数</h5><p>除了上述的内建函数外，还有部分高阶函数接受匿名 <code>lambda</code> 函数作为参数，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># transform 函数接受一个数组和匿名函数作为输入，通过对数组的每个元素应用匿名函数，该函数将结果赋值到输出数组，透明地创建出一个新数组。</span><br><span class="line">transform(<span class="keyword">values</span>, <span class="keyword">values</span> <span class="operator">-</span><span class="operator">&gt;</span> lambda expression)</span><br></pre></td></tr></table></figure>

<ol>
<li><p><code>transform()</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform(array&lt;T&gt;, function&lt;T, U&gt;): array&lt;U&gt;</span><br></pre></td></tr></table></figure>
<p>通过对输入数组的每个元素使用一个函数，<code>transform()</code> 函数会生成新的数组。 </p>
</li>
<li><p><code>filter()</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter(array&lt;T&gt;, function&lt;T, Boolean&gt;): array&lt;T&gt;</span><br></pre></td></tr></table></figure>
<p><code>filter()</code> 函数输出的数组仅包含输入数组中让布尔表达式结果为 <code>true</code> 的元素。</p>
</li>
<li><p><code>exists()</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exists(array&lt;T&gt;, function&lt;T, V, Boolean&gt;): Boolean</span><br></pre></td></tr></table></figure>
<p>当输入数组中有任意一元素满足布尔函数时，<code>exists()</code> 函数返回 <code>true</code>。</p>
</li>
<li><p><code>reduce()</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(array&lt;T&gt;, B, function&lt;B, T, B&gt;, function&lt;B, R&gt;)</span><br></pre></td></tr></table></figure>
<p>通过函数 <code>function&lt;B, T, B&gt;</code>，<code>reduce()</code> 函数可以将数组的元素合并到缓冲区 <code>B</code>，最后对最终缓冲区使用最终函数 <code>function&lt;B, R&gt;</code>，并将数组归约为单个值。</p>
</li>
</ol>
<h5 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h5><ol>
<li><p>联合<br>将具有相同表结构的 <code>DataFrame</code> 联合起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 联合两种表</span></span><br><span class="line">bar = deplays.union(foo)</span><br><span class="line">bar.createOrReplaceTempView(<span class="string">&quot;bar&quot;</span>)</span><br><span class="line"><span class="comment"># 展示联合结果</span></span><br><span class="line">bar.filtyer(expr(<span class="string">&quot;origin == &#x27;SEA&#x27; AND destination == &#x27;SFO&#x27; AND date LIKE &#x27;0010%&#x27; AND deplay &gt; 0&quot;</span>)).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>连接<br>连接两个 <code>DataFrame</code> 是常用操作之一。<br>默认情况下连接为 <code>inner join</code>，可选的种类包含 <code>inner</code>、 <code>cross</code>、 <code>outer</code>、 <code>full</code>、 <code>full_outer</code>、 <code>left</code>、 <code>left_outer</code>、 <code>right</code>、 <code>right_outer</code>、 <code>left_semi</code> 和 <code>left_anti</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foo.join(ports, ports.IATA == foo.origin).select(<span class="string">&quot;City&quot;</span>, <span class="string">&quot;date&quot;</span>, <span class="string">&quot;deplay&quot;</span>, <span class="string">&quot;distance&quot;</span>).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>窗口<br>窗口函数使用窗口（一个范围内的输入行）中各行的值计算出一组值来返回，返回的一般是新的一行。通过使用窗口函数可以在每次操作一组的同时，返回的行数仍然和输入行数一一对应。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> origin, destiation, TotalDelays, rank</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">SELECT</span> origin, destiation, TotalDelays, <span class="built_in">dense_rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> origin <span class="keyword">ORDER</span> <span class="keyword">BY</span> TotalDelays <span class="keyword">DESC</span>) <span class="keyword">as</span> rank) t</span><br><span class="line"><span class="keyword">WHERE</span> rank <span class="operator">&lt;=</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<br>对 <code>DataFrame</code> 进行修改，<code>DataFrame</code> 本身不允许被修改，不过可以通过新建 <code>DataFrame</code> 的方式来实现修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加新列</span></span><br><span class="line">foo2 = (foo.withColumn(<span class="string">&quot;status&quot;</span>, expr(<span class="string">&quot;CASE WHEN delay &lt;= 10 THEN &#x27;On-time&#x27; ELSE &#x27;Delayed&#x27; END&quot;</span>)))</span><br><span class="line">foo2.show()</span><br><span class="line"><span class="comment"># 删除列</span></span><br><span class="line">foo3 = foo2.drop(<span class="string">&quot;delay&quot;</span>)</span><br><span class="line">foo3.show()</span><br><span class="line"><span class="comment"># 修改列名</span></span><br><span class="line">foo4 = foo3.withColumnRenamed(<span class="string">&quot;status&quot;</span>, <span class="string">&quot;flight_status&quot;</span>)</span><br><span class="line">foo4.show()</span><br><span class="line"><span class="comment"># 转置（将行与列数据互换）</span></span><br><span class="line">SELECT * FROM (</span><br><span class="line">   SELECT destination, CAST(SUBSTRING(date, <span class="number">0</span>, <span class="number">2</span>) AS <span class="built_in">int</span>) AS month, delay FROM departureDelays WHERE origin = <span class="string">&#x27;SEA&#x27;</span></span><br><span class="line">)</span><br><span class="line">PIVOT (</span><br><span class="line">   CAST(AVG(delay) AS DECIMAL(<span class="number">4</span>, <span class="number">2</span>)) AS AvgDelay, MAX(delay) AS MaxDelay FOR month IN (<span class="number">1</span> JAN, <span class="number">2</span> FEB)</span><br><span class="line">)</span><br><span class="line">ORDER BY destination</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>

<h3 id="数据结构化-Dataset"><a href="#数据结构化-Dataset" class="headerlink" title="数据结构化 Dataset"></a>数据结构化 <code>Dataset</code></h3><p>前面看过了 <code>DataFrame</code>，那么你基本上就理解了 <code>Dataset</code>，不过还是有那么一些差别，<code>Dataset</code> 主要区分两种特性： <strong>有类型</strong>和<strong>无类型</strong> 。</p>
<p><img src="https://s2.loli.net/2023/04/10/897iHPxjcUebNE5.jpg" alt="hadoop_spark_4.jpg"></p>
<p>从概念上看，可以将 <code>DataFrame</code> 看作是 <code>Dataset[Row]</code> 这种由普通对象组成的集合的一个别称，其中 <code>Row</code> 是普通的无类型对象，可以包含不同数据类型的字段。而 <code>Dataset</code> 则与之相反，是由同一类型的对象所组成的集合。正如官方文档中描述的那样：</p>
<blockquote>
<p>一种由领域专用对象组成的强类型集合，可以使用函数式或关系型的操作将其并行转化。</p>
</blockquote>
<table>
<thead>
<tr>
<th>语言</th>
<th>有类型和无类型的主要抽象结构</th>
<th>有类型或无类型</th>
</tr>
</thead>
<tbody><tr>
<td><code>Scala</code></td>
<td><code>Dataset[T]</code> 和 <code>DataFrame</code>（<code>Dataset[Row]</code> 的别命）</td>
<td>都有</td>
</tr>
<tr>
<td><code>Java</code></td>
<td><code>Dataset&lt;T&gt;</code></td>
<td>有类型</td>
</tr>
<tr>
<td><code>Python</code></td>
<td><code>DataFrame</code></td>
<td>普通 <code>Row</code> 对象，无类型</td>
</tr>
<tr>
<td><code>R</code></td>
<td><code>DataFrame</code></td>
<td>普通 <code>Row</code> 对象，无类型</td>
</tr>
</tbody></table>
<h4 id="转化数据"><a href="#转化数据" class="headerlink" title="转化数据"></a>转化数据</h4><p><code>Dataset</code> 是强类型的对象集合，这些对象可以使用函数式或关系型的算子并行转化。可用的转化操作包括 <code>map()</code>、<code>reduce()</code>、<code>filter()</code>、<code>select()</code> 和 <code>aggregate()</code>，这些方法都属于高阶函数，他们接受 <code>lambda</code> 表达式、闭包或函数作为参数，然后返回结果，因此这些操作非常适合函数式编程。</p>
<p>不过上述 <code>Dataset</code> 是有不足之处的，在使用高阶函数时，会产生从 <code>Spark</code> 内部的 <code>Tungsten</code> 格式反序列化为 <code>JVM</code> 对象的开销，那么避免多余的序列化和反序列化的策略有以下两种：</p>
<ul>
<li>在查询中使用 <code>DSL</code> 表达式，避免过多地使用 <code>lambda</code> 表达式的匿名函数作为高阶函数的参数。</li>
<li>将查询串联起来，以尽量减少序列化和反序列化。</li>
</ul>
<h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p><strong>编码器将堆外内存中的数据从 <code>Tungsten</code> 格式转为 <code>JVM</code> 对象，即编码器承担着在 <code>Spark</code> 内部格式和 <code>JVM</code> 对象之间序列化和反序列化 <code>Dataset</code> 对象</strong> 。<br><code>Spark</code> 支持自动生成原生类型、<code>Scala</code> 样例类和 <code>JavaBean</code> 的编码器。比起 <code>Java</code> 和 <code>Kryo</code> 的序列化和反序列化，<code>Spark</code> 的编码器要快很多。</p>
<p><code>Spark</code> 不为 <code>Dataset</code> 或 <code>DataFrame</code> 创建基于 <code>JVM</code> 的对象，而会分配 <code>Java</code> 堆外内存来存储数据，并使用编码器将内存表示的数据转为 <code>JVM</code> 对象。<br>当数据以紧凑的方式存储并通过指针和偏移量访问时，编码器可以快速序列化和反序列化数据。</p>
<p><img src="https://s2.loli.net/2023/04/10/S5PKNrgB8GFZHRd.jpg" alt="hadoop_spark_5.jpg"></p>
<p>相比 <code>JVM</code> 自建的序列化和反序列化，<code>Dataset</code> 编码器的优点如下：</p>
<ul>
<li><code>Spark</code> 内部的 <code>Tungsten</code> 二进制格式将对象存储在 <code>Java</code> 的堆内存之外，存储的方式很紧凑，因此对象占用的空间更小。</li>
<li>通过使用指针和计算出的内存地址与偏移量来访问内存，编码器可以实现快速序列化。</li>
<li>在接收端，编码器能快速地将二进制格式反序列化为 <code>Spark</code> 内部的表示形式。编码器不受 <code>JVM</code> 垃圾回收暂停的影响。</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/10/a6jyR9tQhSOGAEz.jpg" alt="hadoop_spark_6.jpg"></p>
<hr>

<h3 id="Spark-引擎"><a href="#Spark-引擎" class="headerlink" title="Spark 引擎"></a><code>Spark</code> 引擎</h3><p>在编程层面上，<code>Spark SQL</code> 允许开发人员对带有表结构的结构化数据发起兼容 <code>ANSI SQL:2003</code> 标准的查询。至此 <code>Spark SQL</code>  已经演变成一个非常重要的引擎，许多高层的结构化功能都是基于它构建出来的，除了可以对数据发起类似 <code>SQL</code> 的查询，<code>Spark SQL</code> 引擎还支持下列功能：</p>
<ul>
<li>统一 <code>Spark</code> 的各个组件，允许在 <code>Java</code>、<code>Scala</code>、<code>Python</code>、<code>R</code> 程序中将结构化数据集抽象为 <code>DataFrame</code> 或 <code>Dataset</code>，从而简化编程工作。</li>
<li>连接 <code>Apache Hive</code> 的元数据库和表。</li>
<li>从结构化的文件格式（<code>JSON</code>、<code>CSV</code>、<code>Text</code>、<code>Avro</code>、<code>Parquet</code>、<code>ORC</code> 等）使用给定的表结构读取结构化数据，并将数据转换为临时表。</li>
<li>为快速的数据探索提供交互式 <code>Spark SQL shell</code>。</li>
<li>通过标准的 <code>JDBS/ODBC</code> 连接器，提供与外部工具互相连接的纽带。</li>
<li>生成优化后的查询计划和紧凑的 <code>JVM</code> 二进制代码，用于最终执行。</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/10/yilF4BnUgvVhQTC.jpg" alt="hadoop_spark_7.jpg"></p>
<p><code>Spark SQL</code> 引擎的核心是 <strong><code>Catalyst</code> 优化器</strong>和 <strong><code>Tungsten</code> 项目</strong> ，其两者共同支撑高层的 <code>DataFrame API</code> 和 <code>Dataset API</code>，以及 <code>SQL</code> 查询。</p>
<h4 id="Catalyst-优化器"><a href="#Catalyst-优化器" class="headerlink" title="Catalyst 优化器"></a><code>Catalyst</code> 优化器</h4><p><code>Catalyst</code> 优化器接受计算查询作为参数，并将查询转化为执行计划。共分为四个转换阶段：</p>
<ul>
<li><strong>解析</strong><br><code>Spark SQL</code> 引擎首先会为 <code>SQL</code> 或 <code>DataFrame</code> 查询生成相应的抽象语法树（<code>abstract synrax tree, AST</code>）。所有的列名和表名都会通过查询内部元数据而解析出来，全部解析完成后，会进入下一阶段。</li>
<li><strong>逻辑优化</strong><br>这个阶段在内部共分为两步。通过应用基于规则的优化策略，<code>Catalyst</code> 优化器会首先构建出多个计划，然后使用基于代价的优化器（<code>cost-based optimizer, CBO</code>）为每个计划计算出执行开销。这些计划以算子树的形式呈现，其优化过程包括常量折叠、谓词下推、列裁剪、布尔表达式简化等，最终获得的逻辑计划作为下一阶段的输入，用于生成物理计划。</li>
<li><strong>生成物理计划</strong>hexo<br><code>Spark SQL</code> 会为选中的逻辑计划生成最佳的物理计划，这个物理计划由 <code>Spark</code> 执行引擎可用的物理算子组成。</li>
<li><strong>生成二进制代码</strong><br>在查询优化的最终阶段，<code>Spark</code> 会最终生成高效的 <code>Java</code> 字节码，用于在各个机器上执行。而在这个过程中用到了 <code>Tungsten</code> 项目，实现了执行计划的全局代码生成。<br>那什么是全局代码生成呢？他是物理计划的一个优化阶段，将整个查询合并为一个函数，避免虚函数调用，利用 <code>CPU</code> 寄存器存放中间数据，而这种高效策略可以显著提升 <code>CPU</code> 效率和性能。</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/10/uevImbGLT7N9rJK.jpg" alt="hadoop_spark_8.jpg"></p>
<p>通过上面的图示，可以发现只要执行过程相同，最终会生成相似的查询计划和一样的字节码用于执行，也就是说，无论使用什么编程语言，查询都会经过同样的过程，所生成的字节码很有可能都是一样的。</p>
<p>在经过最初的解析阶段之后，查询计划会被 <code>Catalyst</code> 优化器转化和重排。</p>
<p><img src="https://s2.loli.net/2023/04/10/cUXY3alpo6qZmrn.jpg" alt="hadoop_spark_9.jpg"></p>
<h4 id="Tungsten-项目"><a href="#Tungsten-项目" class="headerlink" title="Tungsten 项目"></a><code>Tungsten</code> 项目</h4><p><code>Tungsten</code> 项目致力于提升 <code>Spark</code> 应用对内存和 <code>CPU</code> 的利用率，使性能达到硬件的极限，主要包含以下内容：</p>
<ul>
<li><code>Memory Management and Binary Processing</code>: <code>off-heap</code> 管理内存，降低对象的开销和消除 <code>JVM GC</code> 带来的延时。</li>
<li><code>Cache-aware computation</code>: 优化存储，提升 <code>CPU L1/L2/L3</code> 缓存命中率。</li>
<li><code>Code generation</code>: 优化 <code>Spark SQL</code> 的代码生成部分，提升 <code>CPU</code> 利用率。</li>
</ul>
<hr>

<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr>

<h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/MySQL-mysqldump-warning-GTID/" rel="prev" title="MySQL-mysqldump warning GTID">
      <i class="fa fa-chevron-left"></i> MySQL-mysqldump warning GTID
    </a></div>
      <div class="post-nav-item">
    <a href="/MySQL-Could-not-find-first-log-file-name-in-binary-log-index-file/" rel="next" title="MySQL-Could not find first log file name in binary log index file">
      MySQL-Could not find first log file name in binary log index file <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E5%85%A5"><span class="nav-number">1.</span> <span class="nav-text">引入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">2.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%9C%E4%B8%9A"><span class="nav-number">2.2.</span> <span class="nav-text">作业</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD"><span class="nav-number">2.3.</span> <span class="nav-text">RDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AA%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="nav-number">2.4.</span> <span class="nav-text">窄依赖和宽依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2%E5%92%8C%E5%8A%A8%E4%BD%9C"><span class="nav-number">2.5.</span> <span class="nav-text">转换和动作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lineage-%E6%9C%BA%E5%88%B6"><span class="nav-number">2.6.</span> <span class="nav-text">Lineage 机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">2.7.</span> <span class="nav-text">持久化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96"><span class="nav-number">2.8.</span> <span class="nav-text">序列化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE"><span class="nav-number">2.8.1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%87%BD%E6%95%B0"><span class="nav-number">2.8.2.</span> <span class="nav-text">函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="nav-number">2.9.</span> <span class="nav-text">共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">2.9.1.</span> <span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">2.9.2.</span> <span class="nav-text">累加器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%9C%E4%B8%9A%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6"><span class="nav-number">3.</span> <span class="nav-text">作业运行机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4"><span class="nav-number">3.1.</span> <span class="nav-text">提交</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DAG-%E6%9E%84%E5%BB%BA"><span class="nav-number">3.2.</span> <span class="nav-text">DAG 构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E5%BA%A6"><span class="nav-number">3.3.</span> <span class="nav-text">调度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C"><span class="nav-number">3.4.</span> <span class="nav-text">执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86"><span class="nav-number">3.5.</span> <span class="nav-text">集群管理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E5%9C%A8-YARN-%E4%B8%8A%E7%9A%84-Spark"><span class="nav-number">3.5.1.</span> <span class="nav-text">运行在 YARN 上的 Spark</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8C%96-DataFrame"><span class="nav-number">4.</span> <span class="nav-text">数据结构化 DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A8%E7%BB%93%E6%9E%84"><span class="nav-number">4.2.</span> <span class="nav-text">表结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%8C%E4%B8%8E%E5%88%97"><span class="nav-number">4.3.</span> <span class="nav-text">行与列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A8%E4%B8%8E%E8%A7%86%E5%9B%BE"><span class="nav-number">4.4.</span> <span class="nav-text">表与视图</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%A1%E7%90%86%E8%A1%A8"><span class="nav-number">4.4.1.</span> <span class="nav-text">管理表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="nav-number">4.4.2.</span> <span class="nav-text">基本操作</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">4.5.</span> <span class="nav-text">数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#DataFrameReader"><span class="nav-number">4.5.1.</span> <span class="nav-text">DataFrameReader</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DataFrameWriter"><span class="nav-number">4.5.2.</span> <span class="nav-text">DataFrameWriter</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.5.3.</span> <span class="nav-text">文件类型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0"><span class="nav-number">4.6.</span> <span class="nav-text">高阶函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%93%E6%95%A3%E5%86%8D%E9%87%8D%E7%BB%84"><span class="nav-number">4.6.1.</span> <span class="nav-text">打散再重组</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="nav-number">4.6.2.</span> <span class="nav-text">自定义函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%8D%E6%9D%82%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%86%85%E5%BB%BA%E5%87%BD%E6%95%B0"><span class="nav-number">4.6.3.</span> <span class="nav-text">复杂类型的内建函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0-1"><span class="nav-number">4.6.4.</span> <span class="nav-text">高阶函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E6%93%8D%E4%BD%9C"><span class="nav-number">4.6.5.</span> <span class="nav-text">高阶操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8C%96-Dataset"><span class="nav-number">5.</span> <span class="nav-text">数据结构化 Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AC%E5%8C%96%E6%95%B0%E6%8D%AE"><span class="nav-number">5.1.</span> <span class="nav-text">转化数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">5.2.</span> <span class="nav-text">编码器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-%E5%BC%95%E6%93%8E"><span class="nav-number">6.</span> <span class="nav-text">Spark 引擎</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Catalyst-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">6.1.</span> <span class="nav-text">Catalyst 优化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tungsten-%E9%A1%B9%E7%9B%AE"><span class="nav-number">6.2.</span> <span class="nav-text">Tungsten 项目</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E7%94%A8"><span class="nav-number">7.</span> <span class="nav-text">引用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E5%A4%87%E6%B3%A8"><span class="nav-number">8.</span> <span class="nav-text">个人备注</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="vgbhfive"
      src="https://i.loli.net/2019/12/10/JF3dKDSkZoPz7h6.jpg">
  <p class="site-author-name" itemprop="name">vgbhfive</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">144</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/vgbhfive" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;vgbhfive" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:vgbhfive@foxmail.com" title="E-Mail → mailto:vgbhfive@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://vgbhfive.com/" title="Web-Site → https:&#x2F;&#x2F;vgbhfive.com" rel="noopener" target="_blank"><i class="fab fa-chrome fa-fw"></i>Web-Site</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">陕ICP备20002937号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 2016 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">vgbhfive</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '2ff0dea213e4c7c0bbcc',
      clientSecret: '7f3d808240b513b00a1dbf20d725809acc316b67',
      repo        : 'vgbhfive.github.io',
      owner       : 'vgbhfive',
      admin       : ['vgbhfive'],
      id          : '26c547993d307fd5a1b41dd82dcde918',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
