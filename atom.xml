<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vgbhfive&#39;s Blog</title>
  
  
  <link href="https://blog.vgbhfive.cn/atom.xml" rel="self"/>
  
  <link href="https://blog.vgbhfive.cn/"/>
  <updated>2024-05-06T13:57:46.184Z</updated>
  <id>https://blog.vgbhfive.cn/</id>
  
  <author>
    <name>vgbhfive</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Moto-摩旅日记2</title>
    <link href="https://blog.vgbhfive.cn/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/"/>
    <id>https://blog.vgbhfive.cn/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/</id>
    <published>2024-05-05T09:01:15.000Z</published>
    <updated>2024-05-06T13:57:46.184Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>昨天长途奔袭，一路飞奔算是回到了北京，大致算了以下本次摩旅跑了将近 <code>1100</code> 公里的路程。全程途径北京市、张家口市、乌兰察布市、大同市、保定市，最终回到我想念了四天的北京（主要是想念我的床，酒店的床非常不舒服😭）。</p><span id="more"></span><h3 id="游记"><a href="#游记" class="headerlink" title="游记"></a>游记</h3><h4 id="第一站"><a href="#第一站" class="headerlink" title="第一站"></a>第一站</h4><p>从北京出发，第一站目的地是张家口市的下花园区，中途还路过八达岭长城、官厅水库和鸡鸣驿古城。</p><p><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/1.jpg" alt="1"></p><p>大概中午一点左右到达官厅水库，停下车拿出我的折叠椅休息了半个小时，风有点大，吃了两个橘子和一个士力架，不远处山上的风力发电机一直转个不停，水库的湖面反射着太阳，那感觉是真的舒服啊（忘记拍照片了🙃）。</p><p>鸡鸣驿古城据说是中国最古老的驿站，始建于明朝，现存还保留有以前的基础设施，不过城里依旧住有居民，据说游客只要不太像“游客”，可以伪装成本地人然后进去转转，哈哈哈哈哈。</p><p><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/2.jpg" alt="2"></p><p>第一天到达张家口市下花园区后，在酒店办理了入住就出门去找点东西吃，那吃什么呢？<br>吃面吧。走了不到一百米看到一家人挺多的菜馆，走近一看有人在吃炒面，值得本地人傍晚六点半钟可以来吃的店保准没错，绝对的味好价廉！进到店里点了一份西红柿鸡蛋炒面、小份凉菜、三颗新上市的大蒜，果然人生又是美满的一天啊。</p><h4 id="第二站"><a href="#第二站" class="headerlink" title="第二站"></a>第二站</h4><p>早晨起的很早，因为睡的不是很舒服，而且窗户外的马路比较吵，紧接着收拾完东西就准备出发乌兰察布看火山了。</p><p>在进入内蒙古省界的时候我还发了一个朋友圈，对于我一个不怎么发朋友圈的人来说真是奇了大怪😂。<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/3.jpg" alt="3"></p><p>接下来请欣赏内蒙古的广阔吧！<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/4.jpg" alt="4"><br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/5.jpg" alt="5"></p><p>马路上的车很少，耳机的声音调到最大，自创的飙车歌单重复循环，镜片外的景色不断向后跑去，我知道我即将到达我的目的地，这一切都是如此的美好。路上还见到了好几个摩旅的摩友，简单的打个招呼就各自走了。</p><p>其实去乌兰察布市区之前，准备绕路去看火山的，本来以为人很少，结果到了目的地，感觉就像是<strong>宇航员批量制造基地</strong>，哈哈哈哈哈，我呆了五分钟就掉头去了乌兰察布市区。</p><p>到达市区就要开始今天的结束之战-<strong>晚饭</strong>。<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/6.jpg" alt="6"></p><p><small>我点的菜是排骨炖莜面，莜面、土豆很好吃，强烈推荐！！！（右上角的炸平菇是我自己想吃点的）</small></p><h4 id="第三站"><a href="#第三站" class="headerlink" title="第三站"></a>第三站</h4><p>说实话，昨晚睡的也不是很舒服，不知道为什么一直睡不踏实，翻来覆去的转圈，可能是因为昨晚看到恒山出了事故，担心旅途有变吧。今天目的地是大同，据说刀削面很好吃，那就收拾东西快些出发吧。</p><p>不过今天早餐是<strong>羊肉沙葱烧麦</strong>（原谅我刚端上来我没忍住就吃了两个），看招牌使用的是苏尼特的羊肉，羊肉很香，没有一点膻味，此时再搭配一碗鸡蛋汤，感觉今天我又能战斗 <code>200</code> 公里了。<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/7.jpg" alt="7"></p><p>中午大约 <code>11</code> 点钟到达大同市区，话不多说先来一碗刀削面，还单独加了两个豆干和一个丸子，各个方面都挺好吃的，只是个人觉得有点腻，加点醋会好吃很多。<br>吃完饭在大同的城墙内逛了不到一个小时就出来了，对于一个西安人来说城墙并不具备多大的诱惑力，那今天的目的地可能又要出现变化，出发灵丘！中途经过悬空寺看了一眼就走了，人太多😑，恒山那就下次再来吧。</p><p>说到灵丘，你可能不知道，但我说一个地方你肯定知道，那就是<strong>平型关</strong>。对的，没错！历史上的平型关战役就是在灵丘境内发生。</p><p>这次摩旅最有意思的就要属于在灵丘解决晚饭时发生的趣事了。到达灵丘时已经晚上七点半左右了，肚子太饿了就准备出去找点吃的。酒店附近有一家当地人比较多的饭馆，看菜单有一个小吃叫<strong>黄糕</strong>，心想这不就是个小吃嘛</p><p>“老板，点菜！”<br>“来一份黄糕，再来一份凉菜，再来一瓶易拉罐装可乐，就这些。”<br>“黄糕你吃不了，换一个吧！”<br>“啊？我为什么吃不了？（当时一脸懵逼，心里想我为啥吃不了，咱两有啥不一样吗？）”<br>此时旁边的大姐看我一个人问我“你是外地人吗？”<br>“是啊，我是来灵丘玩的”<br>此时老板发话了“黄糕外地人是咽不下去的，可以点个抿豆面就够吃，那个好吃！”<br>“行，那就来个抿豆面！ ”<br>一顿狼吞虎咽，抿豆面真好吃，特别是搭配的豆干，老板送的卤鸡蛋也很入味。</p><p>回到酒店我还在想“黄糕”为什么外地人咽不下去这个事，看了介绍后我确实咽不下去，很神奇的一个特色小吃，哈哈哈哈哈哈哈！</p><p><small>当地流传一句方言：“三十里荞面，四十里糕，二十里面条饿断腰。”其含义是指用黏米面做成的糕最耐饥，其次是莜面，最不耐饥的是面条。</small></p><h4 id="第四站"><a href="#第四站" class="headerlink" title="第四站"></a>第四站</h4><p>在灵丘的一晚睡的还不错，主要是枕头很舒服，一面是硬的，而另一面是软的，两个枕头摞在一起睡得很舒服。起床后看了下今天的行程，到家 <code>300</code> 公里，到达计划的目的地 <code>200</code> 公里，深思熟虑五分钟。冲啊啊啊！回家，我太想念我的床了。</p><p>一路狂奔……</p><p>路边休息看见河中的鸭子，跑到河滩看鸭子😂<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B02/8.jpg" alt="8"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>俗话说：“读万卷书不如行万里路”。现在万卷书在读了，万里路也该继续出发。</li><li>珍惜在路上的每一分钟。</li><li>耳朵戴着耳机，打开深度降噪，放一首土味 <code>DJ</code> 歌曲，右手控制时速 <code>80</code> 迈的摩托车，我愿评价此为今年最爽的时刻！</li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;昨天长途奔袭，一路飞奔算是回到了北京，大致算了以下本次摩旅跑了将近 &lt;code&gt;1100&lt;/code&gt; 公里的路程。全程途径北京市、张家口市、乌兰察布市、大同市、保定市，最终回到我想念了四天的北京（主要是想念我的床，酒店的床非常不舒服😭）。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Moto" scheme="https://blog.vgbhfive.cn/tags/Moto/"/>
    
  </entry>
  
  <entry>
    <title>DL-TensorFlow入门</title>
    <link href="https://blog.vgbhfive.cn/DL-TensorFlow%E5%85%A5%E9%97%A8/"/>
    <id>https://blog.vgbhfive.cn/DL-TensorFlow%E5%85%A5%E9%97%A8/</id>
    <published>2024-04-27T03:23:12.000Z</published>
    <updated>2024-05-22T14:22:39.511Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong><code>TensorFlow</code></strong> 是由 <code>Google</code> 团队开发的深度学习框架，其初衷是以最简单的方式实现机器学习和深度学习的概念。该框架融合了计算代数的优化技术，极大地方便了复杂数学表达式的计算。</p><p><code>TensorFlow</code> 深度学习框架的三大核心功能：</p><ul><li><strong>加速计算</strong>。神经网络本质上由大量的矩阵相乘、矩阵相加等基本数学运算构成，<code>TensorFlow</code> 的重要功能就是利用 <code>GPU</code> 方便地实现并行计算加速功能。</li><li><strong>自动梯度</strong>。<code>TensorFlow</code> 可以自动构建计算图，通过 <code>TensorFlow</code> 提供的自动求导的功能，不需要手动推导即可计算输出对网络参数的偏导数。</li><li><strong>常用神经网络接口</strong>。<code>TensorFlow</code> 除了提供底层的矩阵相乘、相加等数学函数，还包含常用神经网络运算函数、常用网络层、网络训练、模型保存与加载、网络部署等一系列深度学习的功能。</li></ul><p>简单示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a+b=&#x27;</span>, a+b) <span class="comment"># a+b= tf.Tensor(6.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="comment"># 运算时同时创建计算图 𝑐=𝑎+𝑏 和数值结果 6.0=2.0+4.0 的方式叫做命令式编程，也称为动态图模式。</span></span><br></pre></td></tr></table></figure><span id="more"></span><hr><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p><code>Tensorflow</code> 中的数据类型包含<strong>数值类型</strong>、<strong>字符串类型</strong>和<strong>布尔类型</strong>。</p><h5 id="数值类型"><a href="#数值类型" class="headerlink" title="数值类型"></a>数值类型</h5><p>按照维度区分为四种类型：<strong>标量</strong>、<strong>向量</strong>、<strong>矩阵</strong>、<strong>张量</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标量</span></span><br><span class="line">a1 = <span class="number">1</span></span><br><span class="line">a2 = tf.constant(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 向量</span></span><br><span class="line">b1 = tf.constant([<span class="number">1</span>, <span class="number">2.</span>, <span class="number">3.3</span>])</span><br><span class="line">b2 = b1.numpy() <span class="comment"># tf 张量转换为 numpy 数组</span></span><br><span class="line"><span class="comment"># 矩阵</span></span><br><span class="line">c1 = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 三维张量</span></span><br><span class="line">d1 = tf.constant([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])</span><br><span class="line"></span><br><span class="line">a1, a2, b1, b2, c1, d1</span><br></pre></td></tr></table></figure><h5 id="字符串类型"><a href="#字符串类型" class="headerlink" title="字符串类型"></a>字符串类型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1 = tf.constant(<span class="string">&#x27;Hello Tensorflow!&#x27;</span>)</span><br><span class="line">s2 = tf.strings.lower(s1)</span><br><span class="line">s1, s2</span><br></pre></td></tr></table></figure><h5 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bool_1 = tf.constant(<span class="literal">True</span>)</span><br><span class="line">bool_0 = tf.constant(<span class="literal">False</span>)</span><br><span class="line">bool_1, bool_0</span><br></pre></td></tr></table></figure><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><ol><li><p>从数组、列表创建张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.convert_to_tensor() 函数可以将 list 对象或 numpy 中的对象倒入到新的 Tensor 中</span></span><br><span class="line">tf.convert_to_tensor([<span class="number">1</span>, <span class="number">2.</span>]), tf.convert_to_tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure></li><li><p>创建全 <code>0</code> 或 <code>1</code> 张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标量</span></span><br><span class="line">tf.zeros([]), tf.ones([])</span><br><span class="line"><span class="comment"># 向量</span></span><br><span class="line">tf.zeros([<span class="number">1</span>]), tf.ones([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 矩阵</span></span><br><span class="line">tf.zeros([<span class="number">2</span>, <span class="number">2</span>]), tf.ones([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 张量</span></span><br><span class="line">tf.zeros([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]), tf.ones([<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></li><li><p>创建自定义值的张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.fill(shape, value)</span></span><br><span class="line">tf.fill([<span class="number">2</span>], -<span class="number">1</span>), tf.fill([<span class="number">2</span>, <span class="number">3</span>], -<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p>创建已知分布的张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.random.normal(shape, mean=0.0, stddev=1.0) 创建形状为 shape，均值为 mean，标准差为 stddev 的正态分布</span></span><br><span class="line">tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p>创建序列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.range(start=0, limit, delta=1) 函数创建一段连续的整数序列</span></span><br><span class="line">tf.<span class="built_in">range</span>(<span class="number">10</span>, delta=<span class="number">2</span>), tf.<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">10</span>, delta=<span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol><h5 id="待优化张量"><a href="#待优化张量" class="headerlink" title="待优化张量"></a>待优化张量</h5><p>为了区分需要计算梯度信息的张量和不需要计算梯度信息的张量。<code>Tensorflow</code> 中增加了一种专门的数据类型来支持梯度信息的记录：<code>tf.Variable()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v1 = tf.constant([-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">v2 = tf.Variable(v1)</span><br><span class="line">v1, v2, v2.name, v2.trainable</span><br></pre></td></tr></table></figure><h5 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h5><p><small>张量的典型应用现在说明可能会有点超时，不需要完全理解，有初步印象即可。</small></p><ol><li><p>标量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 就是一个简单的数字，维度数为 0，shape 为 []。常用在于误差值、各种测量指标等。</span></span><br><span class="line">out = tf.random.normal([<span class="number">4</span>, <span class="number">10</span>])</span><br><span class="line">y = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">y = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line">loss = tf.keras.losses.mse(y, out)</span><br><span class="line">loss = tf.reduce_mean(loss) <span class="comment"># 平均 mse </span></span><br><span class="line">loss</span><br></pre></td></tr></table></figure></li><li><p>向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 2 个输出节点的网络层，创建长度为 2 的偏置向量，并累加在每个输出节点上</span></span><br><span class="line">z = tf.random.normal([<span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.zeros([<span class="number">2</span>])</span><br><span class="line">z = z + b</span><br><span class="line">z</span><br></pre></td></tr></table></figure></li><li><p>矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="comment"># 令连接层的输出节点数为 3，则权值张量 w 的shape [4, 3]</span></span><br><span class="line">w = tf.random.normal([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.zeros([<span class="number">3</span>])</span><br><span class="line">out = x @ w + b</span><br><span class="line">out</span><br></pre></td></tr></table></figure></li><li><p>三维张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三维张量的一个典型应用就是表示序列信号，其格式是 x = [b, sequence len, feature len]</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=<span class="number">10000</span>)</span><br><span class="line">x_train = tf.keras.preprocessing.sequence.pad_sequence(x_train, maxlen=<span class="number">80</span>) <span class="comment"># 填充句子，截断为等长 80 个单词的句子</span></span><br><span class="line">x_train.shape <span class="comment"># (25000, 80)</span></span><br><span class="line"></span><br><span class="line">embedding = tf.layers.Embedding(<span class="number">10000</span>, <span class="number">100</span>) <span class="comment"># 创建词向量 Embedding 层类</span></span><br><span class="line">out = embedding(x_train) <span class="comment"># 将数字编码的单词转换为词向量</span></span><br><span class="line">out.shape <span class="comment"># ([25000, 80, 100])</span></span><br></pre></td></tr></table></figure></li><li><p>四维张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 四维张量在卷积神经网络中应用非常广泛，用于保存特征图数据，格式一般为 [b, h, w, c]，b 表示输入样本的数量，h/w 表示特征图的高/宽，c表示特征图的通道数。</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>]) <span class="comment"># 创造 32x32 的图片输入</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="数值精度"><a href="#数值精度" class="headerlink" title="数值精度"></a>数值精度</h4><p>常见数值精度： <strong><code>tf.int16</code></strong> 、 <strong><code>tf.int32</code></strong> 、 <strong><code>tf.int64</code></strong> 、 <strong><code>tf.float16</code></strong> 、 <strong><code>tf.float32</code></strong> 、 <strong><code>tf.float64</code></strong> 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">d1 = tf.constant(<span class="number">123456789</span>, tf.int16) <span class="comment"># 精度不足发生溢出</span></span><br><span class="line">d2 = tf.constant(<span class="number">123456789</span>, tf.int32)</span><br><span class="line">d1, d2</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">d3 = tf.constant(np.pi, tf.float32)</span><br><span class="line">d4 = tf.constant(np.pi, tf.float64) <span class="comment"># 精度更大保存的数据更多，对应的内存占用更多。</span></span><br><span class="line">d3, d4</span><br><span class="line"></span><br><span class="line">d5 = tf.constant(np.pi, tf.float16)</span><br><span class="line">d5 = tf.cast(d5, tf.float32) <span class="comment"># 类型转换</span></span><br><span class="line">d5</span><br></pre></td></tr></table></figure><h4 id="数值运算"><a href="#数值运算" class="headerlink" title="数值运算"></a>数值运算</h4><h5 id="加减乘除"><a href="#加减乘除" class="headerlink" title="加减乘除"></a>加减乘除</h5><p>加减乘除可以分别通过 <code>tf.add()</code> 、<code>tf.subtract()</code>、<code>tf.multiply()</code>、<code>tf.divide()</code> 函数实现，同时 <code>Tnsorflow</code> 也重载了 <code>+, -, *, /, //, %</code> 运算符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.<span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">b = tf.constant(<span class="number">2</span>)</span><br><span class="line">a//b, a%b</span><br></pre></td></tr></table></figure><h5 id="乘方运算"><a href="#乘方运算" class="headerlink" title="乘方运算"></a>乘方运算</h5><p><code>tf.pow(a, x)</code>、<code>tf.square(x)</code>、<code>tf.sqrt(x)</code> 函数分别实现乘方运算、平方和平方根运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">4</span>)</span><br><span class="line">x**<span class="number">2</span>, tf.<span class="built_in">pow</span>(x, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="指数对数运算"><a href="#指数对数运算" class="headerlink" title="指数对数运算"></a>指数对数运算</h5><p><code>tf.exp(a)</code> 函数实现自然对数 <code>e</code> 运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.exp(<span class="number">1.</span>)</span><br></pre></td></tr></table></figure><h5 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h5><p>使用 <code>@</code> 运算符实现矩阵相乘，而 <code>tf.matmul(a, b)</code> 函数也可以实现。其中 <code>a</code> 的倒数第一个维度长度（行）必须要 <code>b</code> 的倒数第二个维度长度（列）必须相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">32</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">2</span>])</span><br><span class="line">a@b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 哈达马乘积，要求矩阵 a 和 b 必须具有相同的阶</span></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">a*b</span><br></pre></td></tr></table></figure><h4 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h4><p>通过索引和切片可以提取张量部分的数据，实践中使用频率很高。</p><h5 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 第一张图片的数据，第一张图片数据的第二行，第一张图片数据的第二行第三列，第一张图片数据的第二行第三列B通道</span></span><br><span class="line">x[<span class="number">0</span>], x[<span class="number">0</span>][<span class="number">1</span>], x[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>], x[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于上述方式的索引</span></span><br><span class="line">x[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h5 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过 start:end:step 切片方式可以提取一段数据</span></span><br><span class="line">x[<span class="number">0</span>, ::] <span class="comment"># :: 表示读取在行维度上的所有行</span></span><br><span class="line">x[:, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>, :] <span class="comment"># :: 简写为 :</span></span><br></pre></td></tr></table></figure><h4 id="维度转换"><a href="#维度转换" class="headerlink" title="维度转换"></a>维度转换</h4><p><strong>维度变换</strong>是最核心的张量操作，算法的每个模块对于数据张量的格式有不同的逻辑需求，即现有的数据格式不能满足计算的要求，就需要通过维度变换将数据切换形式，满足不同场合的运算需求。<br>基本的维度变换操作函数有以下几种：<strong>改变视图 <code>reshape()</code></strong> 、 <strong>插入维度 <code>expand_dims()</code></strong> 、 <strong>删除维度 <code>squeeze()</code></strong> 、 <strong>交换维度 <code>transpose()</code></strong> 、 <strong>复制数据 <code>tile()</code></strong> 。</p><h5 id="改变视图"><a href="#改变视图" class="headerlink" title="改变视图"></a><strong>改变视图</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量的存储 Stroage 和视图 View 概念，同一个存储，在不同的角度观察数据，即可以产生不同的视图。</span></span><br><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">96</span>) <span class="comment"># 生成向量</span></span><br><span class="line">x = tf.reshape(x, [<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>]) <span class="comment"># 改变 x 的视图获得 4D 张量</span></span><br><span class="line">x</span><br><span class="line"><span class="comment"># 在通过 reshape 改变视图时，必须记住张量的存储顺序，新视图的维度不能与存储顺序相悖，否则需要通过【维度交换】将存储顺序调整。</span></span><br></pre></td></tr></table></figure><h5 id="增删视图"><a href="#增删视图" class="headerlink" title="增删视图"></a><strong>增删视图</strong></h5><p><code>tf.expand_dims(x, axis)</code> 在指定的 <code>axis</code> 轴前插入一个新的维度。<br><code>tf.squeeze(x, axis)</code> 其中 <code>axis</code> 为待删除维度的索引号，该参数默认值会删除所有长度为 <code>1</code> 的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加维度，增加一个长度为 1 的维度相当于给原有的数据添加一个新维度的概念，维度长度为 1，故数据格式不需要改变，其仅仅是改变了数据结构的理解方式。</span></span><br><span class="line">x = tf.random.uniform([<span class="number">28</span>, <span class="number">28</span>]) </span><br><span class="line">x.shape <span class="comment"># shape=(28, 28)</span></span><br><span class="line">x = tf.expand_dims(x, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除维度，是增加维度的逆操作，删除维度只能删除维度为 1 的维度，也不会改变张量的存储</span></span><br><span class="line">x = tf.squeeze(x, axis=<span class="number">0</span>)</span><br><span class="line">x.shape <span class="comment"># TensorShape([28, 28, 1])</span></span><br><span class="line">x = tf.squeeze(x, axis=<span class="number">2</span>)</span><br><span class="line">x.shape <span class="comment"># TensorShape([28, 28])</span></span><br></pre></td></tr></table></figure><h5 id="交换维度"><a href="#交换维度" class="headerlink" title="交换维度"></a><strong>交换维度</strong></h5><p><code>tf.transpose(x, perm)</code> 其中 <code>perm</code> 表示新维度的顺序 <code>list</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line">x = tf.transpose(x, [<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># shape=(2, 3, 32, 32)</span></span><br><span class="line">x.shape</span><br></pre></td></tr></table></figure><h5 id="复制数据"><a href="#复制数据" class="headerlink" title="复制数据"></a><strong>复制数据</strong></h5><p><code>tf.tile(x, multiples)</code> 完成数据在指定维度上的复制操作，<code>multiples</code> 表示每个维度上面的复制倍数，对应位置为 <code>1</code> 表示不复制，对应 <code>2</code> 表示复制一份。该函数会创建一个新的张量来保存复制后的张量，因为涉及较多的 <code>IO</code> 操作，计算代价较高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">b = tf.expand_dims(b, axis=<span class="number">0</span>) <span class="comment"># 插入新维度，变为矩阵</span></span><br><span class="line">b = tf.tile(b, multiples=[<span class="number">2</span>, <span class="number">1</span>]) <span class="comment"># 样本维度上复制一份</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure><h4 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a><code>Broadcasting</code></h4><p><strong><code>Broadcasting</code></strong> 称为广播机制，是一种轻量级的张量复制手段，在逻辑上扩展张量数据的形状，但只有实际使用时才会执行数据复制操作。<br><code>Broadcasting</code> 核心设计思想是<strong>普适性</strong>，即同一份数据可以普遍适合于其他位置。</p><ul><li>验证普适性之前首先需要将 <code>shape</code> 靠右对齐，然后进行普适性判断；</li><li>对于长度为 <code>1</code> 的维度，默认这个数据普遍适合于当前维度的其他位置。</li><li>对于不存在的维度，则在增加维度后默认当前数据是普适于新维度的，从而可以扩展更多维度数、任意长度的张量形状。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">w = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">y = x @ w + b</span><br><span class="line"><span class="comment"># 上述运行未发生异常的原因在于自动调用 Broadcasting 函数 tf.broadcast_to(x, new_shape) 将两者的shape 扩张为相同的 [2,3]</span></span><br><span class="line">x, w, b, y</span><br></pre></td></tr></table></figure><hr><h3 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h3><h4 id="合并与分割"><a href="#合并与分割" class="headerlink" title="合并与分割"></a>合并与分割</h4><h5 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h5><p>合并是指将多个张量在某个维度上合并为一个张量。张量的合并可以通过<strong>拼接（<code>Concatenate</code>）</strong>和<strong>堆叠（<code>Stack</code>）</strong>来实现，其中拼接不会产生新的维度，而堆叠会创造新的维度。<br><small>拼接和堆叠的唯一约束在于非合并维度的长度必须一致。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.contact(tensors, axis) 函数拼接张量，tensors 表示所有需要合并的张量 list，而 axis 表示拼接张量的维度索引</span></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>, <span class="number">35</span>, <span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">6</span>, <span class="number">35</span>, <span class="number">8</span>])</span><br><span class="line">tf.concat([a, b], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.stack(tensors, axis) 函数采用堆叠方式合并多个张量，其中参数 axis 的用法与 expand_dims() 一致</span></span><br><span class="line">a = tf.random.normal([<span class="number">35</span>, <span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">35</span>, <span class="number">8</span>])</span><br><span class="line">x = tf.stack([a, b], axis=<span class="number">0</span>)</span><br><span class="line">x.shape</span><br></pre></td></tr></table></figure><h5 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h5><p>合并的逆操作就是分割，即将一个张量分拆为多个张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.split(x, num_or_size_splits, axis)，其中参数 num_or_size_splits 指切割方案：单个数值时等长切割；列表时按列表内数值切分。</span></span><br><span class="line"><span class="comment"># tf.unstack(x, axis) 固定长度为 1 的方式切割。</span></span><br><span class="line">x = tf.random.normal([<span class="number">10</span>, <span class="number">35</span>, <span class="number">8</span>])</span><br><span class="line">res = tf.split(x, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">len</span>(res), res[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h4><p>在神经网络的计算中通常需要统计数据的各种属性，例如<strong>最值</strong>、<strong>最值位置</strong>、<strong>均值</strong>、<strong>范数</strong>等。</p><h5 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h5><p>向量范数是表征向量“长度”的一种度量方法，其可以推广到张量上，常用于表示张量的权值大小、梯度大小等。<br>常用的向量范数：</p><ul><li><strong><code>L1</code> 范数</strong>，定义为向量 <code>x</code> 的所有元素绝对值之和。</li><li><strong><code>L2</code> 范数</strong>，定义为向量 <code>x</code> 的所有元素的平方和，再开根号。</li><li><strong><code>np.inf</code> 范数</strong>，定义为向量 <code>x</code> 的所有元素绝对值得最大值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.ones([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">tf.norm(x, <span class="built_in">ord</span>=<span class="number">1</span>), tf.norm(x, <span class="built_in">ord</span>=<span class="number">2</span>), tf.norm(x, <span class="built_in">ord</span>=np.inf) <span class="comment"># 计算L1范数，计算L2范数，计算np.inf范数</span></span><br></pre></td></tr></table></figure><h5 id="最值、均值、和"><a href="#最值、均值、和" class="headerlink" title="最值、均值、和"></a>最值、均值、和</h5><p><code>tf.reduce_max(x,axis)</code>, <code>tf.reduce_min(x,axis)</code>, <code>tf.reduce_mean(x,axis)</code>, <code>tf.reduce_sum(x,axis)</code> 函数可以求解张量在某个维度或全局上的最大值、最小值、平均值、和。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>])</span><br><span class="line">tf.reduce_max(x, axis=<span class="number">0</span>), tf.reduce_min(x, axis=<span class="number">1</span>), tf.reduce_mean(x, axis=<span class="number">0</span>), tf.reduce_sum(x, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>`tf.argmax(x, axis)`, `tf.argmin(x, axis)` 可以获取 `axis` 轴上的最大值、最小值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line">out = tf.nn.softmax(out, axis=<span class="number">1</span>) <span class="comment"># 通过softmax() 函数转化为概率值</span></span><br><span class="line">tf.argmax(out, axis=<span class="number">0</span>), tf.argmin(out, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></code></pre><h4 id="张量比较"><a href="#张量比较" class="headerlink" title="张量比较"></a>张量比较</h4><p>为了计算分类任务的准确率，一般需要将预测结果和真实标签比较，统计比较结果中的正确值来计算准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = tf.random.normal([<span class="number">100</span>, <span class="number">10</span>])</span><br><span class="line">out = tf.nn.softmax(out, axis=<span class="number">1</span>) <span class="comment"># 输出转化为概率</span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 计算预测值</span></span><br></pre></td></tr></table></figure><p><code>tf.equal(a, b)</code> 或者 <code>tf.math.equal(a, b)</code> 均可以实现比较两个张量是否相等，返回布尔类型的张量比较结果。<br>类似的比较函数还有 <code>tf.math.greater()</code>, <code>tf.math.less()</code>, <code>tf.math.grater_equal()</code>, <code>tf.math.less_equal()</code>, <code>tf.math.not_equal()</code>, <code>tf.math.is_nan()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y = tf.random.uniform([<span class="number">100</span>], dtype=tf.int64, maxval=<span class="number">10</span>) <span class="comment"># 构建真实值</span></span><br><span class="line">x = tf.equal(pred, y) <span class="comment"># 比较张量</span></span><br><span class="line"><span class="comment"># 统计比较张量结果中的True</span></span><br><span class="line">x = tf.cast(x, dtype=tf.float32) <span class="comment"># 布尔类型转 int 类型 False -&gt; 0, True -&gt; 1</span></span><br><span class="line">correct = tf.reduce_sum(x) <span class="comment"># 统计 True 的个数</span></span><br><span class="line">correct <span class="comment"># 准确率</span></span><br></pre></td></tr></table></figure><h4 id="填充与复制"><a href="#填充与复制" class="headerlink" title="填充与复制"></a>填充与复制</h4><h5 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h5><p>在需要长度的数据开始或结束处填充足够数量的特定数值，这些数值通常代表无意义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.pad(x, padding) padding 包含多个 [Left Padding, Right Padding] 嵌套方案 list</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">b = tf.constant([<span class="number">7</span>,<span class="number">8</span>,<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line">b = tf.pad(b, [[<span class="number">0</span>,<span class="number">2</span>]]) <span class="comment"># 末尾填充两个0</span></span><br><span class="line">b <span class="comment"># [7,8,1,6,0,0]</span></span><br><span class="line"></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>])</span><br><span class="line">a = tf.pad(a, [[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">2</span>,<span class="number">2</span>], [<span class="number">2</span>,<span class="number">2</span>], [<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">a.shape <span class="comment"># (4,32,32,3)</span></span><br></pre></td></tr></table></figure><h5 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.tile() 函数可以在任意维度将数据重复复制多份</span></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>])</span><br><span class="line">a = tf.tile(a, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">a.shape <span class="comment"># (8, 84, 84, 3)</span></span><br></pre></td></tr></table></figure><h4 id="数据限幅"><a href="#数据限幅" class="headerlink" title="数据限幅"></a>数据限幅</h4><p>考虑如何实现非线性激活函数 <code>ReLU</code> 的问题，那么其可以通过数据限幅运算实现，限制元素的范围即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.maximum(x, a) 函数实现限制数据的下限幅。tf.minimum(x, a) 函数实现限制数据的上限幅。</span></span><br><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 下限幅，上限幅</span></span><br><span class="line">tf.maximum(x, <span class="number">2</span>), tf.minimum(x, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于 tf.maximum() 函数实现 ReLU 函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.maximum(x, <span class="number">0.</span>)</span><br><span class="line"><span class="comment"># 组合使用 tf.maximum(), tf.minimum() 可以同时对数据的上下边界限幅。tf.clip_by_value(x, a, b) 函数也可以实现同时对上下边界限幅</span></span><br><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">10</span>)</span><br><span class="line">tf.maximum(tf.minimum(x, <span class="number">7</span>), <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h4><ol><li><p><strong><code>tf.gather</code></strong><br>根据索引号收集数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>], maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">tf.gather(x, [<span class="number">0</span>,<span class="number">1</span>], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong><code>tf.gather_nd</code></strong><br>通过指定每次采样点的多维坐标来实现采样多个点的目的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>], maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">tf.gather_nd(x, [[<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">3</span>]])</span><br></pre></td></tr></table></figure></li><li><p><strong><code>tf.boolean_mask</code></strong><br>除了上述索引号的方式采样，还可以通过给定掩码（<code>Mask</code>）的方式进行采样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>], maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">tf.boolean_mask(x, mask=[<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], axis=<span class="number">0</span>) <span class="comment"># 掩码长度必须与维度长度一致</span></span><br></pre></td></tr></table></figure></li><li><p><strong><code>tf.where</code></strong><br>通过 <code>th.where(cond,a,b)</code> 操作可以根据 <code>cond</code> 条件的真假从参数 <code>A</code> 或参数 <code>B</code> 中读取数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.zeros([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">cond = tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>], [<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>], [<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line">tf.where(cond, a, b)</span><br></pre></td></tr></table></figure></li><li><p><strong><code>tf.scatter_nd</code></strong><br>通过 <code>tf.scatter_nd(indices, updates, shape)</code> 函数可以高效地刷新张量的部分数据，但该函数只能在全 <code>0</code> 的白板张量上面执行刷新操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">indices = tf.constant([[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">1</span>], [<span class="number">7</span>]])</span><br><span class="line">updates = tf.constant([<span class="number">4.4</span>, <span class="number">3.3</span>, <span class="number">1.1</span>, <span class="number">7.7</span>])</span><br><span class="line">tf.scatter_nd(indices, updates, [<span class="number">8</span>])</span><br></pre></td></tr></table></figure></li><li><p><strong><code>tf.meshgrid</code></strong><br>通过 <code>tf.meshgrid()</code> 函数可以方便地生成二维网格的采样点坐标，方便可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = tf.linspace(-<span class="number">8</span>, <span class="number">8</span>, <span class="number">100</span>)</span><br><span class="line">y = tf.linspace(-<span class="number">8</span>, <span class="number">8</span>, <span class="number">100</span>)</span><br><span class="line">x,y = tf.meshgrid(x, y)</span><br><span class="line">z = tf.sqrt(x**<span class="number">2</span> + y**<span class="number">2</span>)</span><br><span class="line">z = tf.sin(z) / z</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.contour3D(x.numpy(), y.numpy(), z.numpy(), <span class="number">50</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/1.jpg" alt="1"></p></li></ol><hr><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络属于机器学习的一个分支，特指利用<strong>多个神经元去参数化映射函数</strong>的模型。</p><h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><p>感知机模型如下：<br><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/2.jpg" alt="2"></p><p>其接受长度为 <code>n</code> 的一维向量 <code>x=[x1,x2,...,x]</code>，每个输入节点通过权值 <code>w</code> 的连接汇集为变量 <code>z</code>，即 <code>z = w1*x1 + w2*x2 + ... + w*x + b</code>。</p><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>感知机模型的<strong>不可导特征</strong>严重束缚其潜力，使得其只能解决简单的任务。在感知机的基础上，将不连续的阶跃激活函数更换成平滑连续可导的激活函数，并通过堆叠多个网络层来增强网络的表达能力。</p><p>通过替换感知机的激活函数，同时并行堆叠多个神经元来实现多输入、多输出的网络结构。</p><p>由于每个输出节点与全部的输入节点相连接，这种网络层被称为<strong>全连接层 <code>Fully-connected layer</code></strong> 或者<strong>稠密连接层 <code>Dense layer</code></strong> ，而 <code>W</code> 矩阵叫做全连接层的<strong>权值矩阵</strong> ，<code>b</code> 向量叫做全连接层的<strong>偏置向量</strong> 。</p><h5 id="张量实现"><a href="#张量实现" class="headerlink" title="张量实现"></a>张量实现</h5><p>在 <code>TensorFlow</code> 中，要实现全连接层，只需要定义好权值张量 <code>W</code> 和 偏执张量 <code>B</code>，并利用 <code>TensorFlow</code> 提供的批量矩阵相乘函数 <code>tf.matmul()</code> 即可完成网络层的计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">2</span>, <span class="number">784</span>])</span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line">o1 = tf.matmul(x, w1) + b1 <span class="comment"># 线性变换</span></span><br><span class="line">o1 = tf.nn.relu(o1) <span class="comment"># 激活函数</span></span><br><span class="line">o1.shape</span><br></pre></td></tr></table></figure><h5 id="层实现"><a href="#层实现" class="headerlink" title="层实现"></a>层实现</h5><p>全连接层本质上是矩阵的相乘和相加运算，实现并不复杂。但 <code>Tensorflow</code> 中有更方便的实现：<code>layers.Dense(units, activation)</code>，函数参数 <code>units</code> 指定输出节点数，<code>activation</code> 激活函数类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">fc = layers.Dense(<span class="number">512</span>, activation=tf.nn.relu) <span class="comment"># 创建全连接层，指定输出节点数和激活函数</span></span><br><span class="line">h1 = fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dense 类的权值矩阵，Dense 类的偏置向量，待优化参数列表，所有参数列表</span></span><br><span class="line">fc.kernel, fc.bias, fc.trainable_variables, fc.variables</span><br></pre></td></tr></table></figure><h4 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h4><p>在设计全连接网络时，网络的结构配置等超参数可以按经验法则自由设置，只需要遵循少量的约束即可。<br><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/3.jpg" alt="3"></p><h5 id="张量实现-1"><a href="#张量实现-1" class="headerlink" title="张量实现"></a>张量实现</h5><p>网络模型实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 隐藏层 1 张量</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>]), name=<span class="string">&#x27;w1&#x27;</span>)</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]), name=<span class="string">&#x27;b1&#x27;</span>)</span><br><span class="line"><span class="comment"># 隐藏层 2 张量</span></span><br><span class="line">w2 = tf.Variable(tf.random.truncated_normal([<span class="number">256</span>, <span class="number">128</span>]), name=<span class="string">&#x27;w2&#x27;</span>)</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">128</span>]), name=<span class="string">&#x27;b2&#x27;</span>)</span><br><span class="line"><span class="comment"># 隐藏层 3 张量</span></span><br><span class="line">w3 = tf.Variable(tf.random.truncated_normal([<span class="number">128</span>, <span class="number">64</span>]), name=<span class="string">&#x27;w3&#x27;</span>)</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">64</span>]), name=<span class="string">&#x27;b3&#x27;</span>)</span><br><span class="line"><span class="comment"># 输出层张量</span></span><br><span class="line">w4 = tf.Variable(tf.random.truncated_normal([<span class="number">64</span>, <span class="number">10</span>]), name=<span class="string">&#x27;w3&#x27;</span>)</span><br><span class="line">b4 = tf.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">&#x27;b3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># 隐藏层 1 前向计算，[b, 28*28] -&gt; [b, 256]</span></span><br><span class="line">    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[<span class="number">0</span>], <span class="number">256</span>])</span><br><span class="line">    <span class="comment"># 通过激活函数</span></span><br><span class="line">    h1 = tf.nn.relu(h1)</span><br><span class="line">    <span class="comment"># 隐藏层 2 前向计算，[b, 256] -&gt; [b, 128]</span></span><br><span class="line">    h2 = h1@w2 + b2</span><br><span class="line">    h2 = tf.nn.relu(h2)</span><br><span class="line">    <span class="comment"># 隐藏层 3 前向计算，[b, 128] -&gt; [b, 64]</span></span><br><span class="line">    h3 = h2@w3 + b3</span><br><span class="line">    h3 = tf.nn.relu(h3)</span><br><span class="line">    <span class="comment"># 输出层计算，[b, 64] -&gt; [b, 10]</span></span><br><span class="line">    h4 = h3@w4+b4</span><br></pre></td></tr></table></figure><p>使用 <code>TensorFlow</code> 自动求导计算梯度时，需要将前向计算过程放置在 <code>tf.GradientTape()</code> 环境中，利用 <code>GradientTape()`` 对象的 </code>gradient()&#96;&#96; 函数自动求解参数的梯度，并利用 <code>optimizer</code> 对象更新参数。</p><h5 id="层实现-1"><a href="#层实现-1" class="headerlink" title="层实现"></a>层实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">fc1 = layers.Dense(<span class="number">256</span>, activation=tf.nn.relu)</span><br><span class="line">fc2 = layers.Dense(<span class="number">128</span>, activation=tf.nn.relu)</span><br><span class="line">fc3 = layers.Dense(<span class="number">64</span>, activation=tf.nn.relu)</span><br><span class="line">fc4 = layers.Dense(<span class="number">10</span>, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">h1 = fc1(x)</span><br><span class="line">h2 = fc2(h1)</span><br><span class="line">h3 = fc3(h2)</span><br><span class="line">h4 = fc4(h3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然也可以使用 Sequential 构建一个网络大类对象</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=tf.nn.relu)</span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=tf.nn.relu)</span><br><span class="line">    layers.Dense(<span class="number">10</span>, activation=tf.nn.relu)</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 前向计算只需要调用一次网络大类对象，就可以完成所有层的按序计算</span></span><br><span class="line">out = model(x)</span><br></pre></td></tr></table></figure><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>激活函数与阶跃函数、符号函数不同，因为这些函数都是平滑可导的，适合用于于梯度下降算法。</p><h5 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a><strong><code>Sigmoid</code></strong></h5><p><code>Sigmoid</code> 函数也被称为 <code>Logistic</code> 函数，其定义为<br>$$ Sigmoid(x) &#x3D; {\frac {1} {1 + e^{-x}}} $$<br>该函数最大的特性在于可以将 <code>x</code> 的输入压缩到 $x \in (0,1)$ 区间，这个区间的数值在机器学习中可以表示以下含义：<br>     + 概率分布 <code>(0,1)</code> 区间的输出和概率的分布范围 <code>[0,1]</code> 一致，可以通过 <code>Sigmoid</code> 函数将输出转译为概率输出。<br>     + 信号，可以将 <code>0,1</code> 理解为某种信号，如像素的颜色强度，<code>1</code> 表示当前通道颜色最强，<code>0</code> 则表示当前通道无颜色。</p><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.linspace(-<span class="number">6.0</span>, <span class="number">6.0</span>, <span class="number">100</span>)</span><br><span class="line">y_sigmoid = tf.nn.sigmoid(x)</span><br></pre></td></tr></table></figure>![4](/DL-TensorFlow入门/4-1.jpg)</code></pre><h5 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a><strong><code>ReLU</code></strong></h5><p>在使用 <code>Sigmoid</code> 函数时，遇到输入值较大或较小时容易出现梯度为 <code>0</code> 的现象，该现象被称为<strong>梯度弥散现象</strong>，而在出现梯度弥散时，梯度长时间无法更新，会导致训练难以收敛或训练停止不动的现象发生。<br>为了解决上述问题 <code>ReLU</code> 函数被开始广泛使用，其函数定义如下：<br>$$ ReLU(x) &#x3D; max(0, x) $$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.linspace(-<span class="number">6.0</span>, <span class="number">6.0</span>, <span class="number">100</span>)</span><br><span class="line">y_relu = tf.nn.relu(x)</span><br></pre></td></tr></table></figure><p><small><code>ReLU</code> 函数的设计来源于神经科学，其函数值和导数值的计算十分简单，同时有着优秀的梯度特性，是目前最广泛应用的激活函数之一。</small><br><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/5.jpg" alt="5"></p><h5 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a><strong><code>LeakyReLU</code></strong></h5><p><code>ReLU</code> 函数在遇到输入值 <code>x&lt;0</code> 时也会出现梯度弥散现象，因此 <code>LeakyReLU</code> 函数被提出，其表达式如下：<br>$$ LeakyReLU &#x3D;<br>\begin{cases}<br>x,  &amp; x &gt;&#x3D; 0 \\<br>px, &amp; x &lt; 0<br>\end{cases} $$<br>其中 <code>p</code> 为用户自行设置的较小参数的超参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.linspace(-<span class="number">6.0</span>, <span class="number">6.0</span>, <span class="number">100</span>)</span><br><span class="line">y_leakyrelu = tf.nn.leaky_relu(x, alpha=<span class="number">0.1</span>) <span class="comment"># alpha 参数代表 p</span></span><br></pre></td></tr></table></figure><p><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/6.jpg" alt="6"></p><h5 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a><strong><code>Tanh</code></strong></h5><p><code>Tanh</code> 函数可以将 <code>x</code> 的输入压缩到 $x \in (-1,1)$ 区间，其定义为：<br>$$ tanh(x) &#x3D; {\frac {e^x - e^{-x}} {e^x + e^{-x}}} &#x3D; 2 * sigmoid(2x) - 1 $$<br><small><code>tanh</code> 激活函数可通过 <code>Sigmoid</code> 函数缩放平移后实现。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.linspace(-<span class="number">6.0</span>, <span class="number">6.0</span>, <span class="number">100</span>)</span><br><span class="line">y_tanh = tf.nn.tanh(x)</span><br></pre></td></tr></table></figure><p><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/7.jpg" alt="7"></p><h4 id="误差函数"><a href="#误差函数" class="headerlink" title="误差函数"></a>误差函数</h4><p>在搭建完模型结构后，接下来就是选择合适的误差函数来计算误差。常见的误差函数有<strong>均方差</strong>、<strong>交叉熵</strong>、 <strong><code>KL</code>散度</strong> 、 <strong><code>Hinge Loss</code> 函数</strong>等。<br>其中均方差函数和交叉熵函数较为常见，<em>均方差函数用于回归问题</em>，<em>交叉熵函数用于分类问题</em>。</p><h5 id="均方差误差函数"><a href="#均方差误差函数" class="headerlink" title="均方差误差函数"></a><strong>均方差误差函数</strong></h5><p>均方差 <code>MSE</code> 函数将输出向量和真实向量映射到笛卡尔坐标系的两个点上，通过计算这两个点之间的欧式距离（准确来讲是欧式距离的平方）来衡量两个向量之间的差距。<br><code>MSE</code> 误差函数的值总是大于等于 <code>0</code>，当 <code>MSE</code> 达到最小值 <code>0</code> 时，输出等于真实值，此时的神经网络的参数达到最优状态。<br><br>在 <code>TensorFlow</code> 中可以通过函数方式或层方式实现 <code>MSE</code> 误差计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过函数方式计算</span></span><br><span class="line">o = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造网络层输出值</span></span><br><span class="line">y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line">y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>)</span><br><span class="line">loss = keras.losses.MSE(y_onehot, o) <span class="comment"># 计算均方差</span></span><br><span class="line">loss</span><br><span class="line">loss = tf.reduce_mean(loss) <span class="comment"># 计算 batch 均方差</span></span><br><span class="line">loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过层方式实现</span></span><br><span class="line">criteon = keras.losses.MeanSquaredError() <span class="comment"># 创建 MSE 类</span></span><br><span class="line">loss = criteon(y_onehot, o) <span class="comment"># 计算 batch 均方差</span></span><br><span class="line">loss</span><br></pre></td></tr></table></figure><h5 id="交叉熵误差函数"><a href="#交叉熵误差函数" class="headerlink" title="交叉熵误差函数"></a><strong>交叉熵误差函数</strong></h5><p>熵用于衡量信息中的不确定度。熵越大代表不确定性越大，信息量也就越大。</p><p>基于熵引出<strong>交叉熵（<code>Cross Entropy</code>）</strong>的定义：<br>$$ H(p||q) &#x3D; - sum_i p(i) log_2 q(i) $$<br>通过变换，交叉熵可以分解为 <code>p</code> 的熵 $H(p)$ 和 <code>p</code> 与 <code>q</code> 的 <code>KL</code> 散度（<code>Kullback-Leibler Divergence</code>）的和：<br>$$ H(p||q) &#x3D; H(p) + D_{KL}(p||q) $$<br>而其中 <code>KL</code> 定义为：<br>$$ D_{KL}(p||q) &#x3D; sum_i p(i) log({\frac {p(i)} {q(i)}}) $$<br><code>KL</code> 散度是用于衡量两个分布之间距离的指标。<br>根据 <code>KL</code> 散度定义推导分类问题中交叉熵的计算表达式：<br>$$ H(p||q) &#x3D; H(p) + D_{KL}(p||q) &#x3D; D_{KL}(p||q) &#x3D; sum_j y_j log({\frac {y_j} {o_j}}) &#x3D; 1*log{\frac {1} {o_i}} + sum_{j!&#x3D;i} 0*log({\frac {0} {o_j}}) &#x3D; - log o_i $$</p><p><small>二分类问题时 $H(p)&#x3D;0$</small><br><small>最小化交叉熵损失函数的过程就是最大化正确类别的预测概率的过程。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">y_true = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">y_pred = tf.constant([-<span class="number">18.6</span>, <span class="number">0.51</span>, <span class="number">2.94</span>, -<span class="number">12.8</span>])</span><br><span class="line">bce = keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">loss = bce(y_true, y_pred)</span><br><span class="line">loss.numpy() <span class="comment"># 0.865458</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">y_true = tf.constant([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.05</span>, <span class="number">0.95</span>, <span class="number">0</span>], [<span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.1</span>], [<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]])</span><br><span class="line">cce = tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line">loss = cce(y_true, y_pred)</span><br><span class="line">loss.numpy() <span class="comment"># 0.85900736</span></span><br></pre></td></tr></table></figure><h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><p>当模型的容量过大时，网络模型除了学习训练集的模态之外，还会把额外的观测误差也学习，这样就会导致学习的模型在训练集上表现较好，但在未知的样本上表现不佳，也就是模型泛化能力较弱，这种现象被称为<strong>过拟合（<code>Overfitting</code>）</strong>。<br>当模型的容量过小时，模型不能够很好地学习到训练集数据的模态，导致模型在训练集上表现不佳，同时在未知的样本上也表现不佳，这种现象被称为<strong>欠拟合（<code>Underfitting</code>）</strong>。</p><p>用一个例子来解释模型容量和数据分布之间的关系：</p><ul><li><code>a</code>：使用简单线性函数去学习时，会很难学习到一个较好的函数，从而出现训练集和测试集均表现不佳。</li><li><code>b</code>：学习的模型和真实模型之间容量大致匹配时，模型才具有较好的泛化能力。</li><li><code>c</code>：使用过于复杂的函数去学习时，学习到的函数会过度“拟合”训练集，从而导致在测试集上表现不佳。</li></ul><p><img src="/DL-TensorFlow%E5%85%A5%E9%97%A8/8.png" alt="8"></p><hr><h3 id="Keras-高层接口"><a href="#Keras-高层接口" class="headerlink" title="Keras 高层接口"></a><code>Keras</code> 高层接口</h3><p><code>Keras</code> 是一个由 <code>Python</code> 语言开发的开源神经网络计算库，其被设计为高度模块化和易扩展的高层神经网络接口，使用户可以不需要通过过多的专业知识就可以轻松、快速地完成模型的搭建和训练。</p><p>在 <code>TensorFlow</code> 中 <code>Keras</code> 被实现在 <code>tf.keras</code> 子模块中。</p><h4 id="常见功能模块"><a href="#常见功能模块" class="headerlink" title="常见功能模块"></a>常见功能模块</h4><p><code>tf.keras</code> 提供了一系列高层的神经网络相关类和函数，如经典数据集加载函数、网络层类、模型容器、损失函数类、优化器类、经典模型类等。</p><h5 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h5><p>对于常见的神经网络层，可以直接使用张量方式的底层接口函数来实现，这些接口函数一般在 <code>tf.nn</code> 模块中。</p><p>在 <code>tf.keras.layers</code> 命名空间下提供了大量常见的网络层类，如<strong>全连接层</strong>、<strong>激活函数层</strong>、<strong>池化层</strong>、<strong>卷积层</strong>、<strong>循环神经网络层</strong>等。对于这些网络层类，只需要在创建时指定相关参数，并调用 <code>__call__</code> 方法即可完成前向计算。在调用 <code>__call__</code> 方法时，<code>Keras</code> 会自动触发每个层的前向传播逻辑，而这些逻辑也一般实现在 <code>call</code> 方法中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">2.</span>, <span class="number">1.</span>, <span class="number">0.1</span>])</span><br><span class="line">layer = layers.Softmax(axis=-<span class="number">1</span>) <span class="comment"># 创建 Softmax 层</span></span><br><span class="line">out = layer(x)</span><br><span class="line">out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另一种实现方式</span></span><br><span class="line">out_2 = tf.nn.softmax(x) <span class="comment"># 调用 softmax 函数完成前向计算</span></span><br><span class="line">out_2</span><br></pre></td></tr></table></figure><h5 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h5><p>通过 <code>Keras</code> 的网络容器 <code>Sequential</code> 可以将多个网络层封装成一个网络模型，只需要调用一次网络模型即可完成数据从第一层到最后一层的顺序传播运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">3</span>, activation=<span class="literal">None</span>),</span><br><span class="line">    layers.ReLU(),</span><br><span class="line">    layers.Dense(<span class="number">2</span>, activation=<span class="literal">None</span>),</span><br><span class="line">    layers.ReLU()</span><br><span class="line">])</span><br><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">out = model(x)</span><br><span class="line">out</span><br><span class="line"><span class="comment"># 还可以通过 add() 方法继续追加新的网络层，实现动态创建网络的功能。</span></span><br></pre></td></tr></table></figure><h4 id="模型装配、训练与测试"><a href="#模型装配、训练与测试" class="headerlink" title="模型装配、训练与测试"></a>模型装配、训练与测试</h4><p>在训练网络模型时，一般的流程是通过前向计算获得网络的输出值，再通过损失函数计算网络误差，然后通过自动求导工具计算梯度并更新，同时间隔性地测试网络的性能。</p><h5 id="模型装配"><a href="#模型装配" class="headerlink" title="模型装配"></a>模型装配</h5><p>在 <code>Keras</code> 中有两个比较特殊的类：</p><ul><li><strong><code>keras.Layer</code> 类</strong>：网络层的母类，其定义了网络层的一些常见功能，如添加权值、管理权值列表等。</li><li><strong><code>keras.Model</code> 类</strong>：网络的母类，除了具有 <code>Layer</code> 类的功能之外，还具有保存模型、加载模型、训练与测试模型等功能。<code>Sequential</code> 也是 <code>Model</code> 的子类。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">10</span>)</span><br><span class="line">]) <span class="comment"># 构建网络层</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure><p>创建网络之后，正常流程就是迭代数据集多个 <code>epoch</code>，每次按批产生训练数据集、前向计算，然后通过损失函数计算误差值，并反向传播自动计算梯度、更新网络参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers, losses</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile() 函数指定网络使用的优化器对象、损失函数类型、评价指标等参数。</span></span><br><span class="line">network.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=optimizers.Adam(learning_rate=<span class="number">0.001</span>), <span class="comment"># 采用 Adam 优化器，学习率设置为 0.001</span></span><br><span class="line">    loss=losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>), <span class="comment"># 使用交叉熵损失函数，</span></span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">) <span class="comment"># 模型装配</span></span><br></pre></td></tr></table></figure><h5 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h5><p>模型装配完成后，通过 <code>Model.fit()</code> 函数传入待训练和测试的数据集，其会返回训练过程中的数据记录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_db 为 tf.data.Dataset 对象；epochs 指定训练迭代的数量；validation_data 指定用于验证的数据集和验证的频率</span></span><br><span class="line">history = network.fit(train_db, epochs=<span class="number">5</span>, validation_data=val_db, validation_freq=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指标包含loss、测量指标等记录</span></span><br><span class="line">history.history</span><br></pre></td></tr></table></figure><h5 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h5><p>通过 <code>Model.predict()</code> 方法即可完成模型的预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(db_test))</span><br><span class="line">out = network.predict(x) <span class="comment"># 模型预测</span></span><br><span class="line">out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果仅测试模型的性能，通过 Model.evaulate() 循环测试数据集的所有样本，并打印性能指标</span></span><br><span class="line">network.evaulate(db_test)</span><br></pre></td></tr></table></figure><h4 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h4><p>模型在训练完成后，需要将模型保存到文件系统上，从而方便后续的模型测试与部署工作。 </p><p>在 <code>Keras</code> 中有三种常用的模型保存与加载方法：</p><ol><li><p><strong>张量方式</strong><br>网络的状态主要体现在网络结构以及网络层内部张量数据上，因此在拥有网络数据结构的前提下，直接保存网络张量参数是最轻量级的方式。通过 <code>Model.save_weights(path)</code> 将当前的网络参数保存到 <code>path</code> 文件上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network.save_weights(<span class="string">&#x27;path&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在需要使用网络参数时可以通过 <code>load_weights(path)</code> 加载保存的张量数据，但其需要使用相同的网络结构才能够正确恢复网络状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(<span class="number">10</span>)</span><br><span class="line">]) <span class="comment"># 构建网络层</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br><span class="line">network.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=optimizers.Adam(learning_rate=<span class="number">0.001</span>), <span class="comment"># 采用 Adam 优化器，学习率设置为 0.001</span></span><br><span class="line">    loss=losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>), <span class="comment"># 使用交叉熵损失函数，</span></span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">) <span class="comment"># 模型装配</span></span><br><span class="line">network.load_weights(<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>网络方式</strong><br>通过 <code>Model.save(path)</code> 即可将模型的结构和模型的参数保存到 <code>path</code> 文件中。<br>通过 <code>keras.models.load_model(path)</code> 可以恢复网络结构和网络参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">network.save(<span class="string">&#x27;path&#x27;</span>) <span class="comment"># 保存模型结构与模型参数</span></span><br><span class="line">network = keras.models.load_model(<span class="string">&#x27;path&#x27;</span>) <span class="comment"># 从文件加载模型结构与模型参数</span></span><br></pre></td></tr></table></figure></li><li><p><strong><code>SavedModel</code> 方式</strong><br><code>tf.saved_model.save(network, path)</code> 即可将模型以 <code>SaveModel</code> 的方式保存到 <code>path</code> 目录中，用户无需关心文件的保存格式。<br>通过 <code>tf.saved_model.load(path)</code> 函数可以实现从文件中恢复模型对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.saved_model.save(network, <span class="string">&#x27;path&#x27;</span>) <span class="comment"># 保存模型结构与模型参数到文件</span></span><br><span class="line">tf.saved_model.load(<span class="string">&#x27;path&#x27;</span>) <span class="comment"># 从文件中恢复模型对象</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="测量指标"><a href="#测量指标" class="headerlink" title="测量指标"></a>测量指标</h4><p><code>Keras</code> 提供了一些常用的指标测量工具，其位于 <code>keras.metrics</code> 模块中，专门用于统计训练过程中常用的指标数据。</p><p><code>Keras</code> 的指标测量工具使用方法一般分为四个主要步骤：<strong>新建测量器</strong>、<strong>写入数据</strong>、<strong>读取统计数据</strong>和<strong>清零测量器</strong>。</p><ol><li><p><strong>新建测量器</strong><br><code>keras.metrics</code> 模块中有常用的测量器类，例如平均值 <code>Mean</code> 类，准确率 <code>Accuracy</code> 类，余弦相似度 <code>CosineSimilarity</code> 类等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 适合 Loss 数据</span></span><br><span class="line">loss_meter = metrics.Mean()</span><br></pre></td></tr></table></figure></li><li><p><strong>写入数据</strong><br>通过测量器的 <code>update_state()</code> 函数写入新的数据，测量器会根据自身逻辑记录并处理采样数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 记录采样数据，通过 float() 函数将张量转换为普通数值</span></span><br><span class="line">loss_meter.update_state(<span class="built_in">float</span>(loss))</span><br></pre></td></tr></table></figure></li><li><p><strong>读取统计数据</strong><br>在多次采样数据后，可以在需要的地方调用测量器的 <code>result()</code> 函数来获取统计值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印统计提前的平均 loss</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;loss:&#x27;</span>, loss_meter.result())</span><br></pre></td></tr></table></figure></li><li><p><strong>清零测量器</strong><br>通过 <code>reset_states()</code> 函数即可实现清除状态功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss:&#x27;</span>, loss_meter.result())</span><br><span class="line">    loss_meter.reset_states() <span class="comment"># 打印完成后，清零测量器</span></span><br></pre></td></tr></table></figure></li><li><p><strong>实战</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用准确率测量器 Accuracy 类来统计训练过程中的准确率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建准确率测量器</span></span><br><span class="line">acc_meter = metrics.Accuracy()</span><br><span class="line"><span class="comment"># 将当前 batch 样本的标签和预测结果写入测量器</span></span><br><span class="line">out = network(x)</span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>)</span><br><span class="line">pred = tf.cast(pred, dtype=tf.int32)</span><br><span class="line">acc_meter.update_state(y, pred)</span><br><span class="line"><span class="comment"># 输出统计的平均准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;acc&#x27;</span>, acc_meter.result().numpy())</span><br><span class="line">acc_meter.reset_states()</span><br></pre></td></tr></table></figure></li></ol><h4 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h4><p>对于需要创建自定义逻辑的网络层，可以通过自定义类来实现。在创建自定义网络层类时，需要继承自 <code>layers.Layer</code> 基类；创建自定义网络类时，需要继承自 <code>keras.Model</code> 基类，这样建立的自定义类才能够方便地利用 <code>Layer/Model</code> 基类提供的参数管理等功能，同时也可以与其他标准网络层类交互使用。</p><h5 id="自定义网络层"><a href="#自定义网络层" class="headerlink" title="自定义网络层"></a>自定义网络层</h5><p>对于自定义网络层，至少需要实现初始化 <code>__init__</code> 方法和前向传播逻辑 <code>call</code> 方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建自定义类，并继承自 layers.Layer。创建初始化方法，并调用母类的初始化方法，由于是全连接层，因此需要设置两个参数：输入特征的长度和输出特征的长度。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDense</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inp_dim, outp_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDense, self).__init__()</span><br><span class="line">        <span class="comment"># 创建权值张量，并添加到类管理列表中，设置为需要优化</span></span><br><span class="line">        self.kernel = self.add_variable(<span class="string">&#x27;w&#x27;</span>, [inp_dim, outp_dim], trainable=<span class="literal">True</span>) <span class="comment"># trainable 参数是否需要被优化</span></span><br><span class="line">        self.t = self.Variable(tf.random.normal([inp_dim, outp_dim]), trainable=<span class="literal">False</span>) <span class="comment"># 通过 tf.Variable 定义的张量也会被纳入参数列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 实现自定义类的前向传播逻辑</span></span><br><span class="line">        out = inputs @ self.kernel <span class="comment"># X@W</span></span><br><span class="line">        out = tf.nn.relu(out) <span class="comment"># 执行激活函数运算</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = MyDense(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">net.trainble_variables, net.variables <span class="comment"># 查看自定义层的参数列表</span></span><br></pre></td></tr></table></figure><h5 id="自定义网络-1"><a href="#自定义网络-1" class="headerlink" title="自定义网络"></a>自定义网络</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义网络类，需要继承自 keras.Model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(keras.Model):</span><br><span class="line">    <span class="comment"># 完成网络内需要的网络层的创建工作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.fc1 = MyDense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc2 = MyDense(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc3 = MyDense(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc4 = MyDense(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.fc5 = MyDense(<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 自定义前向传播逻辑</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.fc1(inputs)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        x = self.fc5(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>之前在学习机器学习内容的时候，感觉好难啊，我怎么什么都不会，但自从开始看深度学习相关的内容，就发现好些东西突然醒悟，之前好多不理解的东西也能理解，哈哈哈哈哈哈哈。这就是学习新东西的魅力。<br>总之就是持续学习，继续进步。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习《TensorFlow深度学习》所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;TensorFlow&lt;/code&gt;&lt;/strong&gt; 是由 &lt;code&gt;Google&lt;/code&gt; 团队开发的深度学习框架，其初衷是以最简单的方式实现机器学习和深度学习的概念。该框架融合了计算代数的优化技术，极大地方便了复杂数学表达式的计算。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TensorFlow&lt;/code&gt; 深度学习框架的三大核心功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;加速计算&lt;/strong&gt;。神经网络本质上由大量的矩阵相乘、矩阵相加等基本数学运算构成，&lt;code&gt;TensorFlow&lt;/code&gt; 的重要功能就是利用 &lt;code&gt;GPU&lt;/code&gt; 方便地实现并行计算加速功能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动梯度&lt;/strong&gt;。&lt;code&gt;TensorFlow&lt;/code&gt; 可以自动构建计算图，通过 &lt;code&gt;TensorFlow&lt;/code&gt; 提供的自动求导的功能，不需要手动推导即可计算输出对网络参数的偏导数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;常用神经网络接口&lt;/strong&gt;。&lt;code&gt;TensorFlow&lt;/code&gt; 除了提供底层的矩阵相乘、相加等数学函数，还包含常用神经网络运算函数、常用网络层、网络训练、模型保存与加载、网络部署等一系列深度学习的功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单示例：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = tf.constant(&lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = tf.constant(&lt;span class=&quot;number&quot;&gt;4.0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&amp;#x27;a+b=&amp;#x27;&lt;/span&gt;, a+b) &lt;span class=&quot;comment&quot;&gt;# a+b= tf.Tensor(6.0, shape=(), dtype=float32)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 运算时同时创建计算图 𝑐=𝑎+𝑏 和数值结果 6.0=2.0+4.0 的方式叫做命令式编程，也称为动态图模式。&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="DL" scheme="https://blog.vgbhfive.cn/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>量化投资实践</title>
    <link href="https://blog.vgbhfive.cn/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/"/>
    <id>https://blog.vgbhfive.cn/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/</id>
    <published>2024-03-24T05:43:41.000Z</published>
    <updated>2024-04-14T15:52:48.289Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前序"><a href="#前序" class="headerlink" title="前序"></a>前序</h3><p>本篇文章其目的在于实践<a href="https://blog.vgbhfive.com/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/">《量化投资》</a>而特意编写，内容主要包含<strong>收集数据</strong>、<strong>整理和分析数据</strong>、<strong>因子指标计算</strong>、<strong>策略制定</strong>、<strong>量化回测</strong>。</p><p>阅读本篇需要一定的 <code>Python</code> 基础、金融基础知识和计算机编程经验，各位予取予求。</p><span id="more"></span><hr><h3 id="数据收集及整理"><a href="#数据收集及整理" class="headerlink" title="数据收集及整理"></a>数据收集及整理</h3><p>本次数据采用的是中国平安近一年的历史股票数据，其数据指标包含<strong>开盘价</strong>、<strong>最高价</strong>、<strong>最低价</strong>、<strong>收盘价</strong>、<strong>成交量</strong>、<strong>成交额</strong>、<strong>换手率</strong>。</p><h4 id="获取历史数据"><a href="#获取历史数据" class="headerlink" title="获取历史数据"></a>获取历史数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="keyword">from</span> browsermobproxy <span class="keyword">import</span> Server</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 股票每日价格</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 股票数据保存 csv 文件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_basic_data_to_csv</span>(<span class="params">code, daliy_data</span>):</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(daliy_data) &lt;= <span class="number">0</span>:</span><br><span class="line"><span class="built_in">print</span>(code + <span class="string">&#x27; certificate data length is &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(daliy_data)))</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">col_data = &#123;&#125;</span><br><span class="line"><span class="comment"># 确定数据结构</span></span><br><span class="line">col_num = <span class="built_in">len</span>(daliy_data[<span class="number">0</span>].split(<span class="string">&#x27;,&#x27;</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(col_num):</span><br><span class="line">col_data[i] = []</span><br><span class="line"><span class="comment"># 改造数据结构</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> daliy_data:</span><br><span class="line">rows = data.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(rows)):</span><br><span class="line">col_data[i].append(rows[i])</span><br><span class="line"><span class="comment"># save csv</span></span><br><span class="line">data = pd.DataFrame(col_data)</span><br><span class="line">data.to_csv(<span class="built_in">str</span>(code)+<span class="string">&#x27;.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">code</span>):</span><br><span class="line">url = url_where(code)</span><br><span class="line"><span class="comment"># 启动代理</span></span><br><span class="line">server = Server(<span class="string">&#x27;selenium_install_path/browsermob-proxy-2.1.4/bin/browsermob-proxy&#x27;</span>) <span class="comment"># 此处配置安装的 selenium 路径</span></span><br><span class="line">server.start()</span><br><span class="line">proxy = server.create_proxy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动浏览器</span></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--ignore-certificate-errors&#x27;</span>)</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--proxy-server=&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(proxy.proxy))</span><br><span class="line"><span class="comment"># chrome_options.add_argument(&#x27;--headless&#x27;)  # 无头模式</span></span><br><span class="line">browser = webdriver.Chrome(options=chrome_options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 监听结果</span></span><br><span class="line">proxy.new_har(options=&#123;<span class="string">&#x27;captureContent&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;captureHeaders&#x27;</span>: <span class="literal">True</span>&#125;)</span><br><span class="line">browser.get(url)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">new_har = proxy.har</span><br><span class="line"><span class="keyword">for</span> entry <span class="keyword">in</span> new_har[<span class="string">&#x27;log&#x27;</span>][<span class="string">&#x27;entries&#x27;</span>]:</span><br><span class="line">url = entry[<span class="string">&#x27;request&#x27;</span>][<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;qt/stock/kline/get&#x27;</span> <span class="keyword">in</span> url:</span><br><span class="line">header_data = entry[<span class="string">&quot;request&quot;</span>][<span class="string">&quot;headers&quot;</span>]</span><br><span class="line"><span class="comment"># 格式化headers</span></span><br><span class="line">headers = &#123;item[<span class="string">&#x27;name&#x27;</span>]: item[<span class="string">&#x27;value&#x27;</span>] <span class="keyword">for</span> item <span class="keyword">in</span> header_data&#125;</span><br><span class="line">headers.pop(<span class="string">&#x27;Accept-Encoding&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(headers)</span><br><span class="line"><span class="comment"># 使用headers 发送请求</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line"><span class="comment"># 格式化处理返回体</span></span><br><span class="line">content = <span class="built_in">str</span>(response.content)</span><br><span class="line">content = content[content.index(<span class="string">&#x27;&#123;&#x27;</span>): content.index(<span class="string">&#x27;&#125;&#x27;</span>)+<span class="number">2</span>]</span><br><span class="line">content = content.replace(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;\\\\&#x27;</span>)</span><br><span class="line"><span class="comment"># print(content)</span></span><br><span class="line">data = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;code&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(data[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;klines&#x27;</span>]))</span><br><span class="line"><span class="comment"># save data</span></span><br><span class="line">save_basic_data_to_csv(data[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;code&#x27;</span>], data[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;klines&#x27;</span>])</span><br><span class="line"><span class="comment"># 关闭代理和浏览器</span></span><br><span class="line">proxy.close()</span><br><span class="line">browser.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">url_where</span>(<span class="params">code</span>):</span><br><span class="line">url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">prefix = code[<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">nowTime = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>)</span><br><span class="line">preTime = nowTime - random.randint(<span class="number">3</span>, <span class="number">10</span>) * <span class="number">1000</span></span><br><span class="line"><span class="keyword">if</span> prefix == <span class="string">&#x27;6&#x27;</span>: <span class="comment"># sh</span></span><br><span class="line">url = <span class="string">&#x27;https://push2his.eastmoney.com/api/qt/stock/kline/get?cb=jQuery35109638153987050799_&#x27;</span> + <span class="built_in">str</span>(nowTime) + <span class="string">&#x27;&amp;secid=1.&#x27;</span> + code + <span class="string">&#x27;&amp;ut=fa5fd1943c7b386f172d6893dbfba10b&amp;fields1=f1%2Cf2%2Cf3%2Cf4%2Cf5%2Cf6&amp;fields2=f51%2Cf52%2Cf53%2Cf54%2Cf55%2Cf56%2Cf57%2Cf58%2Cf59%2Cf60%2Cf61&amp;klt=101&amp;fqt=1&amp;end=20500101&amp;lmt=1000000&amp;_=&#x27;</span> + <span class="built_in">str</span>(preTime)</span><br><span class="line"><span class="keyword">elif</span> prefix == <span class="string">&#x27;0&#x27;</span> <span class="keyword">or</span> prefix == <span class="string">&#x27;3&#x27;</span> <span class="keyword">or</span> prefix == <span class="string">&#x27;4&#x27;</span> <span class="keyword">or</span> prefix == <span class="string">&#x27;8&#x27;</span>: <span class="comment"># sz bj</span></span><br><span class="line">url = <span class="string">&#x27;https://push2his.eastmoney.com/api/qt/stock/kline/get?cb=jQuery35109638153987050799_&#x27;</span> + <span class="built_in">str</span>(nowTime) + <span class="string">&#x27;&amp;secid=0.&#x27;</span> + code + <span class="string">&#x27;&amp;ut=fa5fd1943c7b386f172d6893dbfba10b&amp;fields1=f1%2Cf2%2Cf3%2Cf4%2Cf5%2Cf6&amp;fields2=f51%2Cf52%2Cf53%2Cf54%2Cf55%2Cf56%2Cf57%2Cf58%2Cf59%2Cf60%2Cf61&amp;klt=101&amp;fqt=1&amp;end=20500101&amp;lmt=1000000&amp;_=&#x27;</span> + <span class="built_in">str</span>(preTime)</span><br><span class="line"><span class="keyword">return</span> url</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">data(<span class="string">&#x27;000001&#x27;</span>)</span><br></pre></td></tr></table></figure><p>原始数据格式如下：<br><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/screenshot-20240326-202930.png" alt="screenshot-20240326-202930"></p><h4 id="整理数据"><a href="#整理数据" class="headerlink" title="整理数据"></a>整理数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./000001.csv&#x27;</span>)</span><br><span class="line">df = df.loc[:, [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;10&#x27;</span>]]</span><br><span class="line">df.columns = [<span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>, <span class="string">&#x27;high&#x27;</span>, <span class="string">&#x27;low&#x27;</span>, <span class="string">&#x27;volume&#x27;</span>, <span class="string">&#x27;volume_amount&#x27;</span>, <span class="string">&#x27;turnover_rate&#x27;</span>]</span><br><span class="line"></span><br><span class="line">df.set_index(pd.to_datetime(df.date), inplace=<span class="literal">True</span>)</span><br><span class="line">df.drop(<span class="string">&#x27;date&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df = df[datetime.datetime(<span class="number">2023</span>, <span class="number">3</span>, <span class="number">27</span>):]</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p>数据经过预处理之后的格式如下：<br><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/Snipaste_2024-04-14_23-51-28.jpg" alt="Snipaste_2024-04-14_23-51-28"></p><hr><h3 id="因子计算"><a href="#因子计算" class="headerlink" title="因子计算"></a>因子计算</h3><p>本次使用 <strong>简单移动平均线 <code>SMA</code></strong> 因子来构建量化策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;ma5&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].rolling(<span class="number">5</span>).mean()</span><br><span class="line">df[<span class="string">&#x27;ma20&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].rolling(<span class="number">20</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">sns.lineplot(data=df.loc[:, [<span class="string">&#x27;ma5&#x27;</span>, <span class="string">&#x27;ma20&#x27;</span>]])</span><br></pre></td></tr></table></figure><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/screenshot-20240328-171714.png" alt="screenshot-20240328-171714"></p><h4 id="标记行动节点"><a href="#标记行动节点" class="headerlink" title="标记行动节点"></a>标记行动节点</h4><p>有了前面计算的 <strong>简单移动平均线 <code>SMA</code></strong> 因子数据，之后根据上一篇文章中介绍的均线八大买卖法则来标记买入和卖出的节点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">df.loc[(df[<span class="string">&#x27;ma5&#x27;</span>] &gt; df[<span class="string">&#x27;ma20&#x27;</span>]), <span class="string">&#x27;signal&#x27;</span>] = <span class="number">1</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;ma5&#x27;</span>] &lt; df[<span class="string">&#x27;ma20&#x27;</span>]), <span class="string">&#x27;signal&#x27;</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;order&#x27;</span>] = df[<span class="string">&#x27;signal&#x27;</span>].diff()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化交易节点</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">12</span>))</span><br><span class="line">plt.plot(df[<span class="string">&#x27;close&#x27;</span>], color=<span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;close&#x27;</span>)</span><br><span class="line">plt.plot(df[<span class="string">&#x27;ma5&#x27;</span>], ls=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;gray&#x27;</span>, label=<span class="string">&#x27;ma5&#x27;</span>)</span><br><span class="line">plt.plot(df[<span class="string">&#x27;ma20&#x27;</span>], ls=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;k&#x27;</span>, label=<span class="string">&#x27;ma20&#x27;</span>)</span><br><span class="line">plt.scatter(df.loc[df[<span class="string">&#x27;order&#x27;</span>]==<span class="number">1</span>].index, df[<span class="string">&#x27;close&#x27;</span>][df.order==<span class="number">1</span>], marker=<span class="string">&#x27;^&#x27;</span>, s=<span class="number">100</span>, color=<span class="string">&#x27;m&#x27;</span>, label=<span class="string">&#x27;in&#x27;</span>)</span><br><span class="line">plt.scatter(df.loc[df[<span class="string">&#x27;order&#x27;</span>]==-<span class="number">1</span>].index, df[<span class="string">&#x27;close&#x27;</span>][df.order==-<span class="number">1</span>], marker=<span class="string">&#x27;v&#x27;</span>, s=<span class="number">100</span>, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;out&#x27;</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">20</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">20</span>)</span><br><span class="line">plt.legend(fontsize=<span class="number">20</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/screenshot-20240409-113918.png" alt="screenshot-20240409-113918"></p><hr><h3 id="量化回测"><a href="#量化回测" class="headerlink" title="量化回测"></a>量化回测</h3><p>在得知了有效的量化策略以后，那我们就该在模拟盘中进行回测。</p><h4 id="策略模型"><a href="#策略模型" class="headerlink" title="策略模型"></a>策略模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtrader <span class="keyword">as</span> bt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入大脑，并实例化</span></span><br><span class="line">cerebro = bt.Cerebro()</span><br><span class="line"><span class="comment"># 数据样本</span></span><br><span class="line">datafeed = bt.feeds.PandasData(dataname=df, fromdate=datetime.datetime(<span class="number">2023</span>,<span class="number">3</span>,<span class="number">27</span>), todate=datetime.datetime(<span class="number">2024</span>,<span class="number">3</span>,<span class="number">27</span>))</span><br><span class="line"><span class="comment"># 通过cerebro.adddata添加给大脑，并通过 name 赋值 实现数据集与股票的一一对应</span></span><br><span class="line">cerebro.adddata(datafeed, name=<span class="string">&#x27;000001.SH&#x27;</span>) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;读取成功&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置起始资金</span></span><br><span class="line">cerebro.broker.setcash(<span class="number">1000000.0</span>)</span><br><span class="line"><span class="comment"># ma_qt 为此量化策略的名称</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ma_qt</span>(bt.Strategy):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 全局只调用一次，一般在此函数中计算指标数据。</span></span><br><span class="line">        self.ma_5 = bt.ind.SMA(self.data0.close, period=<span class="number">5</span>)</span><br><span class="line">        self.ma_20 = bt.ind.SMA(self.data0.close, period=<span class="number">20</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#计算买卖条件</span></span><br><span class="line">        <span class="keyword">if</span> self.getposition(self.data).size == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 通过此函数查询持仓，若持仓等于0</span></span><br><span class="line">            <span class="keyword">if</span> self.ma_5[<span class="number">0</span>] &gt; self.ma_20[<span class="number">0</span>] <span class="keyword">and</span> self.ma_5[-<span class="number">1</span>] &lt; self.ma_20[-<span class="number">1</span>]:</span><br><span class="line">            <span class="comment"># 当前一日接近下轨，当日跌破下轨时买入</span></span><br><span class="line">                self.order = self.buy(self.data, <span class="number">100</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;买入 100 股成功&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.getposition(self.data).size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 持仓大于0时</span></span><br><span class="line">            <span class="keyword">if</span> self.ma_20[<span class="number">0</span>] &gt; self.ma_5[<span class="number">0</span>] <span class="keyword">and</span> self.ma_20[-<span class="number">1</span>] &lt; self.ma_5[-<span class="number">1</span>]:</span><br><span class="line">            <span class="comment"># 前一期小于上轨，当日突破上轨卖出</span></span><br><span class="line">                self.order = self.close(self.data, <span class="number">100</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;卖出 100 股成功&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">cerebro.broker.set_slippage_perc(perc=<span class="number">0.002</span>)       </span><br><span class="line">cerebro.broker.setcommission(commission=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 将策略添加给大脑    </span></span><br><span class="line">cerebro.addstrategy(ma_qt)</span><br><span class="line"><span class="comment"># 添加分析指标</span></span><br><span class="line"><span class="comment"># 返回年初至年末的年度收益率</span></span><br><span class="line">cerebro.addanalyzer(bt.analyzers.AnnualReturn, _name=<span class="string">&#x27;_AnnualReturn&#x27;</span>)</span><br><span class="line"><span class="comment"># 计算最大回撤相关指标</span></span><br><span class="line">cerebro.addanalyzer(bt.analyzers.DrawDown, _name=<span class="string">&#x27;_DrawDown&#x27;</span>)</span><br><span class="line"><span class="comment"># 计算年化夏普比率：</span></span><br><span class="line">cerebro.addanalyzer(bt.analyzers.SharpeRatio_A, _name=<span class="string">&#x27;_SharpeRatio_A&#x27;</span>)</span><br><span class="line"><span class="comment"># 返回收益率时</span></span><br><span class="line">cerebro.addanalyzer(bt.analyzers.TimeReturn,_name=<span class="string">&#x27;_TimeReturn&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行</span></span><br><span class="line">result=cerebro.run()</span><br></pre></td></tr></table></figure><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/screenshot-20240409-142801.png" alt="screenshot-20240409-142801"></p><h4 id="分析器"><a href="#分析器" class="headerlink" title="分析器"></a>分析器</h4><ol><li>最终收益率<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最终收益率</span></span><br><span class="line">ret = pd.Series(result[<span class="number">0</span>].analyzers._TimeReturn.get_analysis()) <span class="comment"># 获得收益时序图</span></span><br><span class="line">(ret+<span class="number">1</span>).cumprod()</span><br></pre></td></tr></table></figure></li></ol><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/screenshot-20240409-142818.png" alt="screenshot-20240409-142818"></p><ol start="2"><li>最大回撤<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看最大回撤</span></span><br><span class="line">result[<span class="number">0</span>].analyzers._DrawDown.get_analysis()[<span class="string">&#x27;max&#x27;</span>][<span class="string">&#x27;drawdown&#x27;</span>] * (-<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%AE%9E%E8%B7%B5/screenshot-20240409-142826.png" alt="screenshot-20240409-142826"></p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>股市有风险，入市需谨慎！</li><li>股市有风险，入市需谨慎！</li><li>股市有风险，入市需谨慎！</li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;前序&quot;&gt;&lt;a href=&quot;#前序&quot; class=&quot;headerlink&quot; title=&quot;前序&quot;&gt;&lt;/a&gt;前序&lt;/h3&gt;&lt;p&gt;本篇文章其目的在于实践&lt;a href=&quot;https://blog.vgbhfive.com/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/&quot;&gt;《量化投资》&lt;/a&gt;而特意编写，内容主要包含&lt;strong&gt;收集数据&lt;/strong&gt;、&lt;strong&gt;整理和分析数据&lt;/strong&gt;、&lt;strong&gt;因子指标计算&lt;/strong&gt;、&lt;strong&gt;策略制定&lt;/strong&gt;、&lt;strong&gt;量化回测&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;阅读本篇需要一定的 &lt;code&gt;Python&lt;/code&gt; 基础、金融基础知识和计算机编程经验，各位予取予求。&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
    <category term="Quantum" scheme="https://blog.vgbhfive.cn/tags/Quantum/"/>
    
  </entry>
  
  <entry>
    <title>量化投资</title>
    <link href="https://blog.vgbhfive.cn/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/"/>
    <id>https://blog.vgbhfive.cn/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/</id>
    <published>2024-01-31T14:10:10.000Z</published>
    <updated>2024-03-03T08:26:40.765Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><strong>量化</strong>是指一种投资交易策略，利用统计学、数学、计算机、人工智能等方法取代传统人工做出决策，通过模型来实现资产交易进而构建投资组合。即利用计算机技术和数学模型来实现投资策略的过程。</p><p>主观投资与量化投资的对比：</p><table><thead><tr><th>主观投资</th><th>量化投资</th></tr></thead><tbody><tr><td>基于基金经理的判断</td><td>基于模型运算的客观结果</td></tr><tr><td>基金经理对宏观环境、行业、公司的研究，预测未来趋势</td><td>核心在于利用计算机技术在海量数据中寻找到投资规律</td></tr><tr><td>注重研究深度，对少数资产进行深入研究</td><td>注重广度，全市场筛选标的，多维度分析</td></tr><tr><td>持股集中，稳定性略差</td><td>持股分散，组合投资</td></tr><tr><td>交易依靠主管认知与判断</td><td>模型自主下单</td></tr></tbody></table><span id="more"></span><p>优势：</p><ul><li>投资范围广泛。借助计算机技术，可以分析整体市场，促进更多的投资机会。</li><li>程序化交易，避免主观因素干扰。通过回测验证策略的有效性和可行性，自助下单，程序化交易。</li><li>数据处理快速响应，创造交易价值。利用计算机技术，分析海量数据，支持高频交易，可及时验证每个决策后模型的有效性。</li></ul><p>风险：</p><ul><li>策略失效。量化投资最大的风险是策略失效，但更大的风险在于策略在何时失效。</li><li>流通性风险。量化投资由于很多因素很容易导致同质化，带来的问题就是会产生共振，更容易产生系统性的风险。</li><li>模型本身风险。量化投资需要借助模型，而建立模型需要设定各种参数，由于参数的不确定性可能会导致巨大损失。</li></ul><p>一般流程：</p><ul><li>策略设计。基于金融理论、历史数据或其他分析方法，构建量化投资策略。</li><li>回测验证。使用历史数据对策略进行回测，检验策略的有效性和可行性，以及找到优化策略的方法。</li><li>模拟盘验证。使用虚拟账户和资金进行模拟交易，检验策略在实际市场中的表现，根据反馈及时调整和优化策略。</li><li>实盘交易。经过前面的验证和优化后，将策略投入实际交易中执行。</li></ul><p>量化投资的策略设计和实施过程需要严谨、科学、系统化的方法，同时也需要一定的技术和数学功底。成功的量化投资不仅仅依赖于策略的设计，也需要严格的风险控制和资金管理。</p><hr><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><h4 id="资产数据"><a href="#资产数据" class="headerlink" title="资产数据"></a>资产数据</h4><p>资产数据信息根据来源和分析方法的不同，可以分为两类：</p><ul><li>技术面数据。技术面数据主要是根据资产的历史价格和交易量等市场数据进行分析和计算得出的指标信息。</li><li>基本面数据。通过分析公司的财务状况、业绩表现、竞争力等信息得出的评估信息。</li></ul><h4 id="技术面常见指标"><a href="#技术面常见指标" class="headerlink" title="技术面常见指标"></a>技术面常见指标</h4><h5 id="移动平均线-MA"><a href="#移动平均线-MA" class="headerlink" title="移动平均线 MA"></a>移动平均线 <code>MA</code></h5><p><strong>移动平均线</strong>是通过计算一段时间内的资产平均价格来平滑价格波动。</p><p>常见的移动平均线有两种：</p><ul><li><strong>简单移动平均线（<code>SMA</code>）</strong>，最基本的移动平均线类型，通过将一段时间内的资产收盘价相加，然后除以时间段的天数来计算。简单移动平均线可以平滑价格波动，显示出长期趋势。</li><li><strong>指数移动平均线（<code>EMA</code>）</strong>，对近期价格给予更高的权重，可以反映市场更近期的变化。计算指数移动平均线时，当前价格会根据选定的时间段和权重系数，与之前的移动平均线值相结合。</li></ul><p>移动平均线的应用主要有以下几个方面：</p><ul><li><strong>确定趋势</strong>：使用不同期限的移动平均线来确定<strong>趋势的强度和方向</strong>。</li><li><strong>交叉信号</strong>：移动平均线的交叉均可以提供<strong>买入和卖出的信号</strong>。</li><li><strong>支撑与阻力线</strong>：移动平均线经常被称为<strong>支撑线</strong>和<strong>阻力线</strong>的参考。</li></ul><p>均线八大买卖法则：</p><ul><li>买点 <code>1</code>：均线整体上行，价格由下至上上穿均线，此为黄金交叉，形成第一个买点。</li><li>买点 <code>2</code>：价格出现下跌迹象，但尚未跌破均线，此时均线成为支撑线，形成第二个买点。</li><li>买点 <code>3</code>：价格仍处于均线上方，但呈现急剧下跌趋势，当跌破均线时，形成第三个买点。</li><li>买点 <code>4</code>：价格和均线均处于下降通道，且价格处于均线下方，严重远离均线，出现第四个买点。</li><li>卖点 <code>1</code>：均线由上升状态变为缓慢下降状态，价格开始下跌，当价格跌破均线时，此为死亡交叉点，形成第一个卖点。</li><li>卖点 <code>2</code>：价格仍处于均线之下，但价格开始呈现上涨趋势，当价格无限接近均线但尚未突破时，此时均线变为阻力线，形成第二个卖点。</li><li>卖点 <code>3</code>：价格终于突破均线，处于均线上方，但持续时间不长，价格开始下跌，直至再一次跌破均线，形成第三个卖点。</li><li>卖点 <code>4</code>：价格和均线均在上涨，价格上涨速度大于均线上涨速度，当价格严重便宜均线时，出现第四个卖点。</li></ul><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-03_15-37-16.jpg" alt="Snipaste_2024-02-03_15-37-16.jpg"></p><h5 id="相对强弱指数-RSI"><a href="#相对强弱指数-RSI" class="headerlink" title="相对强弱指数 RSI"></a>相对强弱指数 <code>RSI</code></h5><p><strong><code>RSI</code></strong> 是一种用于衡量资产价格变动强度和速度的技术指标。可以帮助投资者判断资产市场的超买和超卖情况，以及价格的反转和确认信号。<code>RSI</code> 是一种短期的指标，对价格的反应较为敏感。</p><p><code>RSI</code> 的计算基于一定时期内资产价格的平均涨幅和跌幅。通常情况下 <code>RSI</code>的取值范围在 <code>0~100</code> 之间。<br><code>RSI</code> 指标的常用参数为 <code>14</code>，计算步骤如下：</p><ul><li>首先计算 <code>14</code> 个交易周期内涨幅和跌幅的平均值。</li><li>计算涨幅平均值与跌幅平均值的相对强弱比率（<code>Relative Strength, RS</code>）。<br><code>RS</code> &#x3D; 交易周期内涨幅平均值 &#x2F; 交易周期内跌幅平均值</li><li>计算相对强弱指数（<code>RSI</code>）。<br><code>RSI = 100 - (100 / (1 + RS))</code></li></ul><p><code>RSI</code> 的数值解读如下：</p><ul><li><strong>指数在 <code>0~30</code> 之间</strong>：表示资产市场被超卖，可能存在价格反弹的机会。</li><li><strong>指数在 <code>70~100</code> 之间</strong>：表示资产市场被超买，可能存在价格下跌的机会。</li><li><strong>指数在 <code>30~70</code> 之间</strong>：表示资产市场相对平稳，没有明显的超买或超卖信号。</li></ul><h5 id="MACD"><a href="#MACD" class="headerlink" title="MACD"></a><code>MACD</code></h5><p><strong><code>MACD</code></strong> 指标是资产技术分析中常用的趋势追踪和买卖信号指标，其通过比较两条移动平均线的差异，来判断资产价格的趋势以及价格的买卖信号。<code>MACD</code> 指标是一种相对较慢的指标，较长时间的移动平均线会导致滞后性。</p><p><code>MACD</code> 指标由以下几个元素组成：</p><ul><li><strong><code>DIF</code>（<code>Difference Line</code>）</strong>：短期指数移动平均线减去长期指数移动平均线得到的差值线。<code>DIF</code> 线可以较为敏感地反应价格的短期波动。</li><li><strong><code>DEA</code>（<code>Signal Line</code>）</strong>：对 <code>DIF</code> 线进行平滑处理，一般使用 <code>DIF</code> 线的 <code>9</code> 日移动平均线得到。<code>DEA</code> 线可以平滑 <code>DIF</code> 线的波动，更好地体现价格的中期趋势。</li><li><strong><code>MACD</code>（<code>MACD Histogram</code>）</strong>： <code>DIF</code> 线与 <code>DEA</code> 线的差值，可将价格的快速波动变化显示为柱状图。</li></ul><p><code>MACD</code> 指标的应用主要包括以下几个方面：</p><ul><li><strong>趋势判断</strong>：当 <code>DIF</code> 线与 <code>DEA</code> 线发生金叉（<code>DIF</code> 线向上穿过 <code>DEA</code> 线）时，表示价格可能出现上涨趋势；当 <code>DIF</code> 线与 <code>DEA</code> 线发生死叉（<code>DIF</code> 线向下穿过 <code>DEA</code> 线）时，表示价格可能出现下跌趋势。</li><li><strong>买卖信号</strong>：当 <code>MACD</code> 柱由负值转为正值时，被视为买入信号；当 <code>MACD</code> 柱由正值转为负值时，被视为卖出信号。这些转折点可能表示价格快速波动的转变。</li><li><strong>背离信号</strong>：观察价格和 <code>MACD</code> 指标的背离情况。例如，当价格创新高而 <code>MACD</code> 指标未能创新高时，可能表示价格上涨动能下降，可能出现价格回调。</li></ul><h5 id="成交量"><a href="#成交量" class="headerlink" title="成交量"></a>成交量</h5><p><strong>成交量</strong>指标衡量了资产交易的活跃程度。当资产价格上涨时，成交量增加可以视为价格上涨的确认，而当资产价格下跌时，成交量增加可能表示价格下跌的确认。成交量指标相对于价格指标而言，属于非价格类指标。</p><p>成交量指标的计算非常简单，即某一时间周期内的总成交量。</p><p>成交量指标的应用主要包括以下几个方面：</p><ul><li><strong>确认趋势</strong>：成交量指标可以用来确认价格趋势的有效性。在上涨趋势中，成交量增加可能意味着买盘增加，市场看涨情绪强烈。而在下跌趋势中，成交量增加可能意味着卖盘增加，市场看跌情绪强烈。如果价格和成交量出现背离，即价格上涨而成交量下降，或价格下跌而成交量增加，可能意味着趋势反转的可能性。</li><li><strong>确认突破</strong>：成交量指标也可用于确认价格突破的有效性。当价格突破重要的价格水平（如支撑或阻力线）时，如果成交量也大幅增加，可能预示着价格突破的力度和可持续性增加。</li><li><strong>观察分析</strong>：通过观察成交量指标的变化，研究市场情绪和买卖压力。</li><li><strong>交易量模型</strong>：使用成交量指标构建交易量模型，例如量价分析（<code>Volume Price Analysis</code>）或成交量波动指标（<code>Volume Oscillator</code>），这些模型通过比较成交量与价格的关系，以此寻找特定的交易信号。</li></ul><h5 id="随机指标"><a href="#随机指标" class="headerlink" title="随机指标"></a>随机指标</h5><p><strong>随机指标</strong>用于判断资产价格的超买和超卖情况，以及价格反转的可能性。可以帮助确定适合买入或卖出资产的时机。随机指标是一种短期的技术指标，可以辅助投资者判断市场价格走势和买卖时机，但并不是绝对准确的。</p><p>随机指标的计算基于一段时间内的收盘价与最高价和最低价的关系。该指标通常使用 <code>%K</code> 线和 <code>%D</code> 线两条线，并结合超买区和超卖区进行解读。</p><p>随机指标的计算步骤如下：</p><ul><li>首先计算一定时间段（常见为 <code>14</code> 个交易日）内的最高价和最低价。</li><li>计算当前收盘价与该时间段内最低价的差值（收盘价-最低价），并除以最高价和最低价的差值（最高价-最低价）。<br> <code>%K</code> &#x3D; （收盘价 - 最低价）&#x2F; （最高价 - 最低价） * <code>100</code></li><li>计算 <code>%K</code> 的移动平均值作为 <code>%D</code> 线的值，通常使用 <code>3</code> 日平均。<br> <code>%D</code> &#x3D; <code>%K</code> 的 <code>3</code> 日简单移动平均线</li></ul><p>随机指标的数值解读如下：</p><ul><li>当 <code>%K</code> 线从下方穿越 <code>%D</code> 线时，被视为买入信号，可能预示着价格的反转和上涨。</li><li>当 <code>%K</code> 线从上方穿越 <code>%D</code> 线时，被视为卖出信号，可能预示着价格的反转和下跌。</li><li>当 <code>%K</code> 线位于高位（一般超过<code>80</code>）时，表示市场可能超买，价格的下跌风险增加。</li><li>当 <code>%K</code> 线位于低位（一般低于<code>20</code>）时，表示市场可能超卖，价格的上涨机会增加。</li></ul><h4 id="基本面常见指标"><a href="#基本面常见指标" class="headerlink" title="基本面常见指标"></a>基本面常见指标</h4><h5 id="每股收益"><a href="#每股收益" class="headerlink" title="每股收益"></a>每股收益</h5><p><strong>每股收益</strong>用于衡量公司每股可供股东分配的净利润，即每股盈利能力。每股收益是投资者评估公司盈利能力和估值的重要参考指标。</p><p>每股收益的计算公式为：<code>EPS</code> &#x3D; 净利润 &#x2F; 流通股本。<br><small>净利润是指公司在一定会计周期内所创造的净收益，即扣除各项费用和税后利润。流通股本是指公司已经发行并在市场上自由交易的资产数量。</small></p><p>每股收益指标的应用主要包括以下几个方面：</p><ul><li><strong>估值比较</strong>：每股收益可以作为比较不同公司的盈利能力和估值水平的重要依据。通常情况下，具有更高每股收益的公司往往被认为具有更好的盈利能力。</li><li><strong>成长趋势</strong>：观察每股收益的变化趋势可以了解公司盈利能力的增长速度和稳定性。持续增长的每股收益可能意味着公司业绩好于预期，具有良好的成长潜力。</li><li><strong>盈利稳定性</strong>：通过比较每股收益的波动程度，可以判断公司的盈利稳定性。较小的波动通常被认为是公司盈利稳定性较好的表现。</li></ul><h5 id="市净率"><a href="#市净率" class="headerlink" title="市净率"></a>市净率</h5><p><strong>市净率</strong>也称为股价净资产比，是一种用于衡量当前市场价格与每股净资产之间关系的指标，是用来评估公司的市场估值是否低估或高估的重要指标。</p><p>市净率的计算公式为：市净率 &#x3D; 总市值 &#x2F; 净资产。<br><small>总市值是指公司所有已发行资产的市值之和，而净资产是指公司的总资产减去总负债。</small></p><p>市净率是一个相对指标，一般用来比较同行业或同一市场的公司。较低的市净率通常被认为是公司被低估的迹象，而较高的市净率可能意味着公司被高估。<br>根据市净率可以得出以下几个判断：</p><ul><li><strong>市净率低于1</strong>：通常表示公司的市场价值低于其净资产，资产可能被低估。</li><li><strong>市净率约等于1</strong>：表示公司的市场价值大致等于其净资产，资产被市场公平估价。</li><li><strong>市净率高于1</strong>：通常表示公司的市场价值高于其净资产，资产可能被高估。</li></ul><h5 id="股息收益率"><a href="#股息收益率" class="headerlink" title="股息收益率"></a>股息收益率</h5><p><strong>股息收益率</strong>用于衡量资产派发的股息相对于资产的价格的比率。股息是公司利润的一部分，以现金或资产形式派发给股东，股息收益率可以帮助投资者评估持有一只资产所能获得的现金回报。</p><p>股息收益率的计算公式为：股息收益率 &#x3D; 每股股息 &#x2F; 资产价格。<br><small>每股股息是指公司每股派发的股息金额，资产价格是资产在市场上的交易价格。</small></p><p>股息收益率指标的应用主要包括以下几个方面：</p><ul><li><strong>现金回报</strong>：股息收益率可以帮助投资者了解持有资产所能获得的现金回报，尤其对于偏好分红收入的投资者而言，具有重要意义。较高的股息收益率可能表示股东获得较高的分红回报。</li><li><strong>盈利比较</strong>：通过比较不同公司的股息收益率，可以了解公司的盈利能力和分红政策。</li><li><strong>市场情绪</strong>：股息收益率也可以反映市场对公司的情绪和风险偏好。一般而言，较高的股息收益率可能意味着市场对公司前景持谨慎态度，或者公司面临一定的困境。</li></ul><h5 id="净利润"><a href="#净利润" class="headerlink" title="净利润"></a>净利润</h5><p><strong>净利润</strong>用于衡量一家公司在特定会计期间内实际获得的净收益，即扣除各项费用和税后利润。净利润是评估公司盈利能力和基本经营状况的重要指标。</p><p>净利润的计算公式为：净利润 &#x3D; 总收入 - 总成本 - 税收 - 其他费用。<br>其中：</p><ul><li>总收入：是指公司在特定会计期间内所实现的总销售收入或营业收入。</li><li>总成本：包括直接成本和间接成本，指用于生产和销售产品或提供服务的成本。</li><li>税收：是指公司应缴纳的所得税或其他税收费用。</li><li>其他费用：包括财务费用、管理费用、营销费用等。</li></ul><p>净利润是衡量公司盈利能力的重要指标，主要应用包括以下几个方面：</p><ul><li>盈利能力评估：净利润是评估盈利能力的核心指标之一。较高的净利润意味着公司创造了更多的利润，具有较好的盈利能力。</li><li>盈利趋势分析：观察净利润的变化趋势可以了解盈利的增长趋势和稳定性。</li><li>盈利比较：通过比较不同公司的净利润，投资者可以了解公司的盈利水平和和业绩相对强弱。对于同行业的公司，较高的净利润可能意味着公司的竞争力较强。</li></ul><h5 id="负债与资产比率"><a href="#负债与资产比率" class="headerlink" title="负债与资产比率"></a>负债与资产比率</h5><p><strong>负债与资产比率</strong>用于衡量公司的资本结构和债务风险水平。该比率反映了公司负债占总资产的比例（财务杠杆），可以了解公司负债情况以及对负债承受能力的评估。</p><p>负债与资产比率的计算公式为：负债与资产比率 &#x3D; 总负债 &#x2F; 总资产。<br><small>总负债是指公司在特定时点上的所有债务总额，包括长期负债和短期负债。总资产是指公司在特定时点上的全部资产，包括流动资产和固定资产。</small></p><p>负债与资产比率指标的应用主要包括以下几个方面：</p><ul><li>资本结构评估：通过负债与资产比率，投资者可以评估公司的资本结构，了解公司通过负债融资来支持业务运营和发展的程度。</li><li>债务风险评估：负债与资产比率可以帮助投资者评估公司的债务风险水平。</li><li>行业比较：通过比较同行业内不同公司的负债与资产比率，投资者可以了解公司在行业内的相对债务水平。</li></ul><hr><h3 id="选股策略"><a href="#选股策略" class="headerlink" title="选股策略"></a>选股策略</h3><h4 id="选股的原因"><a href="#选股的原因" class="headerlink" title="选股的原因"></a>选股的原因</h4><p>在经济学的有效市场理论模型中，根据有效性可以将当前证券市场分为四种类型：</p><ul><li><strong>无效市场</strong>。当前价格未反应历史价格信息，那么未来的价格将进一步对过去的价格做出反应。</li><li><strong>弱有效市场</strong>。通过技术分析的交易已经无法获取利润，而基本面分析还可以获取利润。</li><li><strong>半强有效市场</strong>。基本面全部失效，此时通过内幕消息可以获得利润。</li><li><strong>强有效市场</strong>。价格已经充分反应所有信息，此时只能采取保守策略。</li></ul><p>以上几种类型是为了研究而构建出的理想模型，在现实中未必全部对应，但市场如果可以将<strong>历史价格信息</strong>、<strong>基本面</strong>和<strong>内幕消息</strong>反映到资产价格上，那么该市场就是有效的。</p><h4 id="效用函数"><a href="#效用函数" class="headerlink" title="效用函数"></a>效用函数</h4><p>为了简化问题，假设我们每个人都有自己的<strong>效用函数</strong>，它的输入是预备投入的金额，输出是可以带来多少<strong>效用</strong>。效用简单来说就是产出的金额，其具有两个性质：</p><ul><li>如果 <code>x &lt; y</code>，那么 <code>u(x) &lt;= u(y)</code>，即产出的金额越多越好。</li><li>如果 <code>d &gt;= 0</code>，并且 <code>x &lt; y</code>，那么 <code>u(x+d)−u(x) &gt;= u(y+d)−u(y)</code>。即输入相同的金额，但本金较少的那个人效用最大。</li></ul><p>那也就是说如果某一个人的效用函数为 <code>u</code>，面对 <code>n</code> 中选项，这些选项的结果为 $X_1, X_2 … X_n$，那么这个人会选择的就是 <code>E[u(X)]</code> 最大的那个选项。</p><h4 id="现代资产配置理论模型"><a href="#现代资产配置理论模型" class="headerlink" title="现代资产配置理论模型"></a>现代资产配置理论模型</h4><p><strong>现代资产配置理论模型（<code>Modern Portfolio Theory, MPT</code>）模型</strong>核心思想是以最小化标准差并最大化预期收益为目标进行资产配置，也被称为<strong>均值-方差分析</strong>。</p><h5 id="假设和模型"><a href="#假设和模型" class="headerlink" title="假设和模型"></a>假设和模型</h5><p>假设市场中存在 <code>n</code> 个不同的资产，对于某个资产 <code>i</code>，使用 $r_i$ 表示该资产的收益率的随机变量，$E[r_i]$ 表示预期收益率，$\sigma_i$ 表示 $r_i$ 的标准差。<br><br>市场中所有收益率没有不确定的资产称<strong>无风险资产</strong>，并且所有无风险资产的收益率都是一致的，称为<strong>无风险利率 $r_f$</strong> 。<br><br><strong>风险资产配置（<code>risky portfilio, P</code>）</strong>是由风险资产 <code>i=1,2 ... n</code> 按照权重比例组成，每个资产 <code>i</code> 在 <code>P</code> 中的权重是 $\omega_i$，所有权重满足 $sum_{i-1}^n \omega_i &#x3D; 1$。<br>根据单个资产的收益率，可以计算资产配置的收益变量，资产组合收益率的随机变量 $r_p &#x3D; sum_{i&#x3D;1}^n \omega_i r_i$，其预期收益为：<br>$$E[r_p] &#x3D; E[sum_{i&#x3D;1}^n \omega_i r_i] &#x3D; sum_{i&#x3D;1}^n \omega_i E[r_i]$$<br>方差为：<br>$$Var(r_p) &#x3D; E[r_p - E[r_p]] &#x3D; sum_{i&#x3D;1}^n sum_{j&#x3D;1}^n \omega_i \omega_j Cov(r_i, r_j) $$</p><h5 id="有效前沿"><a href="#有效前沿" class="headerlink" title="有效前沿"></a>有效前沿</h5><p>对于任意一个预期收益值 $\mu$，均可以找到一个由配置权重 $\omega &#x3D; (\omega_1, \omega_2, …, \omega_n)$ 定义的风险资产配置 $P$，其预期收益率为 $\mu$，并且在所有可以配置出预期收益率为 $\mu$ 的资产组合中，$P$ 的方差是最小的。<em>最优化问题的目标函数</em>如下：<br>$$ Min \quad Var(r_p) &#x3D; E[r_p - E[r_p]] &#x3D; sum_{i&#x3D;1}^n sum_{j&#x3D;1}^n \omega_i \omega_j Cov(r_i, r_j) $$<br>$$ 满足 \quad E[r_p] &#x3D; E[sum_{i&#x3D;1}^n \omega_i r_i] &#x3D; sum_{i&#x3D;1}^n \omega_i E[r_i] &#x3D; \mu, sum_{i&#x3D;1}^n \omega_i &#x3D; 1 $$<br><br>上面的问题可以使用 <em><code>Lagrange</code> 乘子</em>的方法解决，对于每一个值 $\mu$，会得到一个风险资产配置 $P$，满足 $E[r_p] &#x3D; \mu$，并且 $\delta_p$ 是最小的。<br>将上述结果输出，则可以在标准差-预期的坐标上可以一条线，该线被称为<strong>有效前沿（<code>efficient frontier</code>）</strong>，由于其形状类似子弹也被称为<em>马科维兹子弹（<code>Markowitz bullet</code>）</em>。<br>有效前沿存在一个波动率最小的位置 <code>A</code>，并且该点以上的位置才是真正<em>有效</em>的，其含义是在固定风险的前期下选择最大的预期收益。</p><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-26_11-56-51.png" alt="Snipaste_2024-02-26_11-56-51"></p><h5 id="夏普比率"><a href="#夏普比率" class="headerlink" title="夏普比率"></a>夏普比率</h5><p>有效前沿的左侧区域是通过风险资产无法配置的，但如果在资产配置中加入无风险资产则可以获取到左侧的某些位置。<br>选择有效前沿上的某个资产配置 <code>P</code>，并选择比例 $\alpha &gt; 0$，将本金的 $\alpha$ 配置于 <code>P</code>，将 $1-\alpha$ 配置于无风险资产。</p><ul><li>$\alpha &lt;&#x3D; 1$，则 $1-\alpha &gt;&#x3D; 0$，也就是将 $1-\alpha$ 倍的本金购买无风险资产，以此获得无风险利率。</li><li>$\alpha &gt;&#x3D; 1$，则 $1-\alpha &lt;&#x3D; 0$，也就是贷款本金 $\alpha-1$ 倍的资金支付无风险利率，并用贷款和本金一起配置 <code>P</code>。</li></ul><p>以 $\alpha$ 为系数，使用无风险资产和资产组合 <code>P</code>，可以配置出一个投资组合，那么其收益随机变量为 $r_\alpha$，计算可得：<br>$$ E[r_\alpha] &#x3D; E[\alpha r_p + (1-\alpha) r_f] &#x3D; E[\alpha r_p] + (1-\alpha)r_f $$<br>$$ \sigma_\alpha &#x3D; {\sqrt {Var(\alpha r_p + (1-\alpha) r_f)}} &#x3D; \sigma \alpha_p $$</p><p>以上投资组合在期望收益率-标准差曲线上表现为一条经过有效边界上一点的射线，该线被称为<strong>资本配置线（<code>Capital Allocation Line, CAL</code>）</strong>，该线上的每个点都表示一个风险资产和无风险资产的投资组合。</p><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-27_17-49-49.png" alt="Snipaste_2024-02-27_17-49-49"></p><p><code>CAL</code> 的斜率 $ Sharpe(P) &#x3D; {\frac {E[r_p] - r_f} {\alpha_p}} $，该比率被称为<strong>报酬-波动性比率（<code>reward-to-variability ratio</code>）</strong>，又被称为<strong>夏普比率</strong>。</p><h5 id="资本市场线"><a href="#资本市场线" class="headerlink" title="资本市场线"></a>资本市场线</h5><p>用市场信息计算得来的有效前沿上必定有一个夏普比率最高的点 <code>M</code>，称其为<strong>市场组合</strong>；穿过 <code>M</code> 的资产配置线 <code>CAL(M)</code> 称为<strong>资本市场线（<code>capital market line</code>）</strong>。资本市场线的意义在于固定标准差，那么市场上预期收益最高的投资组合就在该线上；或者固定预期收益，那么标准差最低的投资组合就在该线上。因此资本市场线可以通俗理解为<strong>最佳配置线</strong>。</p><h4 id="资本资产定价模型"><a href="#资本资产定价模型" class="headerlink" title="资本资产定价模型"></a>资本资产定价模型</h4><p><strong>资本资产定价模型（<code>Capital Asset Pricing Model, CAPM</code>）</strong>是建立在 <code>MPT</code> 之上，其用简单的数学公式表达资产的收益率与风险系数 $\beta$ 以及系统性风险之间的关系。</p><h5 id="假设和模型-1"><a href="#假设和模型-1" class="headerlink" title="假设和模型"></a>假设和模型</h5><p><code>CAPM</code> 假设市场上所有的投资者均对于风险和收益的评估仅限于对于收益变量的预期值和标准值的分析。并且市场是完全公开的，对于任何投资者信息和机会完全均等，任何投资者均可以以唯一的无风险利率无限制地贷款或借出。</p><p>为了最大化预期收益并最小化标准差，所有的投资者均选择<em>资本市场线</em>上的某一个点作为资产配置，即所有投资者都按一定比例持有现金和市场组合 <code>M</code>。因此市场组合的波动性和不确定性不单单是组合的风险，而是整个市场的风险，其被称为<strong>系统性风险</strong>。</p><h5 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h5><p><code>CAPM</code> 公式表达的是任意风险资产的收益率与市场组合的收益率之间的关系，在该公式中任何风险资产的收益率都可以被分为两部分：无风险收益和风险收益 $\beta$。</p><p>$$ E[r_S] &#x3D; r_f + \beta_S(E[r_M] - r_f) $$<br><small>$r_S$ 表示 <code>S</code> 的收益变量，$r_M$ 表示市场组合的收益变量，$r_f$ 表示市场的无风险利率，$\beta_S &#x3D; {\frac {Cov(r_S, r_M)} {Var(r_M)}}$ 表示组合 <code>S</code> 对于市场风险的敏感度。</small><br>$E[r_M] - r_f$ 是市场组合的风险收益，$\beta_S(E[r_M] - r_f)$ 是资产组合 <code>S</code> 的风险收益，因此可以理解为资产组合 <code>S</code> 承担 $\sigma_S$ 倍的市场风险，其将会承担相应倍数的风险补偿。</p><p>通过 <code>CAPM</code> 公式可以得到资产组合 <code>S</code> 的夏普比率与市场组合 <code>M</code> 的夏普比率之间的关系：<br>$$ Sharpe(S) &#x3D; Corr(r_S, r_M) Sharpe(M) $$<br>从公式可以得出 <code>S</code> 与 <code>M</code> 的相关性越高，则 <code>S</code> 的夏普比率越高，相应的风险与收益的比值也就越大。</p><p><small>风险组合 <code>S</code> 的预期收益与 $\beta_S$ 有关，而与其自身风险 $\sigma_S$ 无关。</small></p><h5 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h5><p>在现实环境中，可以将一个概括市场整体的组合（比如大盘指数）作为市场组合，并以其为基准计算每个风险资产的系统性风险 $\beta$，这样就可以根据对市场整体趋势的判断以及对需要的风险进行控制，进而选择合适的 $\beta$ 进行资产配置。</p><h4 id="套利定价理论"><a href="#套利定价理论" class="headerlink" title="套利定价理论"></a>套利定价理论</h4><p><strong>套利定价理论 <code>APT</code></strong> 可以理解为 <code>CAPM</code> 的一个推广，由 <code>APT</code> 给出的定价模型是将收益归因到不同的因子之上，而 <code>CAPM</code> 仅把收益归咎于市场变化着一个因子。</p><p><code>APT</code> 模型认为在现有市场中，套利行为是市场价格均衡形成的一个决定因素，如果市场价格未达到均衡状态时，则会存在无风险套利机会，而套利行为会使得市场价格回归均衡状态。<br><code>APT</code> 模型使用多个因素来解释风险资产的收益，并根据无套利原则，得出风险资产的收益与多个因素之间存在近似的线性关系，也就是说资产的预期收益率与一组影响他们的系统性因素的预期收益率是线性相关的，即影响资产收益率的因素从 <code>CAPM</code> 的单一因素拓展到多个因素。<strong>多因子模型（<code>Multiple-Factor Model, MFM</code>）</strong>正是基于 <code>APT</code> 模型的思想发展而来的完整风险模型。<br>多因子模型定量刻画了资产收益率与每个因子之间的线性关系，数学表达式如下：<br>$$ \bar r_j &#x3D; sum_{k&#x3D;1}^K X_{jk} * \bar f_k + \bar u_j $$</p><p><small>$ \bar X_{jk}$ 表示资产 <code>j</code> 在因子 <code>k</code> 上的载荷；$ \bar f_k$ 表示因子 <code>k</code> 的收益；$\bar u_j$ 表示资产 <code>j</code> 的残差收益。</small></p><h4 id="多因子模型"><a href="#多因子模型" class="headerlink" title="多因子模型"></a>多因子模型</h4><p><strong>多因子模型</strong>是一个用于选择投资组合的策略，其通过多个与预期收益相关的因子来选择投资组合，而因子可以通过历史数据计算来预测未来的收益表现，以期望达到全面、稳定的预测。<br>多因子模型常见的三类因子如下：</p><ul><li>基本面因子：包括市盈率 <code>PE</code>、市净率 <code>PB</code>、营业收入增长率、估值、营业收入、净利润、负债等。</li><li>技术分析因子：包括动量 <code>Momentum</code>、波动率 <code>Volatility</code>、换手率、量比等。</li><li>宏观经济因子：包括利率、通货膨胀率、<code>GDP</code> 增长、工业增加值、<code>CPI</code>、利率、<code>M1</code>、<code>M2</code>等。</li></ul><h5 id="多因子选股实践"><a href="#多因子选股实践" class="headerlink" title="多因子选股实践"></a>多因子选股实践</h5><p>在实现多因子选股模型时，需要进行以下步骤：</p><ul><li>确定目标和约束条件。</li><li>选择因子并计算。</li><li>异常值处理。</li><li>因子标准化。</li><li>确定因子权重。</li><li>构建多因子模型。</li><li>资产筛选和组合优化。</li><li>回测和调整。</li></ul><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取和准备数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;stock_data.csv&#x27;</span>)</span><br><span class="line">X = df[[<span class="string">&#x27;PE&#x27;</span>, <span class="string">&#x27;PB&#x27;</span>, <span class="string">&#x27;ROE&#x27;</span>]] <span class="comment"># 特征因子</span></span><br><span class="line">y = df[<span class="string">&#x27;Returns&#x27;</span>] <span class="comment"># 目标变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分训练数据和测试数据</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化处理</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型系数，确定因子权重</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Factor weights:&#x27;</span>, model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型预测测试数据的收益</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 DataFrame 来保存资产的预测收益</span></span><br><span class="line">predicted_returns = pd.DataFrame(&#123;<span class="string">&#x27;Stock&#x27;</span>: X_test.index, <span class="string">&#x27;Predicted return&#x27;</span>: y_pred&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据预测收益选择资产</span></span><br><span class="line">selected_stocks = predicted_returns[predicted_returns[<span class="string">&#x27;Predicted return&#x27;</span>] &gt; <span class="number">0.1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Selected stocks:&#x27;</span>, selected_stocks)</span><br></pre></td></tr></table></figure><p>以上示例实现了一个线性回归的多因子模型，通过 <code>PE</code>、<code>PB</code>、<code>ROE</code> 三个因子来预测资产收益，利用模型系数作为因子权重，采用线性回归作为预测模型，但在实际情况下，可以根据需求调整和改进更多因子，同时使用更复杂的模型，例如神经网络、随机森林等，或者使用更复杂的特征工程技术，例如 <code>PCA</code>、特征选择等。另外该模型未进行任何形式的模型验证，例如交叉验证、超参数调优等，可进一步针对优化。</p><hr><h3 id="择时策略"><a href="#择时策略" class="headerlink" title="择时策略"></a>择时策略</h3><p>量化择时策略即采用<strong>数量化分析</strong>方法，利用<strong>单个或多个技术指标的组合</strong>，来对交易标的资产或资产指数进行<strong>低买高卖</strong>的操作，期望获得超越简单买入持有策略的收益风险表现。</p><p>量化择时策略的核心是客观型技术分析。客观型技术分析，是指其分析过程中所用到的分析方法，具有 <code>100%</code> 客观的定义标准，不包含任何主观定义的部分。</p><p>常用策略方法：</p><ul><li><strong>趋势择时</strong>。趋势择时的基本思想来自于技术分析，技术分析认为趋势存在延续性，主要指标有 <code>MA</code>、<code>MACD</code>、<code>DMA</code> 等。</li><li><code>Hurst</code> 指数。</li><li><strong><code>SWARCH</code> 模型</strong>。该模型主要刻画了货币供应量 <code>M2</code> 和大盘走势之间的关系，揭示证券市场指数变化与货币供应量之间的相关关系。</li><li>异常指标择时。异常指标择时主要处理一些特殊情况下的择时，主要有：市场噪声、行业集中度和兴登堡凶兆3个策略。</li><li>市场情绪择时。</li><li><code>SVM</code> 分类。<code>SVM</code> 择时是利用 <code>SVM</code> 技术进行大盘趋势的模式识别，将大盘区分为几个明显的模式，从而找出其中的特征，然后利用历史数据学习的模型来预测未来的趋势。</li><li><strong>有效资金模型</strong>。有效资金是指市场上能对趋势产生影响的资金流，同样的资金量，在趋势顶部和底部的时候，对市场的影响是不一样的。</li><li>牛熊线。</li></ul><h4 id="双均线择时策略"><a href="#双均线择时策略" class="headerlink" title="双均线择时策略"></a>双均线择时策略</h4><p>策略原理：使用两根均线，一根长周期均线，一根短周期均线。当短期均线从下往上穿越长周期均线的时候，买入；当短期均线从上往下穿越长周期均线的时候，卖出。均线八大法则也是一种双均线策略，其短周期均线为M1（当日收盘价）。<br>均线根据不同周期有以下分类：</p><ul><li><strong>短期均线</strong>：<code>5、7、10</code> 用于预测短期走势，<code>MA5</code> 和 <code>MA10</code> 又称为短期监测线。</li><li><strong>中期均线</strong>：<code>20、30、60</code> 用于预测中期走势，<code>MA20</code> 和 <code>MA30</code> 又称为警戒线，<code>MA60</code> 则称之为生死线。</li><li><strong>长期均线</strong>：<code>120、250</code> 用于长期走势，<code>MA120</code> 又称为确认线，<code>MA250</code> 则通常被看做反转线，也称为牛熊分界线。</li></ul><p>策略缺陷：</p><ul><li><strong>滞后性</strong>：均线归根到底是一种平均值，在应用中存在的最大问题就是滞后性。当出现买入卖出信号时，最佳时机早已过去。</li><li><strong>长短周期难以选择</strong>：如果两根均线的周期接近可能会造成很大的亏损，因此两个参数选择的很重要，趋势性越强的品种，均线策略越有效。</li></ul><p>策略优化尝试方向：</p><ul><li><strong>使用加权移动平均值</strong>：均线策略的一大缺陷是指标具有滞后性，因此可以使用加权移动平均值代替移动平均值。计算时将短期（如昨天）的权重增大，以加强指标的敏感性。</li><li><strong>不局限于收盘价</strong>：可以尝试将最高价、最低价等加入到加权移动平均值的计算中。</li><li><strong>自适应调整均线周期</strong>：价格走势进入单边上涨趋势，自适应均线自动缩短周期，采用短期均线，转为向上移动；市场走势进入横盘震荡，自适应均线自动延长周期，采用长期均线，转为横向移动。</li></ul><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取资产数据，选取最近日期的 500 条数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;000001.csv&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">df = df[-<span class="number">500</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算短期均线和长期均线</span></span><br><span class="line">df[<span class="string">&#x27;MA5&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].rolling(window=<span class="number">5</span>).mean()</span><br><span class="line">df[<span class="string">&#x27;MA20&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].rolling(window=<span class="number">20</span>).mean()</span><br><span class="line">df[<span class="string">&#x27;MA60&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].rolling(window=<span class="number">60</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表展示</span></span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(df[<span class="string">&#x27;MA5&#x27;</span>], label=<span class="string">&#x27;MA5&#x27;</span>)</span><br><span class="line">plt.plot(df[<span class="string">&#x27;MA20&#x27;</span>], label=<span class="string">&#x27;MA20&#x27;</span>)</span><br><span class="line">plt.plot(df[<span class="string">&#x27;MA60&#x27;</span>], label=<span class="string">&#x27;MA60&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-02_11-31-48.png" alt="Snipaste_2024-02-02_11-31-48.png"></p><h4 id="MACD-均线择时策略"><a href="#MACD-均线择时策略" class="headerlink" title="MACD 均线择时策略"></a><code>MACD</code> 均线择时策略</h4><p><strong>异同移动平均线（<code>Moving Average Convergence and Divergence, MACD</code>）</strong>是由指数均线演变而来，通过对资产的收盘价进行平滑处理，生成两线一柱，其中两线为快速线(<code>Difference Line, DIF</code>)和慢速线(<code>Signal Line, DEA</code>)，一柱是 <code>MACD</code>。</p><p><code>MACD</code> 指标是 <code>DIF</code> 与 <code>DEA</code> 之间的差，其可以反映出最近价格走势的强弱变化和能量，把握准确的买卖点。</p><p>指标计算公式：</p><ul><li><strong>短期 <code>EMA</code></strong> ：短期（如 <code>12</code> 天）的收盘价指数移动平均值，一般采用 <code>EMA(price, 12)</code>。</li><li><strong>长期 <code>EMA</code></strong> ：长期（如 <code>26</code> 天）的收盘价指数移动平均值，一般采用 <code>EMA(price, 26)</code>。</li><li><strong><code>DIF</code></strong> ：短期 <code>EMA</code> 与长期 <code>EMA</code> 的差值，即 <code>DIF = EMA(price, 12) - EMA(price, 26)</code>。</li><li><strong><code>DEA</code></strong> ：<code>DIF</code> 的多日指数移动平均值，即 <code>DEA=EMA(DIF, 9)</code>。</li><li><strong><code>MACD</code></strong> ：<code>DIF</code> 与 <code>DEA</code> 线的差值，即 <code>MACD = DIF - DEA</code>。</li></ul><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> talib <span class="keyword">as</span> ta </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用最近 500 天的收盘价数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;000001.csv&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">data = data[-<span class="number">500</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 talib 库中的相应函数计算 MACD 指标</span></span><br><span class="line">dif, dea, macd = ta.MACD(data[<span class="string">&#x27;close&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表展示</span></span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(dif)</span><br><span class="line">plt.plot(dea)</span><br><span class="line">plt.bar(macd.index, macd.values)</span><br><span class="line">plt.legend([<span class="string">&quot;dif&quot;</span>, <span class="string">&quot;bea&quot;</span>, <span class="string">&quot;macd&quot;</span>], loc=<span class="string">&quot;upper left&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/./%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-02_14-17-18.png" alt="Snipaste_2024-02-02_14-17-18.png"></p><hr><h3 id="调仓策略"><a href="#调仓策略" class="headerlink" title="调仓策略"></a>调仓策略</h3><h4 id="最优化计算仓位"><a href="#最优化计算仓位" class="headerlink" title="最优化计算仓位"></a>最优化计算仓位</h4><p>之前根据<strong>多因子模型</strong>和<strong>择时策略</strong>对资产进行未来收益的预测；在多因子模型中根据同一个时间截面上不同因子的指标，通过线性、非线性的方法将多个因子综合成多因子作为资产未来收益率高低的预测值；在择时策略中，通过计算资产的离散时序信号连续化，连续信号的差异化使得各个资产的择时信号能够进行比较。至此可能就会发现上述预测并没有说明购买风险资产的数量，即<strong>如何分配资金</strong>。</p><p>常用符号约定如下：</p><ul><li><code>N</code>：资产数量</li><li>$\omega$：权重向量</li><li>$\omega_i$：资产 <code>i</code> 的权重</li><li>$\mu$：预期收益率</li><li>$\sigma$：波动率</li><li>$\sigma_i$：资产 <code>i</code> 的波动率</li><li>$\sigma_{ij}$：资产 <code>i</code> 与资产 <code>j</code> 之间的协方差</li><li>$\Sigma$：方差协方差矩阵</li><li>$\sigma_p$：组合波动率</li><li>$\mu_p$：组合收益率</li><li>$\lambda$：风险厌恶系数</li><li>$r_f$：无风险收益率</li></ul><h5 id="等权重"><a href="#等权重" class="headerlink" title="等权重"></a>等权重</h5><p>顾名思义即在没有任何信息的情况下，等权重是最简单的方法，即每个资产都赋予相同的价值。$$ \omega_i &#x3D; {\frac {1} {N}} $$</p><h5 id="市值加权"><a href="#市值加权" class="headerlink" title="市值加权"></a>市值加权</h5><p>在没有任何信息的情况下，另外的一种方法就是根据市值进行加权。<br>$$ \omega_i &#x3D; Cap_i &#x2F; sum_i{Cap_i}$$<br><small>$ Cap_i $ 为资产 <code>i</code> 的市值。</small><br><small>市值加权会给与过高估值的资产更多权重，因此在某些情况下并不占优。</small></p><h5 id="最小方差组合"><a href="#最小方差组合" class="headerlink" title="最小方差组合"></a>最小方差组合</h5><p>前两种方法都是没有任何信息时才使用，而在实际中可以通过历史收益率的方差来作为代理变量，追求组合整体的方差最小，其数学表达式为：<br>$$ Min \quad \sigma_p &#x3D; \omega^&#96; \Sigma_w $$</p><h5 id="最大分散度"><a href="#最大分散度" class="headerlink" title="最大分散度"></a>最大分散度</h5><p>从组合的方差-协方差矩阵中可知，组合中的一部分风险来源于各个资产的方差，而另外一部分风险来源于资产之间的协方差，因此想要降低风险就要分散投资，其数学表达式为：<br>$$ Max \quad D(\omega) &#x3D; {\frac {\omega^` \sigma} {\sqrt {\omega^` \Sigma_w} }} $$</p><p>目标函数被称为分散比率，其分母为组合波动率，分子为成分波动率加权平均。即可以最大化资产线性加权波动率与投资组合波动率的比值，因此称为最大分散化资产配置组合。</p><h5 id="风险平价"><a href="#风险平价" class="headerlink" title="风险平价"></a>风险平价</h5><p>风险平价是从风险的角度出发，追求所有对组合的风险贡献一致。首先定义边界风险，即每增加 <code>1</code> 单位资产的权重 $\omega_i$ 所引起的组合整体风险变化：<br>$$<br>\begin{equation}<br>\begin{split}<br>MRC_i &amp;&#x3D; {\frac {\delta \sigma_p} {\delta \omega_i}} &#x3D; {\frac {\omega_i \sigma_i^2 + sum_{j!&#x3D;i} \omega_j \rho_{ij} \sigma_i \sigma_j} {\sigma_p}} \\<br>&amp;&#x3D; \frac {sum_{j-1}^N \omega_j \rho_{ij} \sigma_i \sigma_j} {\sigma_p} \\<br>&amp;&#x3D; \frac {\rho_{ip} \sigma_i \sigma_p} {\sigma_p} \\<br>&amp;&#x3D; (\frac {\rho_{ip} \sigma_i} {\sigma_p}) * \sigma_p \\<br>&amp;&#x3D; \beta_i \sigma_p<br>\end{split}<br>\nonumber<br>\end{equation}<br>$$</p><p><small>其中 $\beta_i$ 表示资产 <code>i</code> 收益率对于投资组合收益率的 $\beta$ 系数，在定义过边界风险后，根据其权重就可以得到风险贡献：</small><br>$$ RC_i &#x3D; \omega_i * MRC_i &#x3D; \omega_i {\frac {\delta \sigma_p} {\delta \omega_i}} $$</p><p>因此风险平价组合的目标函数为：<br>$$ Min \quad sum_{i-1}^N sum_{j-1}^N \quad (RC_i - RC_j)^2 $$</p><p>资产的权重与其相对于组合的 $\beta$ 成反比，即 $\beta$ 越高，其权重越低，由此有效地降低风险，每个资产对组合的边界风险贡献相同。</p><h5 id="均值方差优化"><a href="#均值方差优化" class="headerlink" title="均值方差优化"></a>均值方差优化</h5><p>上面说的最小方差、最大分散度、风险平价都是从风险的角度出发的优化方法，而均值方差优化则是在从收益最大化方面出发，实现收益最大化或特定收益风险最小化。即<strong>均值方差优化</strong>，其目标函数如下：<br>$$ Max \quad \omega^T \mu - {\frac {\lambda} {2}} \omega^` \Sigma_\omega $$</p><p>从理论上讲，组合成分间存在无数种可能性，而每种可能性都会有风险收益对，而将多有的结果集合在一起就会形成一个可行域。可行域中的每个结果并非都是<em>好结果</em>，只有在侧边缘的值才是最优解，即 <code>MOV</code> 的解。<br>其中 <code>A ~ D</code> 之间连线，这条线被称为<strong>有效前沿</strong>，任何异于有效前沿上的点，均能找到相同风险下收益率更高的组合。</p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-24_23-17-59.jpg" class="" title="Snipaste_2024-02-24_23-17-59"> <p>自无风险收益率起做一条切线，其与有效前沿相切于 <code>B</code> 点，该点即所有可行域中夏普比率最大的点，也被称为最大夏普组合，即<strong>最大化组合夏普比率</strong>目标函数如下：<br>$$ Max \quad {\frac {\omega^` \mu} {\sqrt{\omega^` \Sigma_\omega}}} $$</p><p><small>相比之前的各种优化方法，均值方差优化引入了较多的参数，尤其是预期收益率的估计，因此优化结果对于参数会非常敏感，体现就是权重在时序上波动较大，容易出现极端值。其后续优化可以参考 <code>Black-Litterman</code> 模型。</small></p><h5 id="常见约束"><a href="#常见约束" class="headerlink" title="常见约束"></a>常见约束</h5><ul><li>单资产权重的范围限制</li><li>做空限制</li><li>行业中性化</li><li>风险敞口限制</li></ul><h4 id="资本资产定价模型-1"><a href="#资本资产定价模型-1" class="headerlink" title="资本资产定价模型"></a>资本资产定价模型</h4><p><strong>资本资产定价模型（<code>Capital Asset Pricing Model, CAPM</code>）</strong>是在 <code>MPT</code> 基础上发展而来，其表述的是任何一种风险资产的收益率相对于市场组合收益率之间的关系：<br>$$ E(r_s) &#x3D; r_f + \beta_s(E(r_M) - r_f) $$<br><small>$r_s$ 表示的是某种风险资产的收益；$r_f$ 表示为无风险资产收益；$r_M$ 表示市场组合收益率；$\beta_s$ 表示该风险资产相对于市场的敏感度，其反映该风险资产随着市场组合收益变动的关联性大小。</small><br>$$ \beta_s &#x3D; {\frac {cov(r_s, r_M)} {var(r_M)}} &#x3D; {\frac {sum_{i-1}^N w_i cov(r_s, r_i)} {sum_{i-1}^N sum_{j-1}^N w_i w_j cov(r_i, r_j)}} $$<br><small>在牛市期间，如果买入 $\beta_s &gt; 1$ 的资产，可以获得比市场平均收益更多；在熊市期间，如果持有 $\beta_s &lt; 1$ 的资产则可以避免损失，甚至可以获取收益。</small><br>综上所述资本资产定价模型的本质就是如果想获得更多的收益，那就必须承担更多的风险。风险与机遇并存。</p><hr><h3 id="量化回测"><a href="#量化回测" class="headerlink" title="量化回测"></a>量化回测</h3><p>量化回测主要用于评估策略在真实市场环境下的好坏，需要从<strong>收益</strong>、<strong>稳定性</strong>、<strong>胜率</strong>、<strong>风险</strong>等方面来综合评估策略，常见的指标可以看净值变化、波动率、最大回撤、夏普比率等。</p><h4 id="回测评估指标"><a href="#回测评估指标" class="headerlink" title="回测评估指标"></a>回测评估指标</h4><p>数据准备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取中国平安000001、贵州茅台600519、沪深300 000300 数据</span></span><br><span class="line">data1 = pd.read_csv(<span class="string">&#x27;./000001.csv&#x27;</span>, index_col=<span class="number">0</span>, header=<span class="literal">None</span>)</span><br><span class="line">data2 = pd.read_csv(<span class="string">&#x27;./600519.csv&#x27;</span>, index_col=<span class="number">0</span>, header=<span class="literal">None</span>)</span><br><span class="line">data3 = pd.read_csv(<span class="string">&#x27;./000300.csv&#x27;</span>, index_col=<span class="number">0</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取最近 500 天的日交易数据</span></span><br><span class="line">data1_close = data1[<span class="number">2</span>][-<span class="number">500</span>:]</span><br><span class="line">data2_close = data2[<span class="number">2</span>][-<span class="number">500</span>:]</span><br><span class="line">data3_close = data3[<span class="number">2</span>][-<span class="number">500</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取当日的收盘价整合为待处理数据</span></span><br><span class="line">df = pd.DataFrame()</span><br><span class="line">df[<span class="string">&#x27;000001&#x27;</span>] = data1_close</span><br><span class="line">df[<span class="string">&#x27;600519&#x27;</span>] = data2_close</span><br><span class="line">df[<span class="string">&#x27;000300&#x27;</span>] = data3_close</span><br></pre></td></tr></table></figure><h5 id="净值曲线"><a href="#净值曲线" class="headerlink" title="净值曲线"></a>净值曲线</h5><p>净值曲线是一组时间序列曲线，其含义表示为资产或基金在不同时间的价值对比最初价值的<strong>倍数</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以 500 天的收盘价作为基准，计算净值并绘制曲线</span></span><br><span class="line">df_worth = df / df.iloc[<span class="number">0</span>]</span><br><span class="line">df_worth.plot(figsize=(<span class="number">15</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Trend of stock value&#x27;</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.xticks(df.index, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-37-08.png" alt="Snipaste_2024-02-22_20-37-08"></p><h5 id="年化收益率"><a href="#年化收益率" class="headerlink" title="年化收益率"></a>年化收益率</h5><p>累计收益率计算公式如下：<br>$$ R_t &#x3D; {\frac {P_T - P_t} {P_t}} $$<br><small>$P_T$ 表示期末的价格，$P_t$ 表示期初的价格。</small><br>年化收益率其最直观的理解就是每年的收益是多少。年化收益率计算公式如下：<br>$$ R_p &#x3D; (1 + R)^{\frac {m} {n}} - 1$$<br><small>$R$ 表示期间总收益率，<code>m, n</code> 表示相对应的计算周期，<code>m</code> 使用 <code>252, 52, 12</code> 分别表示日、周、年的转换，<code>n</code> 表示期间自然日天数。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 区间累积收益率</span></span><br><span class="line">total_return = df_worth.iloc[-<span class="number">1</span>]-<span class="number">1</span></span><br><span class="line">total_return = pd.DataFrame(total_return.values, columns=[<span class="string">&#x27;累计收益率&#x27;</span>], index=total_return.index)</span><br><span class="line">total_return</span><br><span class="line"><span class="comment"># 年化收益率</span></span><br><span class="line">annual_return = pd.DataFrame((<span class="number">1</span> + total_return.values) ** (<span class="number">252</span> / <span class="number">1826</span>) - <span class="number">1</span>, columns=[<span class="string">&#x27;年化收益率&#x27;</span>], index=total_return.index)</span><br><span class="line">annual_return</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-37-21.png" alt="Snipaste_2024-02-22_20-37-21"><br><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-37-32.png" alt="Snipaste_2024-02-22_20-37-32"></p><h5 id="波动率"><a href="#波动率" class="headerlink" title="波动率"></a>波动率</h5><p>波动率是对收益变动的衡量，其本质是风险，用于衡量收益率的不确定性。<br>年波动率计算公式如下：<br>$$ V &#x3D; {\sqrt{ {\frac {252} {n-1}} sum_{i&#x3D;1}^n (r_p - {\bar {r_p} })^2 }} $$<br><small>$r_p$ 表示每日收益率，${\bar {r_p} }$ 表示每日收益率的平均值，$n$ 表示策略执行的天数。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 波动率</span></span><br><span class="line">df_return = df / df.shift(<span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">df_return = ((df_return.iloc[<span class="number">1</span>:] - df_return.mean()) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">volatility = pd.DataFrame(np.sqrt(df_return.<span class="built_in">sum</span>() * <span class="number">252</span> / (<span class="number">1826</span>-<span class="number">1</span>)), columns=[<span class="string">&#x27;波动率&#x27;</span>], index=total_return.index)</span><br><span class="line">volatility</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-37-41.png" alt="Snipaste_2024-02-22_20-37-41"></p><h5 id="最大回撤"><a href="#最大回撤" class="headerlink" title="最大回撤"></a>最大回撤</h5><p>表示在选定的某一周期往后推，在最低点时的收益率回撤的最大幅度。其主要用于<strong>描述最坏的情况</strong>，是量化中最重要的指标。<br>最大回撤计算公式如下：<br>$$ MaxDD &#x3D; {\frac {max(P_i, P_j)} {P_i}} $$<br><small>$P$ 为某一天资产的净值，$P_i, P_j$ 分别表示当天和明天的净值。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最大回撤</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max_drawdown_cal</span>(<span class="params">df</span>):</span><br><span class="line">    md = ((df.cummax() - df) / df.cummax()).<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(md, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">max_drawdown = &#123;&#125;</span><br><span class="line">max_drawdown[<span class="string">&#x27;000001&#x27;</span>] = max_drawdown_cal(df[<span class="string">&#x27;000001&#x27;</span>])</span><br><span class="line">max_drawdown[<span class="string">&#x27;600519&#x27;</span>] = max_drawdown_cal(df[<span class="string">&#x27;600519&#x27;</span>])</span><br><span class="line">max_drawdown[<span class="string">&#x27;000300&#x27;</span>] = max_drawdown_cal(df[<span class="string">&#x27;000300&#x27;</span>])</span><br><span class="line"></span><br><span class="line">max_drawdown = pd.DataFrame(max_drawdown, index=[<span class="string">&#x27;最大回撤&#x27;</span>]).T</span><br><span class="line">max_drawdown</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-37-53.png" alt="Snipaste_2024-02-22_20-37-53"></p><h5 id="Alpha-系数和-Beta-系数"><a href="#Alpha-系数和-Beta-系数" class="headerlink" title="Alpha 系数和 Beta 系数"></a><code>Alpha</code> 系数和 <code>Beta</code> 系数</h5><p><code>Alpha</code> 系数是指投资中的非系统性风险，可获得与市场无关的回报，而 <code>Beta</code> 系数则是指投资中的系统性风险。<br>使用资本资产定价模型来评估策略的 <code>Alpha</code> 和 <code>Beta</code> 系数：<br>$$ E(r_i) &#x3D; r_f + \beta(E(r_m) - r_f) $$<br><small>$E(r_i)$ 表示投资组合的预期收益；$r_f$ 表示无风险利率；$r_m$ 表示市场指数利率；$\beta$ 表示波动风险与投资中的系统性风险；</small></p><p>因此 <code>CAPM</code> 的计量模型可以表示为：<br>$$ r_i &#x3D; \alpha + \beta r_m + \epsilon_\alpha $$<br><small>$\epsilon_\alpha$ 表示随机扰动，可以理解其为个体风险。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># alpha 和 beta 系数</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每日收益率 收盘价缺失值（停牌），使用前值代替</span></span><br><span class="line">rets=(df.iloc[:, :<span class="number">3</span>].fillna(method=<span class="string">&#x27;pad&#x27;</span>)).apply(<span class="keyword">lambda</span> x:x/x.shift(<span class="number">1</span>)-<span class="number">1</span>)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 市场指数为x，个股收益率为y</span></span><br><span class="line">x = rets.iloc[:,<span class="number">1</span>].values</span><br><span class="line">y = rets.iloc[:,:<span class="number">3</span>].values</span><br><span class="line">capm = pd.DataFrame()</span><br><span class="line">alpha = []</span><br><span class="line">beta = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    b, a, r_value, p_value, std_err = stats.linregress(x, y[:,i])</span><br><span class="line">    <span class="comment"># alpha 转化为年化</span></span><br><span class="line">    alpha.append(<span class="built_in">round</span>(a*<span class="number">250</span>, <span class="number">3</span>))</span><br><span class="line">    beta.append(<span class="built_in">round</span>(b, <span class="number">3</span>))</span><br><span class="line">    </span><br><span class="line">capm[<span class="string">&#x27;alpha&#x27;</span>] = alpha</span><br><span class="line">capm[<span class="string">&#x27;beta&#x27;</span>] = beta</span><br><span class="line">capm.index = rets.columns[:<span class="number">3</span>]</span><br><span class="line">capm</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-38-06.png" alt="Snipaste_2024-02-22_20-38-06"></p><h5 id="夏普比率-1"><a href="#夏普比率-1" class="headerlink" title="夏普比率"></a>夏普比率</h5><p>夏普比率是指在承担多一重风险时，可获得的超额回报。该比率是在资本资产定价模型中通过转换而得来。<br>夏普比率计算公式如下：<br>$$ SharpRatio &#x3D; {\frac {R_p - R_f} {\delta_p}} $$<br><small>$R_p$ 表示策略的年化收益率，$R_f$ 表示无风险收益率，$\delta_p$ 表示年化标准差。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 夏普比率</span></span><br><span class="line"><span class="comment"># 超额收益率以无风险收益率为基准，假设无风险收益率为年化 3%</span></span><br><span class="line">ex_return=rets - <span class="number">0.03</span>/<span class="number">250</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算夏普比率</span></span><br><span class="line">sharpe_ratio = np.sqrt(<span class="built_in">len</span>(ex_return)) * ex_return.mean() / ex_return.std()</span><br><span class="line">sharpe_ratio = pd.DataFrame(sharpe_ratio, columns=[<span class="string">&#x27;夏普比率&#x27;</span>])</span><br><span class="line">sharpe_ratio</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-38-16.png" alt="Snipaste_2024-02-22_20-38-16"></p><h5 id="信息比率"><a href="#信息比率" class="headerlink" title="信息比率"></a>信息比率</h5><p>信息比率与夏普比率类似，不过其参照的是策略的<strong>市场基准收益率</strong>。<br>$$ SharpRatio &#x3D; {\frac {R_p - R_f} {\delta_t}} $$<br><small>$R_p$ 表示策略的年化收益率，$R_f$ 表示无风险收益率，$\delta_t$ 表示策略与每日基准收益率差值的年化标准差。</small></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 信息比率（参考的是市场基准收益率）</span></span><br><span class="line">ex_return = pd.DataFrame() </span><br><span class="line">ex_return[<span class="string">&#x27;000001&#x27;</span>] = rets.iloc[:,<span class="number">0</span>] - rets.iloc[:,<span class="number">2</span>]</span><br><span class="line">ex_return[<span class="string">&#x27;600519&#x27;</span>] = rets.iloc[:,<span class="number">1</span>] - rets.iloc[:,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算信息比率</span></span><br><span class="line">information_ratio = np.sqrt(<span class="built_in">len</span>(ex_return)) * ex_return.mean() / ex_return.std()</span><br><span class="line"><span class="comment"># 信息比率的输出结果</span></span><br><span class="line">information_ratio = pd.DataFrame(information_ratio, columns=[<span class="string">&#x27;信息比率&#x27;</span>])</span><br><span class="line">information_ratio</span><br></pre></td></tr></table></figure><p><img src="/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Snipaste_2024-02-22_20-38-25.png" alt="Snipaste_2024-02-22_20-38-25"></p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="https://datawhalechina.github.io/whale-quant/#/">WhaleQuant</a><br><a href="https://ta-lib.github.io/ta-lib-python/">ta-lib</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;量化&lt;/strong&gt;是指一种投资交易策略，利用统计学、数学、计算机、人工智能等方法取代传统人工做出决策，通过模型来实现资产交易进而构建投资组合。即利用计算机技术和数学模型来实现投资策略的过程。&lt;/p&gt;
&lt;p&gt;主观投资与量化投资的对比：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主观投资&lt;/th&gt;
&lt;th&gt;量化投资&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;基于基金经理的判断&lt;/td&gt;
&lt;td&gt;基于模型运算的客观结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;基金经理对宏观环境、行业、公司的研究，预测未来趋势&lt;/td&gt;
&lt;td&gt;核心在于利用计算机技术在海量数据中寻找到投资规律&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;注重研究深度，对少数资产进行深入研究&lt;/td&gt;
&lt;td&gt;注重广度，全市场筛选标的，多维度分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;持股集中，稳定性略差&lt;/td&gt;
&lt;td&gt;持股分散，组合投资&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;交易依靠主管认知与判断&lt;/td&gt;
&lt;td&gt;模型自主下单&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
    <category term="Quantum" scheme="https://blog.vgbhfive.cn/tags/Quantum/"/>
    
  </entry>
  
  <entry>
    <title>Selenium-使用教程</title>
    <link href="https://blog.vgbhfive.cn/Selenium-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    <id>https://blog.vgbhfive.cn/Selenium-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</id>
    <published>2023-12-21T15:17:39.000Z</published>
    <updated>2023-12-26T14:10:31.098Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong><code>Selenium</code></strong> 最初是一个用于网站的自动化测试工具，支持各种 <code>Chrome</code>、<code>Firefox</code>、<code>Safari</code> 等主流浏览器，同时也支持 <code>phantomJS</code> 无界面浏览器。不过其更通常的使用在于爬虫中使用，其主要是用于解决 <code>requests</code> 无法直接执行 <code>JavaScript</code> 代码的问题，不过用于解析 <code>Dom</code> 元素更有其妙用之处。</p><span id="more"></span><hr><h3 id="基础使用"><a href="#基础使用" class="headerlink" title="基础使用"></a>基础使用</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>在 <code>Python</code> 中使用 <code>Selenium</code> 需要通过 <code>pip</code> 安装即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install selenium</span><br></pre></td></tr></table></figure><h4 id="驱动程序"><a href="#驱动程序" class="headerlink" title="驱动程序"></a>驱动程序</h4><p><code>Selenium</code> 在安装之后还需要一个驱动程序来与浏览器交互，不同的浏览器都有自己的驱动程序。列表如下：</p><table><thead><tr><th>浏览器</th><th>下载链接</th></tr></thead><tbody><tr><td><code>Chrome</code></td><td><code>https://sites.google.com/chromium.org/driver/</code></td></tr><tr><td><code>Edge</code></td><td><code>https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/</code></td></tr><tr><td><code>Firefox</code></td><td><code>https://github.com/mozilla/geckodriver/releases</code></td></tr><tr><td><code>Safari</code></td><td><code>https://webkit.org/blog/6900/webdriver-support-in-safari-10/</code></td></tr></tbody></table><p>不过在新版本 <code>4.6.0</code> 之后，发行包中包含一个 <strong><code>Selenium Manager</code> 新工具</strong>，该工具帮助 <code>Selenium</code> 在发现驱动程序不可用时，会自动发现、下载并缓存驱动程序。</p><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><p>模拟点击<a href="https://baidu.com/">百度首页</a>输出其标题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://baidu.com&#x27;</span></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(url)</span><br><span class="line"></span><br><span class="line">title = browser.find_element(By.TAG_NAME, <span class="string">&#x27;title&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(title.get_attribute(<span class="string">&#x27;textContent&#x27;</span>))</span><br><span class="line"></span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure><hr><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><h4 id="元素定位"><a href="#元素定位" class="headerlink" title="元素定位"></a>元素定位</h4><p>通过方法查找元素的函数共有两个：</p><ul><li><code>find_element(By.KEY, &#39;KEY_NAME&#39;)</code></li><li><code>find_elements(By.KEY, &#39;KEY_NAME&#39;)</code></li></ul><p><code>By.KEY</code> 用于定位页面上的元素共有以下八种：</p><ul><li><code>ID</code> 通过元素的 <code>ID</code> 属性定位元素。</li><li><code>NAME</code> 通过元素的名称属性定位元素。</li><li><code>XPATH</code> 通过在 <code>XML</code> 文档中的 <code>XPATH</code> 节点定位元素。</li><li><code>LINK_TEXT</code> 通过锚标记中使用的链接文本定位元素。</li><li><code>PARTIAL_LINK_TEXT</code> 通过锚标记中使用的链接文本定位元素。</li><li><code>TAG_NAME</code> 通过标签名称定位元素。</li><li><code>CLASS_NAME</code> 通过类名定位元素。</li><li><code>CSS_SELECTOR</code> 使用 <code>CSS</code> 选择器语法定位元素。</li></ul><h4 id="WebElement-常用方法"><a href="#WebElement-常用方法" class="headerlink" title="WebElement 常用方法"></a><code>WebElement</code> 常用方法</h4><ol><li><code>clear()</code><br>如果元素是输入文本，则清除文本。</li><li><code>click()</code><br>单击该元素。</li><li><code>get_attribute()</code><br>获取元素的给定属性。</li><li><code>get_dom_attribute()</code>、<br>获取元素在 <code>HTML</code> 标记中声明的元素。</li><li><code>get_property()</code><br>获取元素的给定属性。</li><li><code>is_displayed()</code><br>该元素是否对用户可见。</li><li><code>is_enabled()</code><br>该元素使用可用。</li><li><code>is_selected()</code><br>该元素是否被选中。</li><li><code>send_keys()</code><br>模拟在元素中输入内容。</li><li><code>location</code><br>获取元素的坐标。</li><li><code>submit()</code><br>提交表格。</li><li><code>text</code><br>元素的文本。</li><li><code>tag_name</code><br>元素的 <code>tagName</code> 属性。</li></ol><h4 id="鼠标键盘事件"><a href="#鼠标键盘事件" class="headerlink" title="鼠标键盘事件"></a>鼠标键盘事件</h4><ol><li><code>click(on_element=None)</code><br>单击左键。</li><li><code>context_click(on_element=None)</code><br>单击右键。</li><li><code>double_click(on_element=None)</code><br>双击左键。</li><li><code>key_down(value, on_element=None)</code><br>按下键盘上的某个键。</li><li><code>key_up(value, on_element=None)</code><br>松开键盘上的某个键。</li><li><code>move_by_offset(xoffset, yoffset)</code><br>鼠标从当前位置移动到某个坐标。</li><li><code>move_to_element(to_element)</code><br>鼠标移动到某个元素。</li><li><code>move_to_element_with_offset(to_element, xoffset, yoffset)</code><br>移动到距某个元素（左上角坐标）多少距离的位置。</li><li><code>send_keys()</code><br>发送某个键或者输入文本到当前元素。</li></ol><h4 id="浏览器操作"><a href="#浏览器操作" class="headerlink" title="浏览器操作"></a>浏览器操作</h4><ol><li><p>获取页面的 <code>URL</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">browser.current_url</span><br></pre></td></tr></table></figure></li><li><p>获取页面日志</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">browser.maximize_window() # 最大化</span><br><span class="line">browser.fullscreen_window() # 全屏</span><br><span class="line">browser.minimize_window() # 最小化</span><br><span class="line">browser.get_window_position() # 获取窗口的坐标</span><br><span class="line">browser.get_window_rect() # 获取窗口的大小和坐标</span><br><span class="line">browser.set_window_position(100, 200) # 设置窗口的坐标</span><br><span class="line">browser.set_window_rect(100, 200, 32, 50) # 设置窗口的大小和坐标</span><br><span class="line">browser.set_window_size(200, 300) # 设置窗口的大小</span><br></pre></td></tr></table></figure></li><li><p>关闭页面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">browser.close() # 关闭当前标签页</span><br><span class="line">browser.quit() # 关闭浏览器并关闭驱动</span><br></pre></td></tr></table></figure></li><li><p>屏幕操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">browser.save_screenshot(&#x27;save.png&#x27;) # 截图，只支持 PNG 格式</span><br><span class="line">browser.get_screenshot_as_png() # 获取当前窗口的截图作为二进制数据</span><br><span class="line">browser.get_screenshot_as_base64() # 获取当前窗口的截图作为 base64 编码的字符串</span><br></pre></td></tr></table></figure></li><li><p>前进后退刷新</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">browser.forward() # 前进</span><br><span class="line">browser.back()  # 后退</span><br><span class="line">browser.refresh() # 刷新</span><br></pre></td></tr></table></figure></li></ol><h4 id="cookie-操作"><a href="#cookie-操作" class="headerlink" title="cookie 操作"></a><code>cookie</code> 操作</h4><ol><li><code>get_cookie(NAME)</code><br>获取指定键的 <code>Cookies</code>。</li><li><code>get_cookies()</code><br>获取所有的 <code>Cookies</code>。</li><li><code>delete_cookie(NAME)</code><br>删除指定键的 <code>Cookies</code>。</li><li><code>delete_all_cookies()</code><br>删除所有的 <code>Cookies</code>。</li></ol><h4 id="JavaScript-操作"><a href="#JavaScript-操作" class="headerlink" title="JavaScript 操作"></a><code>JavaScript</code> 操作</h4><p><code>Selenium</code> 可以自定义执行 <code>JavaScript</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">browser.execute_script(<span class="string">&quot;alter(&#x27;hello selenium!&#x27;)&quot;</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="示例-Demo"><a href="#示例-Demo" class="headerlink" title="示例 Demo"></a>示例 <code>Demo</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取居民消费价格指数 CPI 数据，数据来源于东方财富网。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_next_page</span>(<span class="params">browser</span>):</span><br><span class="line"><span class="comment"># 点击下一页，获取数据</span></span><br><span class="line">pagerbox = browser.find_elements(By.CLASS_NAME, <span class="string">&#x27;pagerbox&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">a = pagerbox.find_elements(By.TAG_NAME, <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> ai <span class="keyword">in</span> a:</span><br><span class="line"><span class="keyword">if</span> ai.text == <span class="string">&#x27;下一页&#x27;</span>:</span><br><span class="line">ai.send_keys(Keys.ENTER)</span><br><span class="line"><span class="built_in">print</span>(ai.text)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">get_data(browser)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">browser</span>):</span><br><span class="line"><span class="comment"># 解析数据结构</span></span><br><span class="line">table = browser.find_element(By.CLASS_NAME, <span class="string">&#x27;table-model&#x27;</span>)</span><br><span class="line">tbody = table.find_elements(By.TAG_NAME, <span class="string">&#x27;tbody&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">tr = tbody.find_elements(By.TAG_NAME, <span class="string">&#x27;tr&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(tr))</span><br><span class="line"><span class="comment"># 遍历数据结构并输出</span></span><br><span class="line"><span class="keyword">for</span> tr_e <span class="keyword">in</span> tr:</span><br><span class="line">td = tr_e.find_elements(By.TAG_NAME, <span class="string">&#x27;td&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> td_e <span class="keyword">in</span> td:</span><br><span class="line"><span class="built_in">print</span>(td_e.text)</span><br><span class="line"><span class="comment"># 下一页</span></span><br><span class="line">to_next_page(browser)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">url</span>):</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(url)</span><br><span class="line"><span class="comment"># 获取 CPI 数据</span></span><br><span class="line">get_data(browser)</span><br><span class="line">browser.quit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">url = <span class="string">&#x27;https://data.eastmoney.com/cjsj/cpi.html&#x27;</span></span><br><span class="line"><span class="comment"># 解析 URL 并输出数据</span></span><br><span class="line">parse_url(url)</span><br></pre></td></tr></table></figure><hr><h3 id="异常信息"><a href="#异常信息" class="headerlink" title="异常信息"></a>异常信息</h3><h4 id="NoSuchElementException"><a href="#NoSuchElementException" class="headerlink" title="NoSuchElementException"></a><code>NoSuchElementException</code></h4><p>在获取页面元素后，如果 <code>Dom</code> 发生变化就会出现该异常。</p><h4 id="stale-element-reference-stale-element-not-found"><a href="#stale-element-reference-stale-element-not-found" class="headerlink" title="stale element reference: stale element not found"></a><code>stale element reference: stale element not found</code></h4><p>分析该问题是因为在 <code>Dom</code> 树中未找到该元素。因此其解决方案就是保证在操作前，该元素存在且是可操作的。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="https://selenium-python.readthedocs.io/index.html">官方文档</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;Selenium&lt;/code&gt;&lt;/strong&gt; 最初是一个用于网站的自动化测试工具，支持各种 &lt;code&gt;Chrome&lt;/code&gt;、&lt;code&gt;Firefox&lt;/code&gt;、&lt;code&gt;Safari&lt;/code&gt; 等主流浏览器，同时也支持 &lt;code&gt;phantomJS&lt;/code&gt; 无界面浏览器。不过其更通常的使用在于爬虫中使用，其主要是用于解决 &lt;code&gt;requests&lt;/code&gt; 无法直接执行 &lt;code&gt;JavaScript&lt;/code&gt; 代码的问题，不过用于解析 &lt;code&gt;Dom&lt;/code&gt; 元素更有其妙用之处。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="https://blog.vgbhfive.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Linux-nc使用指南</title>
    <link href="https://blog.vgbhfive.cn/Linux-nc%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>https://blog.vgbhfive.cn/Linux-nc%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</id>
    <published>2023-11-02T14:59:32.000Z</published>
    <updated>2023-11-02T15:02:03.124Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><code>NetCat</code> 是一款调试 <code>TCP/UDP</code> 网络连接的利器，被称作是网络调试的瑞士军刀，可见其功能强大。</p><span id="more"></span><p><code>NetCat</code> 的基本功能如下：</p><ul><li><strong><code>telnet</code> 获取系统 <code>banner</code> 信息</strong></li><li><strong>传输文本信息</strong></li><li><strong>传输文件和目录</strong></li><li><strong>加密传输文件</strong></li><li><strong>端口扫描</strong></li><li><strong>远程控制</strong></li></ul><hr><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -h</span></span><br><span class="line">usage: nc [-46AacCDdEFhklMnOortUuvz] [-K tc] [-b boundif] [-i interval] [-p source_port]</span><br><span class="line">  [-s source_ip_address] [-w timeout] [-X proxy_version]</span><br><span class="line">  [-x proxy_address[:port]] [hostname] [port[s]]</span><br><span class="line">Command Summary:</span><br><span class="line">-4                            Use IPv4</span><br><span class="line">-6                            Use IPv6</span><br><span class="line">-A                            Set SO_RECV_ANYIF on socket</span><br><span class="line">-a                            Set SO_AWDL_UNRESTRICTED on socket</span><br><span class="line">-b ifbound                    Bind socket to interface</span><br><span class="line">-C                            Don&#x27;t use cellular connection</span><br><span class="line">-c                            Send CRLF as line-ending</span><br><span class="line">-D                            Enable the debug socket option</span><br><span class="line">-d                            Detach from stdin</span><br><span class="line">-E                            Don&#x27;t use expensive interfaces</span><br><span class="line">-F                            Do not use flow advisory (flow adv enabled by default)</span><br><span class="line">-G conntimo                   Connection timeout in seconds</span><br><span class="line">-H keepidle                   Initial idle timeout in seconds</span><br><span class="line">-h                            This help text</span><br><span class="line">-I keepintvl                  Interval for repeating idle timeouts in seconds</span><br><span class="line">-i secs                       Delay interval for lines sent, ports scanned</span><br><span class="line">-J keepcnt                    Number of times to repeat idle timeout</span><br><span class="line">-K tclass                     Specify traffic class</span><br><span class="line">-k                            Keep inbound sockets open for multiple connects</span><br><span class="line">-L num_probes                 Number of probes to send before generating a read timeout event</span><br><span class="line">-l                            Listen mode, for inbound connects</span><br><span class="line">-m                            Set SO_INTCOPROC_ALLOW on socket</span><br><span class="line">-N num_probes                 Number of probes to send before generating a write timeout event</span><br><span class="line">-o                            Issue socket options after connect/bind</span><br><span class="line">-n                            Suppress name/port resolutions</span><br><span class="line">-O                            Use old-style connect instead of connectx</span><br><span class="line">-p port                       Specify local port for remote connects (cannot use with -l)</span><br><span class="line">-r                            Randomize remote ports</span><br><span class="line">-s addr                       Local source address</span><br><span class="line">-t                            Answer TELNET negotiation</span><br><span class="line">-U                            Use UNIX domain socket</span><br><span class="line">-u                            UDP mode</span><br><span class="line">-v                            Verbose</span><br><span class="line">-w secs                       Timeout for connects and final net reads </span><br><span class="line">-X proto                      Proxy protocol: &quot;4&quot;, &quot;5&quot; (SOCKS) or &quot;connect&quot;</span><br><span class="line">-x addr[:port]                Specify proxy address and port</span><br><span class="line">-z                            Zero-I/O mode [used for scanning]</span><br><span class="line">Port numbers can be individual or ranges: lo-hi [inclusive]</span><br></pre></td></tr></table></figure><hr><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="监听端口"><a href="#监听端口" class="headerlink" title="监听端口"></a>监听端口</h4><p>持续监听 <code>8888</code> 端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -lk 8888</span></span><br><span class="line">GET /hello HTTP/1.1</span><br><span class="line">Host: localhost:8888</span><br><span class="line">User-Agent: curl/7.79.1</span><br><span class="line">Accept: */*</span><br></pre></td></tr></table></figure><h4 id="端口扫描"><a href="#端口扫描" class="headerlink" title="端口扫描"></a>端口扫描</h4><p>扫描 <code>IP</code> 为 <code>192.168.0.125</code> 的 <code>1-100</code> 端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -v -z -w1 192.168.0.125 1-100</span></span><br><span class="line">nc: connectx to 192.168.0.125 port xx (tcp) failed: Connection refused</span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -v -z -w1 192.168.0.125 80 <span class="comment"># 扫描单个端口</span></span></span><br><span class="line">nc: connectx to 192.168.0.125 port 80 (tcp) failed: Connection refused</span><br></pre></td></tr></table></figure><h4 id="传输文件"><a href="#传输文件" class="headerlink" title="传输文件"></a>传输文件</h4><p>两台机器之间传输文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">接收端（文件名可以自定义）</span></span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -lp 8888 &gt; tmp_receiver.tar.gz</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">发送端（`192.168.0.125` 为接收端的 `IP` 地址），接收端完成接收后自动退出</span></span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -nv 192.168.0.125 8888 -i 1 &lt; tmp_sender.tar.gz</span></span><br></pre></td></tr></table></figure><h4 id="远程控制"><a href="#远程控制" class="headerlink" title="远程控制"></a>远程控制</h4><p>被控端主动设置端口和 <code>bash</code> 环境（如果有防火墙，需开放端口，否则会被拦截）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">被控端</span></span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc -lvnp 8888 -c bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">控制端</span></span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash">nc 192.168.0.125 8888</span></span><br></pre></td></tr></table></figure><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>多使用，多总结，融会贯通。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;code&gt;NetCat&lt;/code&gt; 是一款调试 &lt;code&gt;TCP/UDP&lt;/code&gt; 网络连接的利器，被称作是网络调试的瑞士军刀，可见其功能强大。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="https://blog.vgbhfive.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Moto-摩旅日记</title>
    <link href="https://blog.vgbhfive.cn/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/"/>
    <id>https://blog.vgbhfive.cn/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/</id>
    <published>2023-10-03T03:20:11.195Z</published>
    <updated>2024-05-06T16:38:19.530Z</updated>
    
    <content type="html"><![CDATA[<h3 id="回忆"><a href="#回忆" class="headerlink" title="回忆"></a>回忆</h3><p>记得第一次出去骑车还是在大二的时候，记得路线是从学校去兴平，全程 <code>75</code> 公里，下午四点钟多到达酒店，酒店的那个阿姨人很好，她还允许我把我的自行车带到楼梯间锁起来，短暂休息后就去了杨贵妃墓，在市区里吃了烧烤和鸡蛋醪糟，第二天早早骑车又回了学校。<br>现在回想起来，当时简直就是无所畏惧，一个人带着手机骑着山地车就出发了🤦‍♂️</p><p><strong>多图警告！！！</strong></p><span id="more"></span><hr><h3 id="路线"><a href="#路线" class="headerlink" title="路线"></a>路线</h3><p>此次北京东北方向小环线摩旅主要途径地点：</p><ul><li>北京</li><li>唐山</li><li>秦皇岛</li><li>承德</li><li>北京</li></ul><p>每站之间的距离 <code>200</code> 公里左右，保证长途骑行不赶时间、夜晚有住宿的地方，剩余的时间可以多逛逛当地。</p><hr><h3 id="装备"><a href="#装备" class="headerlink" title="装备"></a>装备</h3><h4 id="车辆"><a href="#车辆" class="headerlink" title="车辆"></a>车辆</h4><p>本田 <code>NS 125 LA</code></p><h4 id="证件"><a href="#证件" class="headerlink" title="证件"></a>证件</h4><ul><li>身份证</li><li>驾驶证</li><li>行驶证</li></ul><h4 id="骑行装备"><a href="#骑行装备" class="headerlink" title="骑行装备"></a>骑行装备</h4><ul><li>头盔（建议全盔）</li><li>骑行服</li><li>手套</li><li>骑行靴</li></ul><h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul><li>辣条补胎工具套装</li><li>充气宝</li><li>螺丝刀、扳手套装</li><li>小刀</li></ul><p><small>建议出发前可以做一次保养，避免路上出现问题。</small></p><h4 id="通讯"><a href="#通讯" class="headerlink" title="通讯"></a>通讯</h4><ul><li>手机（记得带充电线）</li><li>充电宝（摩托车能充电的可以不带）</li></ul><h4 id="衣物"><a href="#衣物" class="headerlink" title="衣物"></a>衣物</h4><ul><li>换洗衣物两套</li><li>保暖内衣一套</li><li>速干内衣一套</li><li>洗漱套装</li><li>帽子、方巾</li><li>鞋子</li></ul><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ul><li>备用食品（饼干、水、巧克力、电解质水、口香糖、牛肉干）</li><li>现金</li><li>保温杯</li><li>常用药品（感冒药、创可贴、消炎药、碘伏、纱布、云南白药）</li><li>折叠椅（走到喜欢的地方可以随时坐下来）</li></ul><hr><h3 id="美景"><a href="#美景" class="headerlink" title="美景"></a>美景</h3><p>国庆第一天早晨 <code>10</code> 点出发<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-1.jpg" alt="img-1.jpg"></p><p>下午骑车有点困，就找了个路边小村子的背阴处休息，结果…，风挡直接给干废了😭，不过好在车没啥事<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-2.jpg" alt="img-2.jpg"></p><p>第一天的目的地是唐山，之前看过《唐山大地震》的电影，既然到了唐山就想去下纪念馆。到了目的地，看到那面遇难人员纪念墙直接给我震撼住了！<br><small>总共有十二面墙，每面墙上有二十四列、十二行，每一小块有八行、五列。</small><br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-3.jpg" alt="img-3.jpg"></p><p>摩旅的第二天中午就直接赶到了本次的重要中转点<strong>北戴河的黄金海岸</strong>，好家伙那叫一个人多，我呆了不到半个小时，我就跑路了😐（人是真的多，给我整害怕了）<br><small>出来后我就发挥了一个人出去玩的优势，地图上找到了一个捕渔码头《大浦河码头》，顺着路一直到门口后，不要右拐，再直着一直面向大海走，你就会获得一个专属的海滩。（记得车不要太靠近海边，跟我同路大哥的猛禽都陷进沙子里了😂）</small><br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-4.jpg" alt="img-4.jpg"></p><p>第二天的住宿是在山海关，竟然误打误撞的通过了山海关城墙下的一个单行隧道（类似于城墙门那种的），挺惊喜的（就是没办法拍照☹）<br>太困了就没怎么在海边玩，所以我第二天早起去看了日出，真的是嘎嘎好看（这是在山海关景区北边一公里的海边停车场）<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-5.jpg" alt="img-5.jpg"></p><p>从山海关刚出发进到山里，就看到了山头的长城。（真的佩服古人修了一万五千公里的城墙）<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-7.jpg" alt="img-7.jpg"></p><p>看日出起的太早了就又回去睡了一会，九点半直接出发承德，在路上喝咖啡休息的时候摘的一朵花（我给绑在了后座的包上，结果从承德回北京的路上就没有了😤）<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-6.jpg" alt="img-6.jpg"></p><p>承德出发回北京，去喝了当地的平泉羊汤。哇，第一口是真的好喝，强烈推荐烧饼夹肉（承德县清真平泉羊汤烧饼-康宁路店）<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-8.jpg" alt="img-8.jpg"></p><p>回京途中，误入修路工地，景色不错。<br><img src="/Moto-%E6%91%A9%E6%97%85%E6%97%A5%E8%AE%B0/img-9.jpg" alt="img-9.jpg"></p><hr><h3 id="趣事"><a href="#趣事" class="headerlink" title="趣事"></a>趣事</h3><p>在山海关的那天早起去看了日出，回去的时候在酒店有一家早餐店，就买了一杯豆浆和两根油条，临走的时候想再买一杯豆浆冲咖啡，然后那个卖早餐的阿姨就很疑惑，跟我说：我记得你早上吃早餐了呀！</p><p>原来在我们最不经意的地方，也是总有一些人在关注着我们。</p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>摩旅最重要的是过程而不是终点，享受在路上的每一分钟。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;回忆&quot;&gt;&lt;a href=&quot;#回忆&quot; class=&quot;headerlink&quot; title=&quot;回忆&quot;&gt;&lt;/a&gt;回忆&lt;/h3&gt;&lt;p&gt;记得第一次出去骑车还是在大二的时候，记得路线是从学校去兴平，全程 &lt;code&gt;75&lt;/code&gt; 公里，下午四点钟多到达酒店，酒店的那个阿姨人很好，她还允许我把我的自行车带到楼梯间锁起来，短暂休息后就去了杨贵妃墓，在市区里吃了烧烤和鸡蛋醪糟，第二天早早骑车又回了学校。&lt;br&gt;现在回想起来，当时简直就是无所畏惧，一个人带着手机骑着山地车就出发了🤦‍♂️&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多图警告！！！&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="Moto" scheme="https://blog.vgbhfive.cn/tags/Moto/"/>
    
  </entry>
  
  <entry>
    <title>MathJax语法指南</title>
    <link href="https://blog.vgbhfive.cn/MathJax%E8%AF%AD%E6%B3%95%E6%8C%87%E5%8D%97/"/>
    <id>https://blog.vgbhfive.cn/MathJax%E8%AF%AD%E6%B3%95%E6%8C%87%E5%8D%97/</id>
    <published>2023-09-27T15:43:21.000Z</published>
    <updated>2024-02-20T13:55:49.038Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong><code>MathJax</code></strong> 是一个 <code>JavaScript</code> 库，可以让你直接用 <code>Latex</code> 语法来写复杂数学公式，使用十分方便。</p><span id="more"></span><hr><h3 id="基础使用"><a href="#基础使用" class="headerlink" title="基础使用"></a>基础使用</h3><h4 id="如何配置"><a href="#如何配置" class="headerlink" title="如何配置"></a>如何配置</h4><p>关于如何在 <code>hexo</code> 中增加 <code>MathJax</code> 的支持，可以看这篇<a href="https://hexo-next.readthedocs.io/zh_CN/latest/next/advanced/%E9%85%8D%E7%BD%AEMathJax/">官方文档</a>。</p><h4 id="使用位置"><a href="#使用位置" class="headerlink" title="使用位置"></a>使用位置</h4><ol><li>行内 <code>$ x $</code>：$x$</li><li>换行 <code>$$ x $$</code>：$$x$$</li></ol><h4 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h4><ol><li><p>下标 <code>$x_1$</code>：$x_1$</p></li><li><p>上标 <code>$x^2$</code>：$x^2$</p></li></ol><h4 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h4><p><code>$\quad$</code></p><h4 id="控制括号大小"><a href="#控制括号大小" class="headerlink" title="控制括号大小"></a>控制括号大小</h4><p>使用 <code>\left</code> 和 <code>\right</code> 能自动控制不同层次括号的大小，需要配对使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\left( \frac&#123;3&#125;&#123;5&#125;  \left[ 3 + 2 * \left( a + b \right) \right] \right)</span><br></pre></td></tr></table></figure><p>$$ \left( \frac{3}{5}  \left[ 3 + 2 * \left( a + b \right) \right] \right) $$</p><h4 id="公式手动编号"><a href="#公式手动编号" class="headerlink" title="公式手动编号"></a>公式手动编号</h4><p>在公式书写时使用 <code>\tag&#123;num&#125;</code> 添加手动编号。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">f(x) = w·x + b \tag&#123;1&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{equation}<br>f(x) &#x3D; w·x + b \tag{1}<br>\end{equation}<br>$$</p><h4 id="多行公式对齐"><a href="#多行公式对齐" class="headerlink" title="多行公式对齐"></a>多行公式对齐</h4><p><code>begin&#123;split&#125;</code> 表示开始多行公式，<code>end&#123;split&#125;</code> 表示结束，公式中用 <code>\\</code> 表示回车到下一行，<code>&amp;</code> 表示对齐的位置，<code>\nonumber</code> 表示不展示公式编号。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">f(x) &amp;= x + 2x + 14 \\\\</span><br><span class="line">&amp;= 3x + 14</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">\nonumber</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{equation}<br>\begin{split}<br>f(x) &amp;&#x3D; x + 2x + 14 \\<br>&amp;&#x3D; 3x + 14<br>\end{split}<br>\nonumber<br>\end{equation}<br>$$</p><hr><h3 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h3><h4 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h4><table><thead><tr><th><code>latex</code></th><th>显示效果</th><th><code>latex</code></th><th>显示效果</th><th><code>latex</code></th><th>显示效果</th></tr></thead><tbody><tr><td><code>\alpha</code></td><td>$\alpha$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\beta</code></td><td>$\beta$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\gamma</code></td><td>$\gamma$</td><td><code>\Gamma</code></td><td>$\Gamma$</td><td><code>\digamma</code></td><td>$\digamma$</td></tr><tr><td><code>\delta</code></td><td>$\delta$</td><td><code>\Delta</code></td><td>$\Delta$</td><td></td><td></td></tr><tr><td><code>\epsilon</code></td><td>$\epsilon$</td><td><code>\varepsilon</code></td><td>$\varepsilon$</td><td></td><td></td></tr><tr><td><code>\zeta</code></td><td>$\zeta$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\eta</code></td><td>$\eta$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\theta</code></td><td>$\theta$</td><td><code>\Theta</code></td><td>$\Theta$</td><td><code>\vartheta</code></td><td>$\vartheta$</td></tr><tr><td><code>\iota</code></td><td>$\iota$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\kappa</code></td><td>$\kappa$</td><td><code>\varkappa</code></td><td>$\varkappa$</td><td></td><td></td></tr><tr><td><code>\lambda</code></td><td>$\lambda$</td><td><code>\Lambda</code></td><td>$\Lambda$</td><td></td><td></td></tr><tr><td><code>\mu</code></td><td>$\mu$</td><td><code>\nu</code></td><td>$\nu$</td><td></td><td></td></tr><tr><td><code>\xi</code></td><td>$\xi$</td><td><code>\Xi</code></td><td>$\Xi$</td><td></td><td></td></tr><tr><td><code>\pi</code></td><td>$\pi$</td><td><code>\Pi</code></td><td>$\Pi$</td><td><code>\varpi</code></td><td>$\varpi$</td></tr><tr><td><code>\rho</code></td><td>$\rho$</td><td><code>\varrho</code></td><td>$\varrho$</td><td></td><td></td></tr><tr><td><code>\sigma</code></td><td>$\sigma$</td><td><code>\Sigma</code></td><td>$\Sigma$</td><td><code>\varsigma</code></td><td>$\varsigma$</td></tr><tr><td><code>\tau</code></td><td>$\tau$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\upsilon</code></td><td>$\upsilon$</td><td><code>\Upsilon</code></td><td>$\Upsilon$</td><td></td><td></td></tr><tr><td><code>\phi</code></td><td>$\phi$</td><td><code>\Phi</code></td><td>$\Phi$</td><td><code>\varphi</code></td><td>$\varphi$</td></tr><tr><td><code>\chi</code></td><td>$\chi$</td><td></td><td></td><td></td><td></td></tr><tr><td><code>\psi</code></td><td>$\psi$</td><td><code>\Psi</code></td><td>$\Psi$</td><td></td><td></td></tr><tr><td><code>\omega</code></td><td>$\omega$</td><td><code>\Omega</code></td><td>$\Omega$</td><td></td><td></td></tr></tbody></table><h4 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h4><ol><li><p>求和<br><code>\sum</code>：$\sum$</p></li><li><p>偏导<br><code>\partial</code>：$\partial$</p></li><li><p>梯度<br><code>\nabla</code>：$\nabla$</p></li><li><p>取整<br>向下取整 <code>\lfloor x \rfloor</code>：$\lfloor x \rfloor$ <br><br>向上取整 <code>\lceil x \rceil</code>：$\lceil x \rceil$</p></li></ol><h4 id="箭头符号"><a href="#箭头符号" class="headerlink" title="箭头符号"></a>箭头符号</h4><table><thead><tr><th><code>latex</code></th><th>显示效果</th><th><code>latex</code></th><th>显示效果</th></tr></thead><tbody><tr><td><code>\uparrow</code></td><td>$\uparrow$</td><td><code>\Uparrow</code></td><td>$\Uparrow$</td></tr><tr><td><code>\downarrow</code></td><td>$\downarrow$</td><td><code>\Downarrow</code></td><td>$\Downarrow$</td></tr><tr><td><code>\updownarrow</code></td><td>$\updownarrow$</td><td><code>\Updownarrow</code></td><td>$\Updownarrow$</td></tr><tr><td><code>\leftarrow</code></td><td>$\leftarrow$</td><td><code>\Leftarrow</code></td><td>$\Leftarrow$</td></tr><tr><td><code>\rightarrow</code></td><td>$\rightarrow$</td><td><code>\Rightarrow</code></td><td>$\Rightarrow$</td></tr><tr><td><code>\leftrightarrow</code></td><td>$\leftrightarrow$</td><td><code>\Leftrightarrow</code></td><td>$\Leftrightarrow$</td></tr><tr><td><code>\leftharpoonup</code></td><td>$\leftharpoonup$</td><td><code>\leftharpoondown</code></td><td>$\leftharpoondown$</td></tr><tr><td><code>\rightharpoonup</code></td><td>$\rightharpoonup$</td><td><code>\rightharpoondown</code></td><td>$\rightharpoondown$</td></tr><tr><td><code>\upharpoonright</code></td><td>$\upharpoonright$</td><td><code>\downharpoonright</code></td><td>$\downharpoonright$</td></tr></tbody></table><h4 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a>特殊符号</h4><table><thead><tr><th><code>latex</code></th><th>显示效果</th></tr></thead><tbody><tr><td><code>\aleph</code></td><td>$\aleph$</td></tr><tr><td><code>\beth</code></td><td>$\beth$</td></tr><tr><td><code>\daleth</code></td><td>$\daleth$</td></tr><tr><td><code>\gimel</code></td><td>$\gimel$</td></tr></tbody></table><hr><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><ol><li><p>方括号矩阵</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">A =</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">1 &amp; 0 &amp; 2 \\\\</span><br><span class="line">-2 &amp; 1 &amp; 3 \\\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>A &#x3D;<br>\begin{bmatrix}<br>1 &amp; 0 &amp; 2 \\<br>-2 &amp; 1 &amp; 3 \\<br>\end{bmatrix}<br>$$</p></li><li><p>无括号矩阵</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;matrix&#125;</span><br><span class="line">1 &amp; x &amp; x^2 \\\\</span><br><span class="line">1 &amp; y &amp; y^2 \\\\</span><br><span class="line">1 &amp; z &amp; z^2 \\\\</span><br><span class="line">\end&#123;matrix&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{matrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2 \\<br>\end{matrix}<br>$$</p></li><li><p>竖线矩阵</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\left[</span><br><span class="line">    \begin&#123;array&#125;&#123;cc|c&#125;</span><br><span class="line">      1&amp;2&amp;3\\\\</span><br><span class="line">      4&amp;5&amp;6</span><br><span class="line">    \end&#123;array&#125;</span><br><span class="line">\right]</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\left[<br> \begin{array}{cc|c}<br>   1&amp;2&amp;3\\<br>   4&amp;5&amp;6<br> \end{array}<br>\right]<br>$$</p></li></ol><h4 id="方程组"><a href="#方程组" class="headerlink" title="方程组"></a>方程组</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">x = x_1 + t_1(x_2 - x_1) \\\\</span><br><span class="line">y = y_1 + t_1(y_2 - y_1)</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">\quad t_1 \in [0,1]</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{cases}<br>x &#x3D; x_1 + t_1(x_2 - x_1) \\<br>y &#x3D; y_1 + t_1(y_2 - y_1)<br>\end{cases}<br>\quad t_1 \in [0,1]<br>$$</p><h4 id="多行公式"><a href="#多行公式" class="headerlink" title="多行公式"></a>多行公式</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial^2 f&#125;&#123;\partial&#123;x^2&#125;&#125; &amp;= \frac&#123;\partial(\Delta_x f(i,j))&#125;&#123;\partial x&#125; = \frac&#123;\partial(f(i+1,j)-f(i,j))&#125;&#123;\partial x&#125; \\\\</span><br><span class="line">&amp;= \frac&#123;\partial f(i+1,j)&#125;&#123;\partial x&#125; - \frac&#123;\partial f(i,j)&#125;&#123;\partial x&#125; \\\\</span><br><span class="line">&amp;= f(i+2,j) -2f(f+1,j) + f(i,j)</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">\nonumber</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial^2 f}{\partial{x^2}} &amp;&#x3D; \frac{\partial(\Delta_x f(i,j))}{\partial x} &#x3D; \frac{\partial(f(i+1,j)-f(i,j))}{\partial x} \\<br>&amp;&#x3D; \frac{\partial f(i+1,j)}{\partial x} - \frac{\partial f(i,j)}{\partial x} \\<br>&amp;&#x3D; f(i+2,j) -2f(f+1,j) + f(i,j)<br>\end{split}<br>\nonumber<br>\end{equation}<br>$$</p><h4 id="多行分支等式"><a href="#多行分支等式" class="headerlink" title="多行分支等式"></a>多行分支等式</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">f(n) =</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">n/2,  &amp; \text&#123;if $n$ is even&#125; \\\\</span><br><span class="line">3n+1, &amp; \text&#123;if $n$ is odd&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>$$<br>f(n) &#x3D;<br>\begin{cases}<br>n&#x2F;2,  &amp; \text{n &gt;&#x3D; 0} \\<br>3n+1, &amp; \text{n &lt; 0}<br>\end{cases}<br>$$</p><hr><h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h3><p><a href="https://mhchem.github.io/MathJax-mhchem/">test drive</a></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;MathJax&lt;/code&gt;&lt;/strong&gt; 是一个 &lt;code&gt;JavaScript&lt;/code&gt; 库，可以让你直接用 &lt;code&gt;Latex&lt;/code&gt; 语法来写复杂数学公式，使用十分方便。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Utils" scheme="https://blog.vgbhfive.cn/tags/Utils/"/>
    
  </entry>
  
  <entry>
    <title>ML-逻辑回归</title>
    <link href="https://blog.vgbhfive.cn/ML-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://blog.vgbhfive.cn/ML-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2023-09-21T13:59:51.511Z</published>
    <updated>2023-09-28T17:02:56.860Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><strong>逻辑回归（<code>logistic regression</code>）</strong>是统计学习中的经典分类方法，虽然被称为回归，但其实是个分类模型。其本质是假设某个数据服从逻辑分布，就可以使用极大似然法估计出其线性回归的参数，之后再使用 <code>Sigmoid</code> 逻辑函数对其分类。<br><small>面试的时候千万不要说你很了解 <code>LR</code>，因为细节真的太多了😂</small></p><span id="more"></span><hr><h3 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h3><h4 id="逻辑回归分布"><a href="#逻辑回归分布" class="headerlink" title="逻辑回归分布"></a>逻辑回归分布</h4><p>设 <code>X</code> 是连续随机变量，<code>X</code> 服从逻辑分布是指具有下列<strong>分布函数</strong>和<strong>密度函数</strong>：<br>$$ F(x) &#x3D; P(X&lt;&#x3D;x) &#x3D; {\frac {1} {1 + e^{-(x-\mu)&#x2F;\gamma}}} $$<br>$$ f(x) &#x3D; F^‘(x) &#x3D; {\frac {e^{-(x-\mu)&#x2F;\gamma}} {\gamma(1+e^{-(x-\mu)&#x2F;\gamma})^2}} $$<br><small>$\mu$ 为位置参数，$\gamma&gt;0$ 为形状参数。</small></p><p><img src="/ML-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/logistic_1.jpg" alt="logistic_1.png"><br>分布函数属于逻辑函数，其分布是一条 <strong><code>S</code> 型曲线（<code>sigmoid curve</code>）</strong>，该曲线以点 $(\mu, {\frac {1} {2}})$ 为中心对成，即满足<br>$$ F(-x + \mu) - {\frac {1} {2}} &#x3D; -F(x-\mu) + {\frac {1} {2}} $$<br>该曲线在中心附近增长速度较快，在两端增长速度较慢。同时逻辑回归的分布函数在 $\mu&#x3D;0,\gamma&#x3D;1$ 的特殊参数下就是 <strong><code>Sigmoid</code> 逻辑函数</strong>。<br><small>形状参数 $\gamma$ 越小，曲线在中心位置的增长越快。</small></p><h4 id="二项逻辑回归模型"><a href="#二项逻辑回归模型" class="headerlink" title="二项逻辑回归模型"></a>二项逻辑回归模型</h4><p><strong>二项逻辑回归模型（<code>binomial logistic regression model</code>）</strong>是一种分类模型，由条件概率分布 <code>P(Y|X)</code> 表示，形式为参数化的逻辑分布。<br>$$ P(Y&#x3D;1|x) &#x3D; {\frac {\exp{(w·x+b)}} {1+\exp{(w·x+b)}}} $$<br>$$ P(Y&#x3D;0|x) &#x3D; {\frac {1} {1+\exp{(w·x+b)}}} $$<br><small>随机变量 <code>X</code> 取值为实数，随机变量 <code>Y</code> 取值为 <code>1</code> 或 <code>0</code>。</small><br><small>$x\in R^n$ 是输入，$Y\in{0,1}$ 是输出，$w\in R^n$ 和 $b\in R$ 是参数，$w$ 称为权值向量，$b$ 称为偏置，$w·x$ 是 $w$ 和 $x$ 的内积。</small></p><p>至此，对于给定的输入实例，可以根据上述公式求得 $P(Y&#x3D;1|x)$ 和 $P(Y&#x3D;0|x)$ 的概率，逻辑回归比较两个值的大小，然后将实例分到概率较大的一类中。</p><p>某个事件的<strong>几率（<code>odds</code>）</strong>是指该事件发生的概率与不发生概率的比值。如果事件发生的概率为 $p$，那么该事件的几率就是 ${\frac {p} {1-p}}$，该事件的<strong>对数几率（<code>log odds</code>）</strong>或 <strong><code>logit</code> 函数</strong>是：<br>$$ logit(p) &#x3D; log {\frac {p} {1-p}} $$</p><p>而对于逻辑回归而言，即可得：<br>$$ log {\frac {P(Y&#x3D;1|x)} {1-P(Y&#x3D;1|x)}} &#x3D; w·x $$</p><p>也就是说在逻辑回归模型中，输出 $Y&#x3D;1$ 的对数几率是输入 $x$ 的线性函数，即可将线性函数 $w·x$ 转换为概率：<br>$$ P(Y&#x3D;1|x) &#x3D; {\frac {exp(w·x)} {1+exp(w·x)}} $$</p><p>此时线性函数的值越接近正无穷，概率值就越接近 <code>1</code>，线性函数的值越接近负无穷，概率值就越接近 <code>0</code>，即此模型就是概率回归模型。</p><h4 id="多项逻辑回归模型"><a href="#多项逻辑回归模型" class="headerlink" title="多项逻辑回归模型"></a>多项逻辑回归模型</h4><p>前面说的二项逻辑回归模型可以通过扩展随机变量 <code>Y</code> 的取值集合 $Y\in{1,2,3,…,K}$，那么<strong>多项逻辑回归</strong>的模型是：<br>$$ P(Y&#x3D;k|x) &#x3D; {\frac {exp(w_k·x)} {1+sum_{k&#x3D;1}^{K-1} exp(w_k·x)}} , \quad k &#x3D; {1, 2, 3, …, K}$$<br><small>其中 $x\in R^{n+1}, w_k\in R^{n+1}$</small></p><hr><h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3><p>在逻辑回归的学习中，对于给定的数据集，可以使用极大似然估计法估计模型参数，从而得到逻辑回归模型<br>假设：<br>$$ P(Y&#x3D;1|X) &#x3D; \pi(x), \quad P(Y&#x3D;0|X) &#x3D; 1 - \pi(x) $$<br>似然函数：<br>$$ \Pi_{i&#x3D;1}^N [\pi(x_i)]^{y_i} [1-\pi(x_i)]^{1-y_i} $$<br>对数似然函数：<br>$$\begin{equation}\begin{split}<br>L(w) &amp;&#x3D; sum_{i&#x3D;1}^N [y_i log \pi(x_i) + (1-y_i) log(1-\pi(x_i))] \\<br>&amp;&#x3D; sum_{i&#x3D;1}^N [y_i log {\frac {\pi(x_i)} {1-\pi(x_i)} + log(1-\pi(x_i))}] \\<br>&amp;&#x3D; sum_{i&#x3D;1}^N [y_i(w·x_i) - log(1+exp(w·x_i))]<br>\end{split}\nonumber\end{equation}$$<br>对 $L(w)$ 求极大值，从而得到 $w$ 的估计值。</p><p>至此问题就变成了以对数似然函数为目标的最优化问题。逻辑斯谛回归中通常采用的方法是：</p><ul><li><strong>改进的迭代尺度法</strong></li><li><strong>梯度下降法</strong></li><li>牛顿法</li><li><strong>拟牛顿法</strong></li></ul><h4 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h4><p><strong>改进的迭代尺度法（<code>improved iterative scaling, IIS</code>）</strong>是一种最大熵模型学习的最优化算法。<br><code>IIS</code> 的想法是：假设最大熵模型当前的参数向量是 $w&#x3D;(w_1,w_2,…,w_n)^T$，那希望找到新的参数向量 $w+\delta &#x3D; (w_1+\delta_1,w_2+\delta_2,…,w_n+\delta_n)^T$，以此使得模型的对数似然函数值增大。即找到一种参数向量的更新方法：$\tau:w \rightarrow w+\delta$，那么就可以使用这一方法，直至找到对数似然函数的最大值。</p><p><strong>改进的迭代尺度算法 <code>IIS</code></strong> ：</p><blockquote><p>输入：特征函数 $f_1,f_2,…,f_n$；经验分布 $\widetilde{P}(X,Y)$，模型 $P_w(y|x)$ <br><br>输出：最优参数值 $w_i^*$；最优模型 $P_{w^*}$ <br><br>过程：</p><ol><li>对所有的 $i \in {1,2,…,n}$，取初始值 $w_i&#x3D;0$。</li><li>对每一个 $i \in {1,2,…,n}$：<br><br> a. 令 $\delta_i$ 是方程 $sum_{x,y} \widetilde{P}(x) P(y|x) f_i(x,y) \exp{(\delta_i sum_{i&#x3D;1}^nf_i(x,y))} &#x3D; E_{\widetilde{P}}(f_i)$ 的解。<br><br>  b. 更新 $w_i$ 值：$w_i \leftarrow w_i + \delta_i$。</li><li>如果所有的 $w_i$ 都收敛，则继续重复步骤 <code>2</code>。<br> 这一步骤的关键是求解 $\delta_i$。如果 $sum_{i&#x3D;1}^nf_i(x,y)$ 是常数，即对所有的 <code>x,y</code> 都有 $sum_{i&#x3D;1}^nf_i(x,y) &#x3D; M$，那么 $\delta_i$ 就可以显示地表示为<br> $$ \delta_i &#x3D; {\frac {1} {M}} \log{\frac {E_{\widetilde{P}} (f_i)} {E_P (f_i)}} $$<br> 如果 $sum_{i&#x3D;1}^nf_i(x,y)$ 不是常数，则必须通过数值计算求 $\delta_i$。</li></ol></blockquote><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p><strong>梯度下降法（gradient descent）</strong>的计算过程就是沿着梯度下降的方向求解极小值（也可以沿着梯度上升的方向求最大值）。<br>$$ \theta_1 &#x3D; \theta_0 - \alpha {\frac {\delta} {\delta \theta_j}} J(\theta) $$<br><small>$\alpha$ 被称为学习率或步长，用来控制每一步行走的距离。其不能太大，也不能太小。</small></p><p><strong>梯度下降法</strong>：</p><blockquote><p>输入：特征函数 $f_1,f_2,…,f_n$；经验分布 $\widetilde{P}(X,Y)$，模型 $P_w(y|x)$ <br><br>输出：最优参数值 $w_i^*$；最优模型 $P_{w^*}$ <br><br>过程：</p><ol><li>初始化终止距离 $\epsilon$ 以及步长 $\alpha$。</li><li>确定当前位置的损失函数梯度，对于 $\theta_i$，其梯度表达式如下：<br> $$ {\frac {\delta} {\delta \theta_j}} J(\theta) $$</li><li>使用步长乘以损失函数的梯度得到当前的下降距离，即 $\alpha {\frac {\delta} {\delta \theta_j}} J(\theta)$。</li><li>确定是否所有的 $\theta_i$ 都小于终止距离 $\epsilon$，如果都小于则确定当前的 $\theta_i$ 为本次计算结果。</li><li>更新表达式 $\theta_i &#x3D; \theta_i - \alpha {\frac {\delta} {\delta \theta_i}} J(\theta)$ 然后转入步骤 <code>2</code>。</li></ol></blockquote><h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>最大熵模型还可以应用<strong>牛顿法</strong>或<strong>拟牛顿法</strong>。</p><p><strong>最大熵模型学习的 <code>BFGS</code> 算法</strong>：</p><blockquote><p>输入：特征函数 $f_1,f_2,…,f_n$；经验分布 $\widetilde{P}(X,Y)$，目标函数 $f(w)$，梯度 $g(w)&#x3D; \nabla{f(w)}$，精度要求 $\epsilon$ <br><br>输出：最优参数值 $w_i^*$；最优模型 $P_{w^*}$ <br><br>过程：</p><ol><li>选定初识点 $w^{(0)}$，取 $B_0$ 为正定对称矩阵，置 $k&#x3D;0$</li><li>计算 $g_k&#x3D;g(w^{(k)})$ 。若 $||g_k||&lt;\epsilon$ 则停止计算，得 $w^*&#x3D;w^{(k)}$；否则转步骤 <code>3</code></li><li>由 $B_k p_k &#x3D; -g_k$ 求出 $p_k$</li><li>一维搜索：求 $\lambda_k$ 使得<br> $$ f(w^{(k)} + \lambda_k p_k) &#x3D; min_{\lambda&gt;&#x3D;0} f(w^{(k)} + \lambda p_k) $$</li><li>置 $w^{(k+1)} &#x3D; w^{(k)} + \lambda_k p_k$</li><li>计算 $g_{k+1} &#x3D; g()w^{(k+1)}$，若 $||g_{k+1}|| &lt; \epsilon$ 则停止计算，得 $w^*&#x3D;w^{(k+1)}$；否则求出 $B_{k+1}$：<br> $$ B_{k+1} &#x3D; B_k + {\frac {y_k y_k^T} {y_k^T \delta_k}} - {\frac {B_k \delta_k \delta_k^T B_k} {\delta_k^T B_k \delta_k}} $$<br> <small>其中 $y_k &#x3D; g_{k+1}-g_k, \delta_k &#x3D; w^{(k+1)}-w^{(k)}$</small></li><li>置 $k&#x3D;k+1$，转步骤 <code>3</code>。</li></ol></blockquote><hr><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>正则化是一个通用的算法和思想，任何会产生过拟合现象的算法都可以使用正则化来避免过拟合。在经验风险最小化的基础上（训练误差最小化），尽可能地采用简单模型，可以有效提高泛化预测精度。而正则化之所以有效是因为其降低了特征的权重，使得模型更加简单。</p><p>正则化一般采用 <code>L1</code> 范式或 <code>L2</code> 范式，其形式分别为 $\Phi(w) &#x3D; ||x||_1$ 和 $\Phi(w) &#x3D; ||x||_2$。</p><h5 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a><code>L1</code> 正则化</h5><p><strong><code>L1</code> 正则化</strong>也称为 <strong><code>LASSO</code> 回归</strong>，其为模型增加了一个参数 <code>w</code>，而此参数 <code>w</code> 服从<strong>零均值拉普拉斯分布</strong>：<br>$$ f(w|\mu,b) &#x3D; {\frac {1} {2b}} \exp{-{\frac {|w-\mu|} {b}}} $$</p><p>那么之后的似然函数如下：<br>$$\begin{equation}\begin{split}<br>L(w) &amp;&#x3D; P(y|w,x) P(w) \\<br>&amp;&#x3D; \Pi_{i&#x3D;1}^N p(x_i)^{y_i} (1-p(x_i))^{1-y_i} \Pi_{j&#x3D;1}^d {\frac {1} {2b}} \exp({-{\frac {|w_j|} {b}}})<br>\end{split}\nonumber\end{equation}$$</p><p>对该式子取 <code>log</code> 再取负可得目标函数：<br>$$ -\ln{L(w)} &#x3D; - sum_i[y_i \ln{p(x_i)} + (1-y_i) \ln{(1-p(x_i))}] + {\frac {1} {2b^2}} sum_j |w_j| $$</p><p>等价于原始损失函数的后面再加上了 <code>L1</code> 正则项，因此 <code>L1</code> 的本质就是为模型增加<strong>模型参数服从零均值拉普拉斯分布</strong>的这一特点。</p><h5 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a><code>L2</code> 正则化</h5><p><strong><code>L2</code> 正则化</strong>也称 <strong><code>Ridge</code> 回归</strong>，其本质是为模型增加了一个参数 <code>w</code>，而此参数服从<strong>零均值正态分布</strong>。<br>$$ f(w|\mu,)\sigma &#x3D; {\frac {1} {\sqrt{2\pi} \sigma}} \exp{(- {\frac {(w-\mu)^2} {2 \sigma^2}})} $$</p><p>那么之后的似然函数如下：<br>$$\begin{equation}\begin{split}<br>L(w) &amp;&#x3D; P(y|w,x)P(w) \\<br>&amp;&#x3D; \Pi_{i&#x3D;1}^N p(x_i)^{y_i} (1-p(x_i))^{1-y_i} \Pi_{j&#x3D;1}^d {\frac {1} {\sqrt{2\pi}\sigma}} \exp{(- {\frac {w_j^2} {2\sigma^2}})} \\<br>&amp;&#x3D; \Pi_{i&#x3D;1}^N p(x_i)^{y_i} (1-p(x_i))^{1-y_i} {\frac {1} {\sqrt{2\pi}\sigma}} \exp{(- {\frac {w^T w} {2\sigma^2}})}<br>\end{split}\nonumber\end{equation}$$</p><p>对上式子取 <code>log</code> 再取负可得到目标函数：<br>$$ - \log{L(w)} &#x3D; - sum_i [y_i \log{p(x_i)} + (1 - y_i) \log{(1-p(x_i))}] + {\frac {1} {2 \sigma^2}} w^T w + const $$</p><p>上述公式等价于原始的 <code>cross - entropy</code> 之后再加上 <code>L2</code> 正则项，至此 <code>L2</code> 正则的本质也就是为模型增加<strong>模型参数服从零均值正态分布</strong>这一特点。 </p><h5 id="L1-正则与-L2-正则的区别"><a href="#L1-正则与-L2-正则的区别" class="headerlink" title="L1 正则与 L2 正则的区别"></a><code>L1</code> 正则与 <code>L2</code> 正则的区别</h5><p><code>L1</code> 正则化增加了所有权重 <code>w</code> 参数的绝对值之和，使得更多的 <code>w</code> 趋向于零，从而导致模型参数变得稀疏。而 <code>L2</code> 正则化则是增加了所有权重 <code>w</code> 参数的平方之和，使得参数 <code>w</code> 仅可能趋向于零但不为零，以此导致模型参数更加稠密，从而防止过拟合。</p><p><img src="/ML-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/logistic_3.jpg" alt="logistic_3"></p><h4 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h4><p>在逻辑回归的求解过程中，无论是改进的迭代尺度法还是牛顿法，其本质都是需要计算梯度的，因此逻辑回归的并行化主要是对<strong>目标函数梯度计算的并行化</strong>。</p><p><a href="https://ssjcoding.github.io/2018/11/20/gradient-descent/">梯度下降法详解</a></p><h4 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h4><p>😭<code>TODO</code></p><hr><h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p><strong>最大熵模型（<code>maximum entropy model</code>）</strong>是由最大熵原理推导而成。最大熵原理认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型也就是最好的模型。通常使用约束条件来确定概率模型的集合，而最大熵原理也可以表示为在满足约束条件的模型集合中选取熵最大的模型。<br>假设离散随机变量 $X$ 的概率分布是 $P(X)$，则其熵为：<br>$$ H(P) &#x3D; - sum_x P(x) logP(x) $$<br><small>熵需要满足 $0&lt;&#x3D;H(P)&lt;&#x3D;log|X|$，其中 $|X|$ 是 <code>X</code> 的取值个数，当且仅当 <code>X</code> 的分布是均匀分布是右边的等号才会成立。（<code>X</code> 服从均匀分布时，其熵最大）</small></p><p>概率模型集合可使用由欧氏空间中的<strong>单纯形</strong>表示，其中一个点表示一个模型，整个单纯形代表模型集合。单纯形中的直线表示约束条件，直线的交集对应于满足所有约束条件的模型集合。然而这样的模型会存在多个，最大熵原理则是在众多的模型中选择最优的模型。<br><small>单纯形是指在 <code>n</code> 维欧氏空间中的 <code>n+1</code> 个仿射无关的点的集合的凸包。</small><br><img src="/ML-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/logistic_2.png" alt="logistic_2.png"></p><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>最大熵原理是统计学习中的一般原理，将它应用到分类得到最大熵模型。假设分类模型是一个条件概率分布 $P(Y|X)$，其表示对于给定的输入 <code>X</code>，会以条件概率 $P(Y|X)$ 输出 <code>Y</code>，其中 $X,Y$ 分别表示输入和输出。</p><p>给定一个训练数据集 $T&#x3D;{(x_1, y_1), …, (x_N, y_N)}$，学习的目标是使用最大熵原理选择最好的分类模型。对于给定的训练数据集，可以确定联合分布 $P(X, Y)$ 的经验分布和边缘分布 $P(X)$ 的经验分布，分别以 $\widetilde{P}(X, Y)$ 和 $\widetilde{P}(X)$ 表示：<br>$$ \widetilde{P}(X&#x3D;x, Y&#x3D;y) &#x3D; {\frac {v(X&#x3D;x, Y&#x3D;y)} {N}} $$<br>$$ \widetilde{P}(X&#x3D;x) &#x3D; {\frac {v(X&#x3D;x)} {N}} $$<br><small>$v(X&#x3D;x, Y&#x3D;y)$ 表示训练数据中样本 $(x, y)$ 出现的频数，$v(X&#x3D;x)$ 表示训练数据中输出 <code>x</code> 出现的频数，<code>N</code> 表示训练样本容量。</small><br><br><br>首要模型需要考虑满足条件，可以使用<strong>特征函数（<code>feature function</code>）</strong> $f(x, y)$ 描述输入 $x$ 和输出 $y$ 之间的某一个事实，其定义为：<br>$$<br>f(x, y) &#x3D;<br>\begin{cases}<br>1,  &amp; \text{x 与 y 满足某一条件} \\<br>0, &amp; \text{否则}<br>\end{cases}<br>$$<br><small>上述定义为一个<strong>二值函数</strong>，当 <code>x</code> 和 <code>y</code> 满足某个事实时取值为 <code>1</code>，否则为 <code>0</code>。</small><br>特征函数 $f(x, y)$ 关于经验分布 $\widetilde{P}(X, Y)$ 的期望值用 $E_\widetilde{p} (f)$ 来表示：<br>$$ E_\widetilde{p} (f) &#x3D; sum_{x,y} \widetilde{P}(x,y) f(x,y) $$<br>特征函数 $f(x, y)$ 关于模型 $P(Y|X)$ 与经验分布 $\widetilde{P}(X)$ 的期望值用 $E_\widetilde{p}(f)$ 来表示：<br>$$ E_p (f) &#x3D; sum_{x,y} E_\widetilde{p}(x) P(y|x) f(x,y) $$<br>那么在训练数据集中获得足够多的条件，就可以假设两个期望值相等，即：<br>$$ E_p (f) &#x3D; E_\widetilde{p}(f) $$<br>$$ sum_{x,y} \widetilde{P}(x) P(y|x) f(x,y) &#x3D; sum_{x,y} \widetilde{P}(x,y) f(x,y) $$<br><br><br>假设满足所有约束条件的模型集合为 $\varsigma &#x3D; {p \in P|E_p(f_i) &#x3D; E_\widetilde{p}(f_i), i&#x3D;1,2, …, n}$，定义在条件概率分布 $P(Y|X)$ 上的条件熵为：<br>$$ H(P) &#x3D; - sum_{x,y} \widetilde{P}(x) P(y|x) logP(y|x) $$<br>则模型集合 $\varsigma$ 中条件熵 $H(P)$ 最大的模型称为最大熵模型，而公式中的对数为<strong>自然对数</strong>。</p><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>最大熵模型的学习过程就是求解最大熵模型的过程，最大熵模型的学习可以形式化为最优化问题。</p><p>对于给定的训练数据集 $T &#x3D; {(x_1, y_1), …, (x_N, y_N)}$ 以及特征函数 $f_i(x,y), i&#x3D;1,2,…,N$，最大熵模型的学习等价于约束最优化问题：<br>$$ max_{P \in \varsigma} H(P) &#x3D; - sum_{x,y} \widetilde{P}(x) P(y|x) logP(y|x) $$<br>$$ E_p(f_i) &#x3D; E_\widetilde{p}(f_i), \quad i&#x3D;1,2,…,n $$<br>$$ sum_y P(y|x) &#x3D; 1 $$<br>按照求解最优化问题的习惯，将求<strong>最大值问题改写为等价的求最小值问题</strong>：<br>$$ min_{P \in \varsigma} -H(P) &#x3D; sum_{x,y} \widetilde{P}(x) P(y|x) logP(y|x) $$<br>$$ s.t. \quad E_p(f_i) - E_\widetilde{p}(f_i) &#x3D; 0, \quad i&#x3D;1,2,…,n $$<br>$$ sum_y P(y|x) &#x3D; 1 $$</p><p>至此可以将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题来求解原始问题。</p><ol><li><p>引入拉格朗日乘子 $w_0, w_1, …, w_n$。<br>定义拉格朗日函数 $L(P, w)$。<br>$$\begin{equation}\begin{split}<br>L(P, w) &amp;&#x3D; -H(P) + w_0(1-sum_yP(y|x)) + sum_{i&#x3D;1}^n w_i (E_\widetilde{p}(f_i) - E_p(f_i)) \\<br>&amp;&#x3D; sum_{x,y} \widetilde{P}(x) P(y|x) logP(y|x) + w_0(1-sum_yP(y|x)) \\<br>&amp;+ sum_{i&#x3D;1}^n w_i (sum_{x,y} \widetilde{P}(x,y) f_i(x,y) - sum_{x,y} \widetilde{P}(x) P(y|x) f_i(x,y)) \\<br>\end{split}\nonumber\end{equation}$$</p><p> 上述可得需要优化的原始问题是:<br> $$ min_{P \in \varsigma} max_w L(P, w) $$<br> 那么对偶问题是：<br> $$ max_w min_{P \in \varsigma} L(P, w) $$</p><p> 由于拉格朗日函数 $L(P, w)$ 是 <code>P</code> 的凸函数，因此原始问题的解等价于对偶问题的解。</p></li><li><p>求解对偶问题内部的极小化。<br>$min_{P \in \varsigma} L(P, w)$ 是 <code>w</code> 的函数，将其记作：<br>$$ \Psi(w) &#x3D; min_{P \in \varsigma} L(P, w) &#x3D; L(P_w, w) $$</p><p> $\Psi(w)$ 是对偶函数，同时将其记作：<br> $$ P_w &#x3D; arg min_{P \in \varsigma} L(P, w) &#x3D; P_w(y|x) $$</p><p> 之后求 $L(P, w)$ 对 $P(y|x)$ 的偏导数。<br> $$\begin{equation}\begin{split}<br> {\frac {\partial{L(P, w)}} {\partial{P(y|x)}}} &amp;&#x3D; sum_{x,y} \widetilde{P}(x)(logP(y|x)+1) - sum_y w_0 - sum_{x,y}(\widetilde{P}(x) sum_{i&#x3D;1}^n w_i f_i(x,y)) \\<br> &amp;&#x3D; sum_{x,y} \widetilde{P}(x)(logP(y|x)+1-w_0-sum_{i&#x3D;1}^n w_i f_i(x,y))<br> \end{split}\nonumber\end{equation}$$</p><p> 在令偏导数等于 <code>0</code>，$\widetilde{P}(x)&gt;0$ 的情况下，解得：<br> $$\begin{equation}\begin{split}<br> P(y|x) &amp;&#x3D; exp(sum_{i&#x3D;1}^n w_i f_i(x,y) + w_0 - 1) \\<br> &amp;&#x3D; {\frac {exp(sum_{i&#x3D;1}^n w_i f_i(x,y))} {exp(1-w_0)}}<br> \end{split}\nonumber\end{equation}$$</p><p> 由于 $sum_yP(y|x)&#x3D;1$，可得：<br> $$ P_w(y|x) &#x3D; {\frac {1} {Z_w(x)}} exp(sum_{i&#x3D;1}^n w_i f_i(x,y)) $$<br> 而这其中 $Z_w(x) &#x3D; sum_y exp(sum_{i&#x3D;1}^n w_i f_i(x,y))$</p><p> <small>$Z_w(x)$ 被称为规范化因子，$f_i(x,y)$ 是特征函数，$w_i$ 是特征的权值，模型 $P_w&#x3D;P_w(y|x)$ 就是最大熵模型，而 <code>w</code> 是最大熵模型的参数向量。</small></p></li><li><p>求解对偶问题外部的极大化。<br>$max_w \Psi(w)$ 将其解记作 $w^*$，即：<br>$$ w^* &#x3D; arg max_w \Psi(w) $$</p><p> 应用最优化算法求对偶问题 $\Psi(w)$ 的极大化得到 $w^*$，表示 $P^* \in \varsigma$。此时这里的 $p^*&#x3D;P_{w^*}&#x3D;P_{w^*}(y|x)$ 也就是学习到的最优模型（最大熵模型）。即可得到最大熵模型的学习就是对偶函数 $\Psi(w)$ 的极大化。</p></li></ol><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于文章中不理解的特有名词，还是需要自己去了解查询。<br>如果你在阅读途中遇到了看不懂、不了解等因素，可以先放一放，阅读完毕后你就会明白之前是什么含义。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="https://ssjcoding.github.io/2018/11/20/gradient-descent/">梯度下降法详解</a><br><a href="https://www.jasper.wang/logistic_r">Logistic回归</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习《统计学习方法》所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;逻辑回归（&lt;code&gt;logistic regression&lt;/code&gt;）&lt;/strong&gt;是统计学习中的经典分类方法，虽然被称为回归，但其实是个分类模型。其本质是假设某个数据服从逻辑分布，就可以使用极大似然法估计出其线性回归的参数，之后再使用 &lt;code&gt;Sigmoid&lt;/code&gt; 逻辑函数对其分类。&lt;br&gt;&lt;small&gt;面试的时候千万不要说你很了解 &lt;code&gt;LR&lt;/code&gt;，因为细节真的太多了😂&lt;/small&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML-支持向量机</title>
    <link href="https://blog.vgbhfive.cn/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://blog.vgbhfive.cn/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2023-08-27T12:19:19.000Z</published>
    <updated>2023-09-09T07:40:55.868Z</updated>
    
    <content type="html"><![CDATA[<h3 id="小故事"><a href="#小故事" class="headerlink" title="小故事"></a>小故事</h3><p>每个人小时候最讨厌的事情就是吃药了，但不幸的是有一天你得了感冒，妈妈给你买了药，你拿到药后打开了包装纸。在药包里有两种药片，一种是白色的另一种是黑色的，白色的看起来比较甜，而黑色的一看就很苦；因此你决定先吃白色的药片，那么如何一把抓住所有的药片呢？你可以找一个勺子这么把药划分出来<br><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_1.jpeg" alt="svm_1.jpg"></p><p>那么如果药包里的药片是这样排布的呢？<br><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_2.jpeg" alt="svm_2.jpg"></p><p>此时你心里想着终于可以祭出我的绝世神功了！哼哈…充满内力的手一拍桌子，药片就飞到了半空中，此时无影手技能发动，你就用一张纸接住了黑色的药片，哈哈哈哈哈哈…<br><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_3.jpeg" alt="svm_3.jpg"></p><span id="more"></span><p>而今天你在学会了 <code>SVM</code> 之后，会将药片成为<strong>数据</strong>，勺子被称为<strong>分类面</strong>，找到间隔药片距离最远位置的过程称为<strong>优化</strong>，而让药片飞到空中的神功称为<strong>核映射</strong>，在空中分隔药片的纸张称为<strong>分类超平面</strong>。</p><hr><h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><p><strong>支持向量机（<code>support vertor machines, SVM</code>）</strong>是一种<strong>二类分类模型</strong>，其基本模型是定义在<strong>特征空间上的间隔最大的线性分类器</strong>，其中间隔最大使其有别于<strong>感知机</strong>；另外支持向量机还包含<strong>核技巧</strong>，这使其成为本质上的非线形分类器。<br>支持向量机的学习策略是使得<strong>间隔最大化</strong>，那么可形成一个<strong>求解凸二次规划（<code>convex quadrtic programming</code>）</strong>的问题，也就是等价于<strong>正则化的合页损失函数的最小化问题</strong>，即支持向量机的学习算法就是<strong>求解凸二次规划的最优算法</strong>。</p><p>支持向量机学习方法包含以下模型：</p><ul><li><strong>线性可分支持向量机（<code>linear support vector machine in linearly separable case</code>）</strong><br> 当训练数据线性可分时，通过<strong>硬间隔最大化（<code>hard margin maximization</code>）</strong>，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机。</li><li><strong>线性支持向量机（<code>linear support vector machine</code>）</strong><br> 当训练数据接近线性可分时，通过<strong>软间隔最大化（<code>soft margin maximization</code>）</strong>，也学习一个线性分类器，即线性支持向量机，又称软间隔支持向量机。</li><li><strong>非线性支持向量机（<code>non-linear support vector machine</code>）</strong><br> 当训练数据不可分时，通过使用<strong>核技巧（<code>kernal trick</code>）</strong>及<strong>软间隔最大化</strong>，学习非线性支持向量机。</li></ul><p>当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，<strong>核函数（<code>kernal function</code>）</strong>表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习向量支持机，这样的方法被称为<strong>核技巧</strong>。</p><p><strong>核方法（<code>kernal method</code>）</strong>是比支持向量机更为一般的机器学习方法。</p><hr><h3 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h3><p>假设给定一个特征空间上的训练数据集<br>$$ T &#x3D; {(x_1, y_1), (x_2, y_2), …, (x_N, y_N)} $$<br><small>其中 $x_i \in X &#x3D; R^n， y_i \in Y &#x3D; \{+1, -1\}， i &#x3D; 1, 2, …, N， x_i $ 第 $ i $ 个特征向量，也称为实例； $ y_i $ 为 $ x_i $ 的类标记，当 $ y_i &#x3D; +1 $ 时，称 $ x_i $ 为正例；当 $ y_i &#x3D; -1 $ 时，称 $ x_i $ 为负例。$ (x_i, y_i) $ 称为样本点。 </small></p><p>目前的学习目标是在特征空间中找到一个<strong>分离超平面</strong>，能将实例分到不同的类。分离超平面对应方程 <code>w·x + b = 0</code>，其由法向量 <code>w</code> 和截距 <code>b</code> 决定，可用 <code>(w, b)</code> 来表示；分离超平面将特征空间划分为两部分，一部分为正类，一部分为负类；法向量指向的一侧为正类，另一侧为负类。</p><p><small>一般当训练数据线性可分时，存在无数个分离超平面可将两类数据正确分开，而感知机使用误分类最小的策略求得分离超平面，不过此时存在无数个解；而线性可分支持向量机利用<strong>间隔最大化</strong>求得最优分离超平面，即此时存在唯一解。</small></p><p>给定线性可分训练数据集，通过<strong>间隔最大化</strong>或<strong>等价求解相应的凸二次规划问题</strong>学习得到的分类超平面为<br>$$ w^* · x + b^* &#x3D; 0 $$<br>以及相应的<strong>分类决策函数</strong><br>$$ f(x) &#x3D; sign(w^* · x + b^*) $$<br>称为线性可分支持向量机。</p><h4 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h4><p><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_5.jpg" alt="svm_5.jpg"></p><p>上述图中有 <code>A</code>、 <code>B</code>、 <code>C</code> 三点表示三个实例，其均在分离超平面的正类一侧。点 <code>A</code> 距分离超平面较远，若预测该点为正类则预测正确的概率最大；点 <code>C</code> 距分离超平面较近，则预测该点为正类的概率没有那么大；而点 <code>B</code> 位于 <code>A</code> 和 <code>C</code> 之间，预测其为正类的概率在 <code>A</code> 与 <code>C</code> 之间。</p><p>一般地点距离分离超平面的远近可以表示分类预测的正确概率。在超平面 $ w·x + b &#x3D; 0 $ 确定的情况下，$ |w·x + b| $ 能够相对地表示点 <code>x</code> 距离超平面的远近，而 $ w·x + b &#x3D; 0 $ 的符号与标记 <code>y</code> 的符号是否一致就能够表示分类是否正确，所以可用量 $ y(w·x + b) $ 来表示分类的正确性及确信度，这就是<strong>函数间隔（<code>functional margin</code>）</strong>的概念。</p><h5 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h5><p>对于给定的训练数据集 $T$ 和超平面 $(w, b)$，其定义超平面 $(w, b)$ 关于样本点 $(x_i, y_i)$ 的函数间隔为<br>$$ \tilde{Y_i} &#x3D; y_i (w · x_i + b) $$</p><p>函数间隔的最小值为<br>$$ \tilde{Y} &#x3D; min_{i&#x3D;1, 2, …, N} \tilde{y_i} $$</p><p>函数间隔可以表示分类预测的正确性和准确度。但是在选择分离超平面时，只有函数间隔是不够的，因为只要成比例地改变 <code>w</code> 和 <code>b</code>，虽然分离超平面不会改变，但函数间隔却会成比例改变，因此需要对分离超平面的法向量 <code>w</code> 加以约束，例如<strong>规范化 $||w|| &#x3D; 1$</strong> ，从而使得间隔是确定的，此时函数间隔成为<strong>几何间隔（<code>geometric margin</code>）</strong>。</p><p><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_6.jpg" alt="svm_6.jpg"><br>点 <code>A</code> 表示某一个实例 $x_i$，其类标记为 $y_i &#x3D; +1$。点 <code>A</code> 与超平面 $(w, b)$ 的距离由线段 <code>AB</code> 给出，记作<br>$$ Y_i &#x3D; y_i ({\frac {w} {||w||}} · x_i + {\frac {b} {||w||}}) $$<br><small>其中 $||w||$ 为 <code>w</code> 的 <strong>$L_2$ 范数</strong>。 </small></p><h5 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h5><p>对于给定的训练数据集 $T$ 和超平面 $(w, b)$，其定义超平面 $(w, b)$ 关于样本点 $(x_i, y_i)$ 的<strong>几何间隔</strong>为<br>$$ Y_i &#x3D; y_i ({\frac {w} {||w||}} · x_i + {\frac {b} {||w||}}) $$</p><p>几何间隔的最小值<br>$$ Y &#x3D; min_{i&#x3D;1, 2, …, N} y_i $$</p><p>超平面 <code>(w, b)</code> 关于样本点 $(x_i, y_i)$ 的几何间隔一般是实例点到超平面的带符号的距离（<code>signed distance</code>），当样本点被正确分类时就是实例点到超平面的距离。</p><p>从函数间隔和几何间隔的定义可知 $Y_i &#x3D; {\frac {\tilde{y_i}} {||w||}}$ 和 $Y &#x3D; {\frac {\tilde{y}} {||w|}}$，如果 $||w|| &#x3D; 1$ 那么函数间隔和几何间隔相等。<br><br><small>如果超平面参数 <code>w</code> 和 <code>b</code> 成比例地改变，那函数间隔也会按此比例改变，而几何间隔不变。</small></p><h4 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h4><p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且集合间隔最大的分离超平面。而对于线性可分的训练数据集而言，线性可分的分离超平面有无数个（等价于感知机），但几何间隔最大的分离超平面却是唯一的（<strong>硬间隔最大化</strong>）。</p><p>间隔最大化的直接解释就是：对于训练数据集找到几何间隔最大的超平面意味着以概率最大的确信度队训练数据进行分类。</p><ol><li><p>最大间隔分离超平面<br>最大间隔分离超平面即<strong>几何间隔最大的分离超平面</strong>，可以表示为下面的约束最有问题：<br>$$ max_{w, b} \quad {\frac {\tilde{Y}} {||w||}} $$<br>$$ s.t. \quad y_i(w·x_i + b) &gt;&#x3D; \tilde{Y} \quad i&#x3D;1, 2, …, N$$<br>几何间隔 $\tilde{Y}$ 的取值并不影响最优化问题的解，即成比例改变 $w, b$，那对应的 $\tilde{Y}$ 也会成比例改变。也就是说会产生一个等价问题，最大化 ${\frac {1} {||w||}}$ 和最小化 ${\frac {1} {2}} {||w||}^2$ 是等价的，由此可得到线性可分支持向量机学习的最优化问题，也是一个<strong>凸二次规划问题（<code>convex quadratic progranmming</code>）</strong>。<br>$$ min_{w, b} \quad {\frac {1} {2}} {||w||}^2 $$<br>$$ s.t. \quad y_i(w·x_i + b) - 1 &gt;&#x3D; 0 \quad i&#x3D;1, 2, …, N $$<br><br><strong>凸优化问题</strong>是指约束最优化问题：<br>$$ min_{w, b} \quad f(w) $$<br>$$ \begin{equation}\begin{split}<br>s.t. \quad g_f(w) &lt;&#x3D; 0, \quad i&#x3D;1, 2, …, k  \\<br>h_i(w) &#x3D; 0, \quad i&#x3D;1, 2, …, l<br>\end{split}\nonumber\end{equation}$$<br><small>目标函数 $f(w)$ 和约束函数 $g_i(w)$ 都是 $R^n$ 上的<strong>连续可微的凸函数</strong>，约束函数 $h_i(w)$ 是 $R^n$ 上的<strong>仿射函数</strong>。当目标函数 $f(w)$ 是二次函数且约束函数 $g_i(w)$ 是仿射函数时，上述凸优化问题就会成为凸二次规划问题。</small><br>获得约束最优化问题的解 $w^*, b^*$，可得到<strong>最大间隔分离超平面 $w^*·x + b^* &#x3D; 0$</strong> 及<strong>分类决策函数 $f(x)&#x3D;sign(w^*·x + b^*)$</strong> ，即线性可分支持向量机模型。</p><p><strong>线性可分支持向量机学习算法-最大间隔法</strong>：</p><blockquote><p>输入：线性可分训练数据集 $T &#x3D; {(x_1, y_1), …, (x_N, y_N)}$，其中 $x_i \in X &#x3D; R^n，y_i \in Y &#x3D; \{+1, -1\}，i &#x3D; 1, …, N$<br>输出：最大间隔分离超平面和分类决策函数<br>过程：</p><ol><li>构建并求解约束最优化问题<br> $$ min_{w, b}  \quad {\frac {1} {2}} {||w||}^2 $$<br> $$ s.t.  \quad y_i(w·x_i + b) - 1 &gt;&#x3D; 0, i&#x3D;1, 2, …, N $$<br> 获取最优解 $ w^*, b^*$</li><li>由此而得分离超平面和分类决策函数<br> $$ w^*·x + b^* &#x3D; 0 $$<br> $$ f(x) &#x3D; sign(w^*·x + b^*) $$</li></ol></blockquote><br></li><li><p>支持向量和间隔边界<br>在线性可分情况下训练数据集的样本点中与分离超平面距离最近的样本点的实例称为<strong>支持向量（<code>support vector</code>）</strong>，即支持向量是使约束条件式（$y_i(w·x_i + b) - 1 &gt;&#x3D; 0$）等号成立的点。<br><br>对于 $y_i&#x3D;1$ 的点，支持向量在超平面 $H_1:w·x+b&#x3D;1$ 上，而对于 $y_i&#x3D;-1$ 的点，支持向量在超平面 $H_2:w·x+b&#x3D;-1$ 上。<br><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_4.jpg" alt="svm_4.jpg"></p><p>注意到 <code>H1</code> 与 <code>H2</code> 之间平行，但却没有实例点在其中间，即 <code>H1</code> 与 <code>H2</code> 中间的长带被称为<strong>间隔（<code>margin</code>）</strong>，间隔依赖于分离超平面的法向量 <code>w</code>，等于 ${\frac {2} {||w||}}$，而 <code>H1</code> 和 <code>H2</code> 也被称为<strong>间隔边界</strong>。</p></li></ol><h4 id="对偶算法"><a href="#对偶算法" class="headerlink" title="对偶算法"></a>对偶算法</h4><p>为了解决线性可分支持向量机的最优化问题，可将其作为原始问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的<strong>对偶算法</strong>。其优点如下：</p><ul><li>转化为对偶问题更容易求解。</li><li>自然引入核函数，进而推广到非线性分类问题。</li></ul><p><small>后续的拉格朗日函数内容实在是看不懂，先允许我 <code>TODO</code> 下😁</small></p><hr><h3 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h3><p>对于线性不可分训练数据集再使用硬间隔最大化来分类是不适用的，需要使用<strong>软间隔最大化</strong>。<br>通常情况下，训练数据集中存在一些<strong>特异点（<code>outlier</code>）</strong>，将这些特异点去除后剩下的大部分样本点是可以满足线性可分。而线性不可分的样本点 $(x_i, y_i)$ 也就意味着不能满足间隔大于等于 <code>1</code> 的约束条件，为了解决这个问题，对每个样本点引入一个<strong>松弛变量 $ \xi_i &gt;&#x3D;0$</strong> ，使得函数间隔加上松弛变量大于等于 <code>1</code>，即约束条件和目标函数如下：<br>$$ y_i(w·x_i + b) &gt;&#x3D; 1 - \xi_i $$<br>$$ {\frac {1} {2}} {||w||}^2 + C sum_{i&#x3D;1}^N \xi_i $$<br><small>$C&gt;0$ 被称为<strong>惩罚参数</strong>，一般由应用问题决定，$C$ 值大时对误分类的惩罚增大，$C$ 值小时对误分类的惩罚减小。</small></p><p>线性不可分的线性支持向量机的学习问题变成了如下的凸二次规划问题：<br>$$ min_{w, b, \xi} \quad {\frac {1} {2}} {||w||}^2 + C sum_{i&#x3D;1}^N \xi_i $$<br>$$ \begin{equation}\begin{split}<br>s.t. \quad y_i(w·x_i + b) &gt;&#x3D; 1 - \xi_i, \quad i&#x3D;1, 2, …, N \\<br>\xi_i &gt;&#x3D; 0, \quad i&#x3D;1, 2, …, N<br>\end{split}\nonumber\end{equation}$$</p><p>上述求解后得到 $w^*, b^*$，就可以达到<strong>分离超平面 $w^*·x + b^* &#x3D; 0$</strong> 及<strong>分类决策函数 $f(x)&#x3D;sign(w^*·x + b^*)$</strong> ，该模型为训练线性不可分时的线性支持向量机，简称为线性支持向量机，其对于现实数据具有更广的适用性。</p><p><strong>线性支持向量机学习算法</strong>：</p><blockquote><p>输入：训练数据集 $T&#x3D;{(x_1, y_1), …, (x_N, y_N)}$，其中 $x_i \in X &#x3D; R^n，y_i \in Y &#x3D; \{+1, -1\}，i &#x3D; 1, …, N$ <br><br>输出：分离超平面和分类决策函数<br>过程：</p><ol><li>选择惩罚参数 $C&gt;0$，构造并求解凸二次规划问题<br> $$ min_{a} \quad {\frac {1} {2}} sum_{i&#x3D;1}^N sum_{j&#x3D;1}^N a_i a_j y_i y_j (x_i · x_j) - sum_{i&#x3D;1}^N a_i $$<br> $$ \begin{equation}\begin{split}<br>s.t. \quad sum_{i&#x3D;1}^N a_i y_i &#x3D; 0 \\<br>0 &lt;&#x3D; a_i &lt;&#x3D; C<br>\end{split}\nonumber\end{equation}$$<br> 求得最优解 $a^* &#x3D; (a_1^*, a_2^*, …, a_N^*)^T $</li><li>计算 $ w^* &#x3D; sum_{i&#x3D;1}^N a_i^* y_i x_i $<br> 选择 $a^*$ 的一个分量 $a_j^*$ 适合条件 $0 &lt;&#x3D; a_j^* &lt;&#x3D; C$，计算<br> $$ b^* &#x3D; y_j - sum_{i&#x3D;1}^N y_i a_i^*(x_i · x_j) $$<br> <small>$a_j^*$ 在实际中取所有符合条件的样本点上的平均值。</small></li><li>获得分离超平面及分类决策函数<br> $$ w^*·x + b^* &#x3D; 0 $$<br> $$ f(x)&#x3D;sign(w^*·x + b^*) $$</li></ol></blockquote><h4 id="对偶算法-1"><a href="#对偶算法-1" class="headerlink" title="对偶算法"></a>对偶算法</h4><p><small>再允许我 <code>TODO</code> 一下😭</small></p><h4 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h4><p>在线性不可分的情况下，将对偶问题的解 $a^* &#x3D; (a_1^*, a_2^*, …, a_N^*)^T$ 中对应的 $a_i^*&gt;0$ 的样本点 $(x_i, y_i)$ 的实例 $x_i$ 称为支持向量（软间隔的支持向量）。</p><p><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_8.jpg" alt="svm_8.jpg"></p><p>软间隔的支持向量 $x_i$ 在间隔边界上、在间隔边界与分离超平面之间或者在分离超平面误分一侧：</p><ul><li>若 $a_i^*&lt;C$，则 $\xi_i&#x3D;0$，支持向量 $x_i$ 恰好落在间隔边界上。</li><li>若 $a_i^*&#x3D;C, 0&lt;\xi_i&lt;1$，则分类正确，支持向量 $x_i$ 在间隔边界与分离超平面之间。</li><li>若 $a_i^*&#x3D;C, \xi_i&#x3D;1$，则支持向量 $x_i$ 恰好落在分离超平面上。</li><li>若 $a_i^*&#x3D;C, \xi_i&gt;1$，则支持向量 $x_i$ 在分离超平面误分一侧。</li></ul><hr><h3 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h3><p>对于线性分类问题，有时分类问题是非线性的，此时可以使用非线性支持向量机，其主要适用<strong>核技巧（<code>kernal trick</code>）</strong>。</p><h4 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h4><h5 id="释义"><a href="#释义" class="headerlink" title="释义"></a>释义</h5><p>非线性分类问题是指通过利用非线性模型才能进行分类的问题。一般来说对于给定的训练数据集 $T&#x3D;{(x_1, y_1), …, (x_N, y_N)}$，其中实例 $x_i$ 属于输入空间 $x_i \in X &#x3D; R^n$，对应的标记有两类 $y_i \in Y &#x3D; {-1, +1}, i&#x3D;1,2,…,N$，如果能用 $R^n$ 中的一个超平面将正负例正确分开，则称这个问题为非线性可分问题。<br><img src="/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_7.jpg" alt="svm_7.jpg"></p><p>非线性问题往往不好求解，因此希望可以使用线性分类问题的方法来解决问题。而采取的方法就是将非线性问题转换为线性问题，通过转换后的线性问题的求解方法来解决原来的非线性问题。 </p><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>核技巧的思路是：在学习与预测中只定义<strong>核函数 $K(x,z)$</strong> ，而不显示地定义<strong>映射函数 $\phi$</strong> ，通常直接计算核函数是比较简单的，而通过映射函数 $\phi(x)$ 和 $\phi(z)$ 计算 $K(x,z)$ 并不简单。并且特征空间 $H$ 一般是高维的，而对于给定的核函数 $K(x,z)$，特征空间 $H$ 和映射函数 $\phi$ 并不唯一，即可以取不同的特征空间，相应的映射函数也可以不同。<br><br><small>$\phi$ 是输入空间 $R^n$ 到特征空间 $H$ 的映射。</small></p><p>设定 $X$ 是输入空间，又设定 $H$ 为特征空间，如果存在一个从 $X$ 到 $H$ 的映射<br>$$ \phi(x): X -&gt; H $$</p><p>使得对所有的 $x,z \in X$，函数 $K(x, z)$ 满足条件<br>$$ K(x, z) &#x3D; \phi(x)·\phi(z) $$</p><p>则称 $K(x, z)$ 为核函数，$\phi(x)$ 为映射函数，式中 $\phi(x)·\phi(z)$ 为 $\phi(x)$ 和 $\phi(z)$ 的内积。</p><h5 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h5><p>在线性支持向量机的对偶问题中，无论是目标函数还是分类决策函数（分离超平面）都仅涉及输入实例与实例之间的<strong>内积</strong>。<br>因此对偶问题的目标函数中的内积 $x_i·x_j$ 可以用核函数 $K(x_i, x_j) &#x3D; \phi(x_i)·\phi(x_j)$ 来代替，此时对偶问题的目标函数称为：<br>$$ W(a) &#x3D; {\frac {1} {2}} sum_{i&#x3D;1}^N sum_{j&#x3D;1}^N a_i a_j y_i y_j K(x_i, x_j) - sum_{i&#x3D;1}^N a_i $$</p><p>类似的，分类决策函数中的内积也可以使用核函数代替，分类决策函数如下：<br>$$ f(x) &#x3D; sign(sum_{i&#x3D;1}^N a_i^* y_i \phi(x_i) · \phi(x) + b^*) &#x3D; sign(sum_{i&#x3D;1}^N a_i^* y_i K(x_i, x) + b^*) $$</p><p>上述就等价于通过映射函数 $\phi$ 将原来的输入空间变换成一个新的特征空间，将输入空间的内积 $x_i·x_j$ 变换为特征空间中的内积 $\phi(x_i)·\phi(x_j)$，在新的特征空间中学习线性支持向量机。<br><br><small>映射函数为非线性函数时，即学习到含有核函数的支持向量机就是非线性支持向量机。</small></p><h4 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h4><ol><li><p><strong>多项式核函数（<code>polynomial kernal function</code>）</strong><br> $$ K(x, z) &#x3D; (x·z + 1)^p $$<br> 对应的支持向量机是一个 <code>p</code> 次多项式分类器，分类决策函数如下：<br> $$ f(x) &#x3D; sign(sum_{i&#x3D;1}^N a_i^* y_i (x_i·x + 1)^p + b^*) $$</p></li><li><p><strong>高斯核函数（<code>Gaussian kernal function</code>）</strong><br> $$ K(x, z) &#x3D; exp(-{\frac {(||x-z||)^2} {2a^2}}) $$<br> 对应的支持向量机是高斯径向基函数（<code>radial basis function</code>）分类器，分类决策函数如下：<br> $$ f(x) &#x3D; sign(sum_{i&#x3D;1}^N a_i^* y_i exp(-{\frac {(||x-z||)^2} {2a^2}}) + b^*) $$</p></li><li><p><strong>字符串核函数（<code>string kernal function</code>）</strong><br> 核函数不仅可以定义在欧氏空间上，还可以定义在离散数据的集合上。字符串核就是定义在字符串集合上的核函数，其在文本分类、信息检索等方面均有应用。</p></li></ol><p>核技巧的具体思路是，在学习与预测中只定义核函数 $K(x, z)$，而不显式定义映射函数，</p><h4 id="非线性支持向量机-1"><a href="#非线性支持向量机-1" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h4><p>利用核技巧可以将线性分类问题的学习方法应用于非线性分类问题中，仅需要将线性支持向量机<strong>对偶形式中的内积替换成核函数</strong>即可。<br>从非线性分类训练集，通过核函数和软间隔最大化或凸二次规划，学习得到的分类决策函数称为非线性支持向量，$K(x, x_i)$ 是正定核函数。<br>$$ f(x) &#x3D; sign(sum_{i&#x3D;1}^N a^* y_i K(x, x_i) + b^*) $$</p><p><strong>非线性支持向量机学习算法</strong>：</p><blockquote><p>输入：训练数据集 $T&#x3D;{(x_1, y_1), …, (x_N, y_N)}$，其中 $x_i \in X &#x3D; R^n, y_i \in Y &#x3D; \{-1, +1\}, i&#x3D;1,2,…,N$ <br><br>输出：分类决策函数<br>过程：</p><ol><li>选取适当的核函数 $K(x, z)$ 和适当的参数 <code>C</code>，构造并求解最优化问题<br> $$ min_a \quad {\frac {1} {2}} sum_{i&#x3D;1}^N sum_{j&#x3D;1}^N a_i a_j y_i y_j K(x_i, y_i) - sum_{i&#x3D;1}^N a_i $$<br> $$ s.t. \quad sum_{i&#x3D;1}^N a_i y_i &#x3D; 0, \quad 0 &lt;&#x3D; a_i &lt;&#x3D; C, \quad i&#x3D;1,2,…,N $$<br> 求得最优解 $a^*&#x3D;(a_1^*, a_2^*, …, a_N^*)$</li><li>选择 $a^*$ 的一个正分量 $0 &lt; a_j^* &lt; C$，计算<br> $$ b^* &#x3D; y_j - sum_{i&#x3D;1}^N a_i^* y_i K(x_i, y_j) $$</li><li>构造决策函数<br> $$ f(x) &#x3D; sign(sum_{i&#x3D;1}^N a^* y_i K(x, x_i) + b^*) $$<br> 当 $K(x, z)$ 是正定核函数时，最优化问题为凸二次规划问题，其解是存在的。</li></ol></blockquote><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>学习原理知识从来都是非常枯燥乏味的，特别是又长又复杂的数学公式，不过建议你可以去看看具体问题的解决方案，那肯定能给你不少的<strong>成就感</strong>。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习《统计学习方法》所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;小故事&quot;&gt;&lt;a href=&quot;#小故事&quot; class=&quot;headerlink&quot; title=&quot;小故事&quot;&gt;&lt;/a&gt;小故事&lt;/h3&gt;&lt;p&gt;每个人小时候最讨厌的事情就是吃药了，但不幸的是有一天你得了感冒，妈妈给你买了药，你拿到药后打开了包装纸。在药包里有两种药片，一种是白色的另一种是黑色的，白色的看起来比较甜，而黑色的一看就很苦；因此你决定先吃白色的药片，那么如何一把抓住所有的药片呢？你可以找一个勺子这么把药划分出来&lt;br&gt;&lt;img src=&quot;/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_1.jpeg&quot; alt=&quot;svm_1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;那么如果药包里的药片是这样排布的呢？&lt;br&gt;&lt;img src=&quot;/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_2.jpeg&quot; alt=&quot;svm_2.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时你心里想着终于可以祭出我的绝世神功了！哼哈…充满内力的手一拍桌子，药片就飞到了半空中，此时无影手技能发动，你就用一张纸接住了黑色的药片，哈哈哈哈哈哈…&lt;br&gt;&lt;img src=&quot;/ML-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/svm_3.jpeg&quot; alt=&quot;svm_3.jpg&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="TODO" scheme="https://blog.vgbhfive.cn/tags/TODO/"/>
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML-决策树</title>
    <link href="https://blog.vgbhfive.cn/ML-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://blog.vgbhfive.cn/ML-%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2023-08-02T13:48:52.000Z</published>
    <updated>2023-08-13T13:05:17.498Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><p>决策树（<code>decision tree</code>）是一种基本的<strong>分类和回归方法</strong>。其主要呈现为<strong>树状结构</strong>，在分类问题中，表示基于特征对实例进行分类的过程，可以被认为是 <strong><code>if-then</code> 的规则集合</strong>，也可以被认为是定义在<strong>特征空间与类空间上的条件概率分布</strong>。</p><p>其优点主要有分类速度快、模型具有可读性，在学习时利用训练数据根据<strong>损失函数最小化</strong>的原则建立决策树模型；而在预测时对新的数据利用<strong>决策树模型</strong>进行分类。</p><p>决策树模型主要包含以下步骤：</p><ul><li><strong>特征选择</strong></li><li><strong>决策树的生成</strong></li><li><strong>决策树的修剪</strong></li></ul><span id="more"></span><hr><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h4><p>分类决策树模型是一种描述对实例进行分类的树状结构。决策树有<strong>结点（<code>node</code>）</strong>和<strong>有向边（<code>directed edge</code>）</strong>组成，结点有两种类型：<strong>内部结点（<code>internal node</code>）</strong>和<strong>叶结点（<code>leaf node</code>）</strong>，其中内部结点表示一个特征或属性，叶结点表示一个类。</p><p>用决策树分类，首先从根结点开始对实例的某一个特征进行测试，根据测试结果将实例分配到其子结点上，每个子结点对应着一个特征的取值。如此递归地对实例进行测试和分配，直至到达叶结点，最后将实例分配到叶结点的分类中。</p><p><img src="/ML-%E5%86%B3%E7%AD%96%E6%A0%91/decision_tree-1.jpeg" alt="decision_tree-1"></p><h4 id="分类过程"><a href="#分类过程" class="headerlink" title="分类过程"></a>分类过程</h4><ol><li><p><strong><code>if-then</code> 规则集合</strong><br>决策树可以看成是 <code>if-then</code> 规则的集合。将决策树转换为 <code>if-then</code> 规则的流程如下：由决策树的根结点到叶结点的每一条路径构建一个规则；路径上的内部结点的特征表示规则的条件，而叶结点的分类则对应的规则的结论。<br>决策树上的路径或其对应的 <code>if-then</code> 规则集合具有一个重要的性质：<strong>互斥且完备</strong>。也就是说每一个实例都被一条路径或对应的一条规则覆盖，而且仅被一条路径或一条规则所覆盖。<br><small>覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</small></p></li><li><p><strong>条件概率分布</strong><br>决策树还可以表示为给定特征条件下类的<strong>条件概率分布</strong>，该条件概率分布定义在特征空间的划分（<code>partition</code>）上。将特征空间划分为互不相交的单元或区域，并在每个单元或区域定义一个类的概率分布就可以构成一个条件概率分布。<br>决策树中的每一条路径对应花粉中的某个单元，决策树所表示的条件概率分布就是由各个单元在给定特征条件下类的条件概率分布组成。<br><img src="/ML-%E5%86%B3%E7%AD%96%E6%A0%91/decision_tree_3.jpg" alt="decision_tree-3"></p></li></ol><h4 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h4><p>决策树学习根据给定的训练数据集构建一个决策树模型，使之可以正确地进行分类，但其本质就是从训练数据集中归纳总结出一组分类规则，而最终所需要的是具有很好的泛化能力，既可以对训练数据有很好的拟合，也可以对未知数据有很好的预测。<br>那么如何对这一要求如何体现呢？决策树模型使用损失函数来表示这一目标，其通常是<strong>正则化的极大似然函数，策略是以损失函数为目标函数的最小化</strong>。</p><p>决策树学习的算法通常就是一个递归地选择特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类，这一过程对应特征空间的划分，也是决策树的构建。<br>在开始时构建根结点，将所有的训练数据都放在根结点，此时选择一个最优特征，根据这一特征将训练数据集划分为子集，使得各个子集在当前分类情况下是最好的分类；如果这些子集可以被基本正确分类，那就构建叶结点，并将子集分配到对应的叶结点中；如果这些子集不能被正确分类，则为这些子集重新选择新的最优特征，继续对其划分并构建相应的结点；如此递归下去直至训练数据子集都能被正确分类，或者没有合适的特征为止，至此每个子集都会被分配到对应的叶结点上，这就生成了一颗决策树。</p><p>以上方法生成的决策树可以很好的对训练数据集进行分类，但对于未知的数据集却未必有很好的分类能力，即可能发生<strong>过拟合现象</strong>。因此需要对生成的决策树进行<strong>剪枝</strong>，将决策树变得简单化，使其具有更好的<strong>泛化能力</strong>。</p><hr><h3 id="生成算法"><a href="#生成算法" class="headerlink" title="生成算法"></a>生成算法</h3><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a><code>ID3</code></h4><p><code>ID3</code> 算法的核心在于对决策树上的各个结点应用<strong>信息增益准则</strong>选择特征，递归地构建决策树。<br>具体方法是：</p><ul><li>从根结点（<code>root node</code>）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点。</li><li>再对子结点递归地调用以上方法，构建决策树。</li><li>直到所有特征的信息增益都很小或没有特征可以选择为止。</li><li>最终就会得到一个决策树，其本质是使用 <em>极大似然法</em> 进行概率模型的选择。</li></ul><p>示例：</p><blockquote><p>输入：训练数据集 <code>D</code>，特征集 <code>A</code>，阈值 $\omega$ <br><br>输出：决策树<br>过程：</p><ol><li>若 <code>D</code> 中所有实例属于同一类 $C_k$，则 <code>T</code> 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 <code>T</code>。</li><li>若 <code>A =</code> $\phi$，则 <code>T</code> 为单结点树，并将 <code>D</code> 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 <code>T</code>。</li><li>否则计算 <code>A</code> 中各个特征对 <code>D</code> 的信息增益，选择信息增益最大的特征 $A_g$。</li><li>如果 $A_g$ 小于阈值 $\omega$，则置 <code>T</code> 为单结点树，并将 <code>D</code> 中实例数最大的类 $C_k$ 作为该类的标记，返回 <code>T</code>。</li><li>否则对 $A_g$ 的每一个可能值 $a_i$，依据 $A_g$ &#x3D; $a_i$ 将 <code>D</code> 划分为若干个非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 <code>T</code>，返回 <code>T</code>。</li><li>对第 <code>i</code> 个子结点，以 $D_i$ 为训练集，以 <code>A - </code>$A_g$ 为特征集，递归地调用 <code>1, 2, 3, 4, 5</code> 得到子树 $T_i$，返回 $T_i$。</li></ol></blockquote><h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a><code>C4.5</code></h4><p><code>C4.5</code> 算法与 <code>ID3</code> 算法类似，<code>C4.5</code> 在生成过程中使用了<strong>信息增益比</strong>来选择特征。</p><h4 id="CART"><a href="#CART" class="headerlink" title="CART"></a><code>CART</code></h4><p><strong>分类与回归树（<code>classification and regression tree, CART</code>）模型</strong>是应用非常广泛的决策树学习方法，既可以用于分类，也可以用于回归，将用于分类和回归的树统称为决策树。</p><p><code>CART</code> 是在给定输入随机变量 <code>X</code> 条件下输出随机变量 <code>Y</code> 的条件概率分布的学习方法。解释其含义就是，<code>CART</code> 假设决策树是二叉树，内部结点特征取值为<strong>“是”</strong>或 <strong>“否”</strong>，即左分支是取值为“是”的分支，右分支是取值为“否”的分支，这样决策树就等价于递归地对每个特征进行二分，即将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p><p><code>CART</code> 算法由以下两步组成：</p><ul><li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽可能的大。</li><li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，此时<strong>使用损失函数最小化作为剪枝的标准</strong>。</li></ul><hr><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以大大提高决策树学习的效率。如果一个特征进行分类的结果和随机分类的结果没有太大的差别，则这个特征没有分类能力，实际中这样的特征对决策树学习的准确度影响不大。<br>特征选择的准则是：</p><ul><li><strong>信息增益</strong></li><li><strong>信息增益比</strong></li><li><strong>基尼系数</strong></li></ul><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>在信息论与概率统计中，<strong>熵（<code>entropy</code>）</strong>用来度量随机变量的不确定性。</p><p><strong>条件熵 <code>H(Y|X)</code></strong> 表示在已知随机变量 <code>X</code> 的条件下随机变量 <code>Y</code> 的不确定性，即定义为 <code>X</code> 给定条件下 <code>Y</code> 的条件概率分布的熵对 <code>X</code> 的数学期望。</p><p>$$ H(Y|X) &#x3D; \sum_{i&#x3D;1}^n p_i H (Y | X &#x3D; x_i) $$</p><p><small>其中 $p_i$ <code>= P(X = </code> $x_i$ <code>), i = 1, 2, ..., n</code></small></p><p>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别被称为<strong>经验熵（<code>empirical entropy</code>）</strong>和<strong>经验条件熵（<code>empirical conditional entropy</code>）</strong>。</p><p><strong>信息增益（<code>information gain</code>）</strong>表示得知特征 <code>X</code> 的信息而使得类 <code>Y</code> 的信息的不确定性减少的程度。<br>特征 <code>A</code> 对训练数据集 <code>D</code> 的信息增益 $g(D, A)$，定义为集合 <code>D</code> 的经验熵 $H(D)$ 与特征 <code>A</code> 给定条件下 <code>D</code> 的经验条件熵 $H(D|A)$ 之差，即</p><p>$$ g(D, A) &#x3D; H(D) - H(D|A) $$</p><p>给定训练数据集 <code>D</code> 与特征 <code>A</code>，<strong>经验熵 $H(D)$</strong> 表示数据集 <code>D</code> 进行分类的不确定性；而<strong>经验条件熵 $H(D|A)$</strong> 表示在特征 <code>A</code> 给定条件下对数据集 <code>D</code> 进行分类的不确定性，那么他们的差即信息增益，表示由于特征 <code>A</code> 而使得对数据集 <code>D</code> 的分类的不确定性减少的程度。由此可以得到信息增益依赖于特征，不同的特征具有不同的信息增益，而信息增益大的特征具有更强的分类能力。</p><p><small>一般地，熵 <code>H(Y)</code> 与条件熵 <code>H(Y|X)</code> 之差称为互信息（<code>mutual information</code>），决策树学习中的信息增益等价于训练数据集中类与特征的互信息，因此决策树学习应用信息增益准则选择特征。</small></p><p>在实际中根据信息增益准则选择特征的方式是：对训练数据集 <code>D</code>，计算其每个特征的信息增益，并比较其大小，择优选择信息增量最大的特征。</p><p>信息增益算法示例： </p><blockquote><p>输入：训练数据集 <code>D</code> 和特征 <code>A</code><br>输出：特征 <code>A</code> 对训练数据集 <code>D</code> 的信息增益 <code>g(D, A)</code><br>过程：</p><ol><li>计算数据集 <code>D</code> 的信息增益<br>$$ H(D) &#x3D; - \sum_{k&#x3D;1}^K {\frac {\vert {C_k}\vert} {\vert {D} \vert} } log_2 {\frac {\vert {C_k}\vert} {\vert {D} \vert} } $$</li><li>计算特征 <code>A</code> 对数据集 <code>D</code> 的经验条件熵<br>$$ H(D|A) &#x3D; \sum_{i&#x3D;1}^n {\frac {\vert {D_i} \vert} {\vert {D} \vert} H(D_i) } $$</li><li>计算信息增益<br>$$ g(D, A) &#x3D; H(D) - H(D|A) $$</li></ol></blockquote><h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>信息增益的大小是相对于训练数据而言的，并没有绝对意义，而在训练数据集的经验熵大的时候，信息增益也会偏大。反之信息增益值会偏小。因此使用信息增益比可以解决这一问题，也是特征选择的另一个准则。<br>特征 <code>A</code> 对训练数据集 <code>D</code> 的信息增益比 $g_r(D, A)$，定义为其信息增益 $g(D, A)$ 与训练数据集 <code>D</code> 的经验熵 $H(D)$ 之比：</p><p>$$ g_r(D, A) &#x3D; {\frac {g(D, A)} {H(D)} } $$</p><h4 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h4><p>决策树的生成是递归地构建二叉决策树的过程，对<strong>回归树使用平方误差最小化准则</strong>，对<strong>分类树使用基尼系数（<code>Gini index</code>）最小化准则</strong>，进行特征选择生成二叉树。</p><h5 id="生成回归树"><a href="#生成回归树" class="headerlink" title="生成回归树"></a>生成回归树</h5><p>假设 <code>X, Y</code> 分别表示输入和输出变量，且 <code>Y</code> 是连续变量，给定的数据集表示为：$D &#x3D; { (x_1, y_1), (x_2, y_2), …, (x_N, y_N)}$，那么回归树如何生成？</p><p>一个回归树对应着输入空间（特征空间）的一个划分以及在划分单元上的输出值。假设已将输入空间划分为 <code>M</code> 个单元 $R_1, R_2, …, R_M$ 并且在每个单元 $R_m$ 上有一个固定的输出值 $C_m$，于是回归树的模型可以表示为</p><p>$$ f(x) &#x3D; \sum_{m&#x3D;1}^M C_m I(x \in R_m) $$</p><p>当输入空间的划分确定时就可以使用平方误差 $\sum_{x_i \in R_m} (y_i - f(x_i))^2$ 表示回归树对训练数据集的预测误差，即平方误差最小时表示该单元上的最优输出值。</p><p>那输入空间（特征空间）的切分点如何划分呢？这里采用启发式的方法，选择第 <code>j</code> 个变量 $X_j$ 和对应的取值 $C_j$ 作为<strong>切分变量（<code>splitting variable</code>）</strong>和<strong>切分点（<code>splitting point</code>）</strong> ，定义两个区域<br>$$R_1(j, C_j) &#x3D; {X | X_j &lt;&#x3D; C_m}, R_2(j, C_j) &#x3D; {x | X_j &gt; C_m}$$<br>然后寻找<strong>最优变量 <code>j</code></strong> 和<strong>最优切分点 $C_m$</strong> 。即求解</p><p>$$ \min_{j,C_m}[\min_{C_1}\sum_{x_i \in R_1(j, C_m)}(y_i - C_1)^2 + \min_{C_2}\sum_{x_i \in R_1(j, C_m)}(y_i - C_2)^2] $$</p><p>而对于固定输入变量 <code>j</code> 就可以找到最优切分点 $C_m$</p><p>$$ \bar{c_1} &#x3D; ave(y_i | x_i \in R_1(j, C_m)) , \bar{c_2} &#x3D; ave(y_i | x_i \in R_2(j, C_m)) $$</p><p>接着遍历输入所有变量，找到最优的切分变量 <code>j</code>，构成一个 $(j, C_m)$。依次将输入空间划分为两个区域，接着对上述区域进行重复划分，直至满足条件为止。即可以生成被称为<strong>最小二乘回归树（<code>least squares regression tree</code>）</strong>。</p><p>最小二乘回归树生成算法示例：</p><blockquote><p>输入：训练数据集 <code>D</code><br>输出：回归树 $f(x)$ <br><br>过程：</p><ol><li>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域的输出值。</li><li>计算划分数据集的最优切分变量 <code>j</code> 和最优切分点 $C_m$，求解<br> $$ \min_{j,C_m}[\min_{C_1}\sum_{x_i \in R_1(j, C_m)}(y_i - C_1)^2 + \min_{C_2}\sum_{x_i \in R_1(j, C_m)}(y_i - C_2)^2] $$<br>遍历变量 <code>j</code>，对固定的切分变量 <code>j</code> 和切分点 $C_m$ 能得到最小值的对 $(j, C_m)$。</li><li>用选定的最小值的对 $(j, C_m)$ 划分区域并计算对应的输出值<br> $$ R_1(j, C_m) &#x3D; {x|x_j &lt;&#x3D; C_m}, R_2(j, C_m) &#x3D; {x|x_j &gt; C_m} $$<br> $$ \bar{C_m} &#x3D; {\frac {1} {N_m}} \sum_{x_i \in R_m(j, C_m)} y_i, x \in R_m, m &#x3D; 1, 2 $$</li><li>继续对两个子区域调用步骤 <code>2, 3</code>，直至满足条件。</li><li>至此就可以得到将输入空间划分为 <code>M</code> 个区域 $R_1, R_2, …, R_M$ 即可生成回归决策树。<br> $$ f(x) &#x3D; \sum_{m&#x3D;1}^M \bar{C_m} I(x \in R_m) $$</li></ol></blockquote><h5 id="生成分类树"><a href="#生成分类树" class="headerlink" title="生成分类树"></a>生成分类树</h5><p>分类树用<strong>基尼指数</strong>选择最优特征，同时决定该特征的最优二值切分点。<br>在分类问题中，假设有 <code>K</code> 个类，样本点属于第 <code>k</code> 类的概率为 <code>Pk</code>，则概率分布的基尼指数定义为：</p><p>$$ Gini(p) &#x3D; \sum_{k&#x3D;1}^K P_k (1-P_k) &#x3D; 1-\sum_{k&#x3D;1}^K p_k^2 $$</p><p>对于二分类问题，若样本点属于第一个类的概率为 <code>p</code>，则概率分布的基尼指数为：</p><p>$$ Gini(p) &#x3D; 2p(1-p) $$</p><p>对于给定的样本集合 <code>D</code>，其基尼指数为：</p><p>$$ Gini(D) &#x3D; 1 - (\sum_{k&#x3D;1}^K)({\frac {\vert{C_k}\vert} {\vert{D}\vert} })^2 $$</p><p><small>其中 $C_k$ 是 <code>D</code> 中属于第 <code>k</code> 类的样本子集，<code>K</code> 是类的个数。</small> </p><p>如果样本集合 <code>D</code> 根据特征 <code>A</code> 是否取某一个可能值 <code>a</code> 被分割为 <code>D1</code> 和 <code>D2</code>，即 $D1 &#x3D; {(x, y) \in D | A(x) \in a}, D2 &#x3D; D - D1$<br>则在特征 <code>A</code> 的条件下，集合 <code>D</code> 的基尼指数定义为：</p><p>$$ Gini(D, A) &#x3D; {\frac {\vert {D1} \vert} {\vert {D} \vert} } Gini(D1) + {\frac {\vert {D2} \vert} {\vert {D} \vert} } Gini(D2) $$</p><p><small>基尼指数 <code>Gini(D, A)</code> 表示 <code>A = a</code> 分割后集合 <code>D</code> 的不确定性。</small></p><p><small>基尼指数越大，样本集合的不稳定性则越大。</small></p><p><code>CART</code> 决策树基尼系数生成算法示例：</p><blockquote><p>输入：训练数据集 <code>D</code>，停止计算的条件<br>输出：<code>CART</code> 决策树<br>过程：</p><ol><li>根据训练数据集，根结点的数据集为 <code>D</code>，计算现有特征对该数据集的基尼指数，对于特征 <code>A</code> 有一个可能的取值 <code>a</code>，根据样本对 <code>A = a</code> 的测试为“是”或“否”，从而将数据集 <code>D</code> 拆分为 $D_1$ 和 $D_2$ 两部分，计算对应的基尼指数，</li><li>在计算特征 <code>A</code> 所有取值 <code>a</code> 对应的基尼指数后，选择其中基尼指数最小的特征作为对应的最优特征和最优切分点，至此就可以将数据集切分为两部分，生成两个子结点，将数据集分配到对应的子结点中。</li><li>对两个子结点递归地调用 <code>1, 2</code>，直至结点中的数据集个数小于预定阈值，或者基尼指数低于预定阈值，又或者没有足够的特征。</li><li>至此在满足所有条件后即可生成 <code>CART</code> 决策树。</li></ol></blockquote><hr><h3 id="决策树修剪"><a href="#决策树修剪" class="headerlink" title="决策树修剪"></a>决策树修剪</h3><p>决策树通过生成算法递归地产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确，但是却对未知的数据分类没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多的考虑如何提高针对训练数据的正确分类，从而构建出复杂的决策树，那么解决这个问题就是将复杂的决策树进行简单化。<br>在决策树的学习中将已生成的树进行简化的过程称为<strong>剪枝（<code>pruning</code>）</strong>，具体就是剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化树模型。</p><p>决策树的剪枝往往通过极小化决策树整体的 <em>损失函数（<code>lose function</code>）</em> 或 <em>代价函数（<code>cost function</code>）</em> 来实现。<br>设树 <code>T</code> 的叶结点个树为 ${\vert {T} \vert}$，<code>t</code> 是树 <code>T</code> 的叶结点，该叶结点有 $N_t$ 个样本点，其中 <code>k</code> 类的样本点有 $N_{tk}$ 个 <code>k = 1, 2, ..., K</code>，$H_t(T)$ 为叶结点 <code>t</code> 上的经验熵，<code>a &gt;= 0</code> 为参数，则决策树学习的损失函数可以定义为：</p><p>$$ C_a(T) &#x3D; \sum_{t&#x3D;1}^{\vert {T} \vert} N_t H_t (T) + a {\vert {T} \vert} $$</p><p>其中经验熵为：</p><p>$$ H_t(T) &#x3D; -\sum_k {\frac {N_{tk} } {N_t} } log {\frac {N_{tk} } {N_t} } $$</p><p>那么：</p><p>$$ C(T) &#x3D;  \sum_{t&#x3D;1}^{\vert {T} \vert} N_t H_t (T) &#x3D; -\sum_{t&#x3D;1}^{\vert {T} \vert} N_t \sum_{k&#x3D;1}^K  {\frac {N_{tk} } {N_t} } log {\frac {N_{tk} } {N_t} } &#x3D;  -\sum_{t&#x3D;1}^{\vert {T} \vert} \sum_{k&#x3D;1}^K {N_{tk} } log {\frac {N_{tk} } {N_t} } $$</p><p>即可得到：</p><p>$$ C_a(T) &#x3D; C(T) + a(T) $$</p><p>上述中 $C(T)$ 表示模型对训练数据的预测误差，即模型和训练数据的拟合程度，${\vert {T} \vert}$ 表示模型复杂度，参数 <code>a &gt;= 0</code> 控制两者之间的影响。那么较大的 <code>a</code> 就意味着选择较简单的决策树，较小的 <code>a</code> 选择较简单的决策树，<code>a = 0</code> 也就意味着只考虑决策树与训练数据的拟合程度，不考虑决策树的复杂度。<br>决策树剪枝也就是当 <code>a</code> 确定时，选择损失函数最小的决策树，即损失函数最小的子树，刚好通过使得损失函数最小化来平衡复杂的模型和训练数据的拟合程度。</p><p>下面就是具体的剪枝算法示例：</p><blockquote><p>输入；算法生成的决策树 <code>T</code>，参数 <code>a</code><br>输出：修剪后的子树 $T_a$ <br><br>过程：</p><ol><li>计算每个结点的经验熵。</li><li>递归地从树的叶结点向上回缩。<br> <img src="/ML-%E5%86%B3%E7%AD%96%E6%A0%91/decision_tree_2.jpg" alt="decision_tree-2"><br> 假设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_B$ 与 $T_A$，其对应的损失函数为 $C_a(T_B)$ 与 $C_a(T_A)$，如果 $ C_a(T_B) &lt;&#x3D; C_a(T_A) $ 则进行剪枝，即将父结点变为新的叶结点。</li><li>返回 <code>2</code>，直至不能继续为止，即可得到损失函数最小的子树 $T_a$。</li></ol></blockquote><h4 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a><code>CART</code> 剪枝</h4><p><code>CART</code> 剪枝算法从决策树底端剪去一些子树，使决策树变小，模型变简单，从而能够对未知数据更准确的预测。<br><code>CART</code> 剪枝算法由两步组成：</p><ul><li><p>首先从算法生成的决策树 $T_0$ 底端开始不断剪枝，直到 $T_0$ 的根结点，形成一个子树序列（$T_0$, $T_1$, …, $T_n$）。<br>  剪枝过程中，计算子树的损失函数：<br>  $$ C_a(T) &#x3D; C(T) + a{\vert {T} \vert} $$<br>  <small><code>T</code> 为任意子树，$C(T)$ 为对训练数据的预测误差，${\vert {T} \vert}$ 为子树的叶结点个树，<code>a &gt;= 0</code> 为参数，$C_a(T)$ 为参数是 <code>a</code> 时的子树 <code>T</code> 的整体损失。</small><br>  对于某个的 <code>a</code>，一定存在使损失函数 $C_a(T)$ 最小的子树 $T_a$，即 $T_a$ 在损失函数 $C_a(T)$ 最小时最优。当 <code>a</code> 大的时候，最优子树 $T_a$ 偏小；当 <code>a</code> 小的时候，最优子树 $T_a$ 偏大；而极端情况下 <code>a = 0</code>，整体树最优；当 <code>a -&gt; </code> $\infty$ 时，根结点组成的单结点树是最优的。</p><p>  具体从整树 $T_0$ 开始剪枝，对 $T_0$ 的任意内部结点 <code>t</code>，以 <code>t</code> 为单结点树的损失函数为：<br>  $$ C_a(T) &#x3D; C(t) + a $$</p><p>  以 <code>t</code> 为根结点的子树 $T_t$ 的损失函数为：<br>  $$ C_a(T_t) &#x3D; C(T_t) + a {\vert {T_t} \vert} $$ </p><ul><li>当 <code>a = 0</code> 及 <code>a</code> 充分小的时候，有不等式：$ C_a(T_t) &lt; C_a(t) $</li><li>当 <code>a</code> 增大时，在某一个 <code>a</code> 有：$ C_a(T_t) &#x3D; C_a(t) $</li><li>当 <code>a</code> 继续增大时，只要 $a &#x3D; {\frac {C(t) - C(T_t)} { {\vert {T_t}\vert} } - 1}$ 即 $T_t$ 与 <code>t</code> 具有相同的损失函数，而 <code>t</code> 的结点较少，因此 <code>t</code> 比 $T_t$ 更可取，对 $T_t$ 进行剪枝。</li></ul><p> 为此计算 $T_0$ 中每个内部结点 <code>t</code><br> $$ g(t) &#x3D; {\frac {C(t) - C(T_t)} { {\vert {T_t}\vert} - 1} }$$<br> 表示剪枝后整体损失函数减小的程度。</p><p> 在 $T_0$ 中剪去 <code>g(t)</code> 最小的子树 $T_t$，将得到子树作为 $T_1$，同时将最小的 <code>g(t)</code> 设为 $a_1$，$T_1$ 为区间 $[a_1, a_2)$ 的最优子树。如此剪枝下去直至根结点，而在这一过程中，不断递增 <code>a</code>，不断产生新的区间。</p></li><li><p>接着通过交叉验证法在独立的验证数据集上对子树序列进行测试，从而选出最优子树。<br>  利用验证数据集测试子树序列 $T_0, T_1, …, T_n$ 中各个子树的平均方差或基尼系数，平方误差或基尼系数最小的子树也就是最优的决策树，而当最优子树被确定时，子树对应的 <code>a</code> 也会被确定，即可得到最优子树 $T_a$。</p></li></ul><p>下面就是具体的 <code>CART</code> 剪枝算法示例：</p><blockquote><p>输入：<code>CART</code> 算法生成的决策树 $T_0$<br><br>输出：最优决策树 $T_a$<br><br>过程：</p><ol><li>设 $k &#x3D; 0, T &#x3D; T_0$。</li><li>设 <code>a </code> $ &#x3D; + \infty$。</li><li>自上而下地对各内部结点 <code>t</code> 计算 $C(T_t)$，${\vert {T_t} \vert}$ 以及<br>  $$ g(t) &#x3D; {\frac {C(t) - C(T_t)} { {\vert {T_t} \vert} - 1} } $$<br>  $$ a &#x3D; min(a, g(t)) $$<br> <small> $T_t$ 表示以 <code>t</code> 为根结点的子树，$C(T_t)$ 是对训练数据的预测误差，${\vert {T_t} \vert}$ 是 $T_t$ 的叶结点个树。</small></li><li>自上而下地访问内部结点 <code>t</code>，如果有 <code>g(t) = a</code>，则进行剪枝并对叶结点 <code>t</code> 以多数表决法决定其类，得到树 <code>T</code>。</li><li>设 $k &#x3D; k + 1, a_k &#x3D;$ <code> a</code> $, T_k &#x3D; T$。</li><li>如果 <code>T</code> 不是由根结点单独组成的树，则返回 <code>4</code>。</li><li>采用交叉验证法在子树序列 $T_0, T_1, …, T_n$ 中选取最优子树 $T_a$。</li></ol></blockquote><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>分类决策树模型是用于表示基于特征对实例进行分类的树形结构体，决策树可以转换为 <code>if-then</code> 规则集合，也可以看作是定义在特征空间上划分的条件概率分布。</li><li>决策树学习宗旨是构建一个可以对训练数据很好的分类，但同时复杂度较小的决策树。<br> 决策树学习算法包含三部分：特征选择、树的生成、树的剪枝。<br> 常用的算法：<code>ID3</code>、 <code>C4.5</code>、 <code>CART</code>。</li><li>特征选择的目的在于能够选取对实例分类有用的特征，常用的准则如下：<ul><li>样本集合 <code>D</code> 对特征 <code>A</code> 的信息增益：<br> $$ g(D, A) &#x3D; H(D) - H(D|A) $$<br> $$ H(D) &#x3D; - \sum_{k&#x3D;1}^K {\frac {\vert {C_k}\vert} {\vert {D} \vert} } log_2 {\frac {\vert {C_k}\vert} {\vert {D} \vert} } $$<br> $$ H(D|A) &#x3D; \sum_{i&#x3D;1}^n {\frac {\vert {D_i} \vert} {\vert {D} \vert} H(D_i) } $$</li><li>样本集合 <code>D</code> 对特征 <code>A</code> 的信息增益比：<br> $$ g_r(D, A) &#x3D; {\frac {g(D, A)} {H(D)} } $$</li><li>样本集合 <code>D</code> 的基尼指数：<br> $$ Gini(D) &#x3D; 1 - \sum_{k&#x3D;1}^K ({\frac {\vert {C_k} \vert} {\vert {D} \vert} })^2 $$<br> 特征 <code>A</code> 条件下集合 <code>D</code> 的基尼指数：<br> $$  Gini(D, A) &#x3D; {\frac {\vert {D1} \vert} {\vert {D} \vert} } Gini(D1) + {\frac {\vert {D2} \vert} {\vert {D} \vert} } Gini(D2) $$</li></ul></li><li>决策树的生成同时是采用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则，然后从根结点开始，通过特征选择准则划分正确的子集，递归地生成决策树。</li><li>生成决策树的过程中会产生过拟合问题，因此需要剪枝，从已生成的决策树上剪掉一些叶结点或者子结点，并将其父结点作为新的叶结点，从而简化决策树。</li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习《统计学习方法》所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概要&quot;&gt;&lt;a href=&quot;#概要&quot; class=&quot;headerlink&quot; title=&quot;概要&quot;&gt;&lt;/a&gt;概要&lt;/h3&gt;&lt;p&gt;决策树（&lt;code&gt;decision tree&lt;/code&gt;）是一种基本的&lt;strong&gt;分类和回归方法&lt;/strong&gt;。其主要呈现为&lt;strong&gt;树状结构&lt;/strong&gt;，在分类问题中，表示基于特征对实例进行分类的过程，可以被认为是 &lt;strong&gt;&lt;code&gt;if-then&lt;/code&gt; 的规则集合&lt;/strong&gt;，也可以被认为是定义在&lt;strong&gt;特征空间与类空间上的条件概率分布&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其优点主要有分类速度快、模型具有可读性，在学习时利用训练数据根据&lt;strong&gt;损失函数最小化&lt;/strong&gt;的原则建立决策树模型；而在预测时对新的数据利用&lt;strong&gt;决策树模型&lt;/strong&gt;进行分类。&lt;/p&gt;
&lt;p&gt;决策树模型主要包含以下步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征选择&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;决策树的生成&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;决策树的修剪&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>ML实践-Pima 数据集</title>
    <link href="https://blog.vgbhfive.cn/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://blog.vgbhfive.cn/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/</id>
    <published>2023-07-17T15:30:07.000Z</published>
    <updated>2023-09-09T07:49:13.562Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h3><p>该数据集最初来自糖尿病&#x2F;消化&#x2F;肾脏疾病研究所，此<a href="https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database">数据集</a>的目标是基于数据集中包含的某些身体指标来诊断性的预测患者是否患有糖尿病。<br>数据集由多个医学指标和一个目标变量 <code>Outcome</code> 组成，医学指标包含患者的怀孕次数、<code>BMI</code> 指数、胰岛素水平、年龄、血压等。</p><span id="more"></span><hr><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h4 id="导入基础依赖"><a href="#导入基础依赖" class="headerlink" title="导入基础依赖"></a>导入基础依赖</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure><h4 id="导入数据查看基础信息"><a href="#导入数据查看基础信息" class="headerlink" title="导入数据查看基础信息"></a>导入数据查看基础信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pima = pd.read_csv(<span class="string">&quot;diabetes.csv&quot;</span>)</span><br><span class="line"><span class="comment"># pima.head()</span></span><br><span class="line"><span class="comment"># pima.info()</span></span><br><span class="line"><span class="comment"># pima.shape</span></span><br><span class="line">pima.describe()</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/1.png" alt="1"></p><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 柱状图</span></span><br><span class="line">pima.hist(figsize=(<span class="number">16</span>, <span class="number">14</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 散点图</span></span><br><span class="line">sns.pairplot(pima, hue=<span class="string">&quot;Outcome&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 箱图</span></span><br><span class="line">pima.plot(kind=<span class="string">&quot;box&quot;</span>, subplots=<span class="literal">True</span>, layout=(<span class="number">3</span>,<span class="number">3</span>), sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, figsize=(<span class="number">16</span>,<span class="number">14</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 热点图</span></span><br><span class="line">column_x = pima.columns[<span class="number">0</span>: <span class="built_in">len</span>(pima.columns)-<span class="number">1</span>]</span><br><span class="line">column_x</span><br><span class="line">corr = pima[pima.columns].corr()</span><br><span class="line">plt.subplots(figsize=(<span class="number">16</span>,<span class="number">14</span>))</span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest </span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"> </span><br><span class="line">X = pima.iloc[:, <span class="number">0</span>:<span class="number">8</span>] <span class="comment"># 特征列 0-7列，不含第8列。</span></span><br><span class="line">Y = pima.iloc[:, <span class="number">8</span>] <span class="comment"># 目标列为第8列</span></span><br><span class="line"><span class="comment"># iloc([rows], [cols]) 第一个参数为要截取的行，第二个参数为要截取的列。loc([rows], [cols_name]) 第一个参数为要截取的行，第二个参数为要截取的列名称</span></span><br><span class="line"></span><br><span class="line">select_top_4 = SelectKBest(score_func=chi2, k =<span class="number">4</span>) <span class="comment"># 通过卡方检验选择4个得分最高的特征</span></span><br><span class="line">fits = select_top_4.fit(X, Y) <span class="comment">#将特征输入到评分函数，获取特征信息和目标值信息</span></span><br><span class="line">features = fits.transform(X) <span class="comment">#展现特征转换后的结果</span></span><br><span class="line">features[<span class="number">0</span>:<span class="number">5</span>] <span class="comment">#新特征列</span></span><br><span class="line"></span><br><span class="line">pima.head() <span class="comment"># 表现最佳的特征为 Glucose， Insulin， BMI， Age</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造新的 DataFrame</span></span><br><span class="line">x_features = pd.DataFrame(data=features, columns=[<span class="string">&#x27;Glucose&#x27;</span>, <span class="string">&#x27;Insulin&#x27;</span>, <span class="string">&#x27;BMI&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>])</span><br><span class="line">x_features</span><br><span class="line">y_features = pd.DataFrame(data=Y, columns=[<span class="string">&#x27;Outcome&#x27;</span>])</span><br><span class="line">y_features</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/2.png" alt="2"><br><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/3.png" alt="3"></p><h4 id="特征标准化"><a href="#特征标准化" class="headerlink" title="特征标准化"></a>特征标准化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将属性值更改为 均值为0，标准差为1 的 高斯分布. 当算法期望输入特征处于高斯分布时，它非常有用</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">rescaled_x = StandardScaler().fit_transform(x_features)</span><br><span class="line">x = pd.DataFrame(data=rescaled_x, columns=x_features.columns)</span><br><span class="line">x.head()</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/4.png" alt="4"></p><h4 id="数据切分"><a href="#数据切分" class="headerlink" title="数据切分"></a>数据切分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y_features, test_size=test_size, random_state=seed)</span><br><span class="line">x_train.head()</span><br><span class="line">y_train.head()</span><br></pre></td></tr></table></figure><h4 id="构建二分类算法模型"><a href="#构建二分类算法模型" class="headerlink" title="构建二分类算法模型"></a>构建二分类算法模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((<span class="string">&quot;LB&quot;</span>, LogisticRegression()))</span><br><span class="line">models.append((<span class="string">&quot;NB&quot;</span>, GaussianNB()))</span><br><span class="line">models.append((<span class="string">&quot;KNN&quot;</span>, KNeighborsClassifier()))</span><br><span class="line">models.append((<span class="string">&quot;DT&quot;</span>, DecisionTreeClassifier()))</span><br><span class="line">models.append((<span class="string">&quot;SVM&quot;</span>, SVC()))</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">names = []</span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">22</span>)</span><br><span class="line">    cv_result = cross_val_score(model, x_train, y_train, cv=kfold, scoring=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">    names.append(name)</span><br><span class="line">    results.append(cv_result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(names)):</span><br><span class="line">    <span class="built_in">print</span>(names[i], results[i].mean())</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/5.png" alt="5"></p><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">classifier = SVC(kernel=<span class="string">&quot;rbf&quot;</span>)</span><br><span class="line">classifier.fit(x_train, y_train)</span><br><span class="line">y_pred = classifier.predict(x_test)</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred) <span class="comment"># 混淆矩阵</span></span><br><span class="line">cm</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred)) <span class="comment"># 显示准确率</span></span><br><span class="line"></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f%%&quot;</span> % (accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/6.png" alt="6"><br><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/7.png" alt="7"><br><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/8.png" alt="8"></p><h4 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用”网格搜索“来提高模型 - 模型优化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>]&#125;</span><br><span class="line">grid = GridSearchCV(SVC(), param_grid, refit=<span class="literal">True</span>, verbose=<span class="number">2</span>)</span><br><span class="line">grid.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">grid_prediction = grid.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, grid_prediction))</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/9.png" alt="9"></p><hr><h3 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h3><h4 id="评分函数"><a href="#评分函数" class="headerlink" title="评分函数"></a>评分函数</h4><ol><li><p><code>SelectKBest()</code><br>只保留 <code>K</code> 个最高分的特征，能够返回特征评价的得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SelectKBest(score_func=&lt;function f_classif&gt;, k=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>SelectPercentile()</code><br>只保留用户指定百分比的最高得分的特征，能够返回特征评价的得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SelectPercentile(score_func=&lt;function f_classif&gt;, percentile=<span class="number">10</span>) </span><br></pre></td></tr></table></figure><p>使用常见的单变量统计检验：假正率 <code>SelectFpr</code>，错误发现率 <code>SelectFdr</code>，或者总体错误率 <code>SelectFwe</code>。</p></li><li><p><code>GenericUnivariateSelect()</code><br>通过结构化策略进行特征选择，通过超参数搜索估计器进行特征选择。</p></li></ol><h4 id="cross-val-score-函数"><a href="#cross-val-score-函数" class="headerlink" title="cross_val_score() 函数"></a><code>cross_val_score()</code> 函数</h4><p><code>cross_val_score()</code> 函数，交叉验证评分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cross_validation.cross_val_score(estimator, X, y=<span class="literal">None</span>, scoring=<span class="literal">None</span>, cv=<span class="literal">None</span>, n_jobs=<span class="number">1</span>, verbose=<span class="number">0</span>, fit_params=<span class="literal">None</span>, pre_dispatch=<span class="string">&#x27;2*n_jobs&#x27;</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>estimator</code>：数据对象 </li><li><code>X</code>：数据 </li><li><code>y</code>：预测数据 </li><li><code>soring</code>：调用的方法</li><li><code>cv</code>：交叉验证生成器或可迭代的次数 </li><li><code>n_jobs</code>：同时工作的 <code>cpu</code> 个数（<code>-1</code> 代表全部）</li><li><code>verbose</code>：详细程度</li><li><code>fit_params</code>：传递给估计器的拟合方法的参数</li><li><code>pre_dispatch</code>：控制并行执行期间调度的作业数量</li></ul><h4 id="KFold-交叉验证"><a href="#KFold-交叉验证" class="headerlink" title="KFold 交叉验证"></a><code>KFold</code> 交叉验证</h4><p><code>K</code> 折交叉验证，将数据集分成 <code>K</code> 份的官方给定方案，所谓 <code>K</code> 折就是将数据集通过 <code>K</code> 次分割，使得所有数据既在训练集出现过，又在测试集出现过，当然每次分割中不会有重叠，相当于无放回抽样。<br><code>StratifiedKFold</code> 用法类似 <code>Kfold</code>，但是他是分层采样，确保训练集，测试集中各类别样本的比例与原始数据集中相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.KFold(n_splits=<span class="number">3</span>, shuffle=<span class="literal">False</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>将训练&#x2F;测试数据集划分 <code>n_splits</code> 个互斥子集，每次用其中一个子集当作验证集，剩下的 <code>n_splits-1</code> 个作为训练集，进行 <code>n_splits</code> 次训练和测试，得到 <code>n_splits</code> 个结果。</p><p>参数说明：</p><ul><li><code>n_splits</code>：表示划分几等份</li><li><code>shuffle</code>：在每次划分时，是否进行洗牌<ul><li>若为 <code>False</code> 时，其效果等同于 <code>random_state</code> 等于整数，每次划分的结果相同。</li><li>若为 <code>True</code> 时，每次划分的结果都不一样，表示经过洗牌，随机取样的。</li></ul></li><li><code>random_state</code>：随机种子数</li></ul><p><small>注意点：对于不能均等份的数据集，其前 <code>n_samples % n_splits</code> 子集拥有 <code>n_samples // n_splits + 1</code> 个样本，其余子集都只有 <code>n_samples // n_splits</code> 样本。</small></p><h4 id="LeaveOneOut-留一法"><a href="#LeaveOneOut-留一法" class="headerlink" title="LeaveOneOut 留一法"></a><code>LeaveOneOut</code> 留一法</h4><p><code>LeaveOneOut</code> 留一法，每一回合中，几乎所有的样本都用于训练模型，因此最接近原始样本的分布，这样的评估所得的结果比较可靠。实验过程中，没有随机因素会影响实验数据，确保实验过程是可以被复制的。<br>但是 <code>LeaveOneOut</code> 也有明显的缺点，就是计算成本高，当原始样本数很多时，需要花费大量的时间去完成算法的运算与评估。</p><h4 id="SVC-函数"><a href="#SVC-函数" class="headerlink" title="SVC() 函数"></a><code>SVC()</code> 函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>,tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, max_iter=-<span class="number">1</span>, decision_function_shape=<span class="literal">None</span>,random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>C</code>：<code>C-SVC</code> 的惩罚参数 <code>C</code>，默认值是 <code>1.0</code>。<br>  <code>C</code> 越大，相当于惩罚松弛变量，希望松弛变量接近 <code>0</code>，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。<br>  <code>C</code> 值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。</li><li><code>kernel</code>：核函数，默认是 <code>rbf</code>，可以是 <code>linear</code>、 <code>poly</code>、 <code>rbf</code>、 <code>sigmoid</code>、 <code>precomputed</code><ul><li>线性：<code>u&#39;v</code></li><li>多项式：<code>(gamma*u&#39;*v + coef0)^degree</code></li><li><code>RBF</code> 函数：<code>exp(-gamma|u-v|^2)</code></li><li><code>sigmoid</code>：<code>tanh(gamma*u&#39;*v + coef0)</code></li></ul></li><li><code>degree</code>：多项式 <code>poly</code> 函数的维度，默认是 <code>3</code>，选择其他核函数时会被忽略</li><li><code>gamma</code>： <code>rbf</code>、 <code>poly</code> 和 <code>sigmoid</code> 的核函数参数，默认是 <code>auto</code>，则会选择 <code>1/n_features</code></li><li><code>coef0</code>：核函数的常数项，对于 <code>poly </code>和 <code>sigmoid</code> 有用</li><li><code>probability</code>：是否采用概率估计，默认为 <code>False</code></li><li><code>shrinking</code>：是否采用 <code>shrinking heuristic</code> 方法，默认为 <code>True</code></li><li><code>tol</code>：停止训练的误差值大小，默认为 <code>1e-3</code></li><li><code>cache_size</code>：核函数 <code>cache</code> 缓存大小，默认为 <code>200</code></li><li><code>class_weight</code>：类别的权重，字典形式传递。设置第几类的参数 <code>C</code> 为 <code>weight*C</code></li><li><code>verbose</code>：是否允许冗余输出</li><li><code>max_iter</code>：最大迭代次数，<code>-1</code> 为无限制</li><li><code>decision_function_shape</code>：<code>ovo</code>、 <code>ovr</code> 或者 <code>None</code>，默认值为 <code>None</code></li><li><code>random_state</code>：数据洗牌时的种子值</li></ul><p>主要调节的参数有：<code>C</code>、 <code>kernel</code>、 <code>degree</code>、 <code>gamma</code>、 <code>coef0</code>。</p><h4 id="confusion-matrix-函数"><a href="#confusion-matrix-函数" class="headerlink" title="confusion_matrix() 函数"></a><code>confusion_matrix()</code> 函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.confusion_matrix(y_true, y_pred, labels=<span class="literal">None</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>y_true</code>：是样本真实分类结果</li><li><code>y_pred</code>：是样本预测分类结果</li><li><code>labels</code>：是所给出的类别，通过这个可对类别进行选择 </li><li><code>sample_weight</code> : 样本权重</li></ul><h4 id="classification-report-预测准确率"><a href="#classification-report-预测准确率" class="headerlink" title="classification_report() 预测准确率"></a><code>classification_report()</code> 预测准确率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report(y_true, y_pred, *, labels=<span class="literal">None</span>, target_names=<span class="literal">None</span>, sample_weight=<span class="literal">None</span>, digits=<span class="number">2</span>, output_dict=<span class="literal">False</span>, zero_division=<span class="string">&#x27;warn&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/ML%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/7.png" alt="7"><br>输出说明：</p><ul><li><code>precision</code>: 准确率，<code>TP/ (TP+FP) </code></li><li><code>recall</code>: 召回率，<code>TP(TP + FN)</code></li><li><code>f1-score</code>: 是准确率与召回率的综合，可以认为是平均结果，<code>2*TP/(2*TP + FP + FN)</code><ul><li><code>TP</code>: 预测为正，实现为正</li><li><code>FP</code>: 预测为正，实现为负</li><li><code>FN</code>: 预测为负，实现为正</li><li><code>TN</code>: 预测为负，实现为负</li></ul></li></ul><h4 id="GridSearchCV"><a href="#GridSearchCV" class="headerlink" title="GridSearchCV()"></a><code>GridSearchCV()</code></h4><p><code>GridSearchCV()</code> 可以拆分为两部分 <code>GridSearch</code> 和 <code>CV</code>，即网格搜索和交叉验证。网格搜索，指的是参数，即在指定的参数范围内，按步长依次调整参数，利用调整的参数训练学习器，从所有的参数中找到在验证集上精度最高的参数，这其实是一个训练和比较的过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, refit=<span class="literal">True</span>, cv=<span class="literal">None</span>, verbose=<span class="number">0</span>, pre_dispatch=<span class="string">&#x27;2*n_jobs&#x27;</span>, error_score=nan, return_train_score=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>estimator</code>：选择使用的分类器，并且传入除需要确定最佳的参数之外的其他参数。并且每一个分类器都需要一个 <code>scoring</code> 参数或者 <code>score</code> 方法。</li><li><code>param_grid</code>：需要最优化的参数的取值，值为字典或者列表。</li><li><code>scoring=None</code>：模型评价标准，默认 <code>None</code>。这时需要使用 <code>score</code> 函数，根据所选模型不同，评价准则不同。</li><li><code>n_jobs</code>：并行数，<code>-1</code> 跟 <code>CPU</code> 核数一致。</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>机器学习总体来说不像编程需要很强的计算机基础知识，但额外需要了解业务方面的知识以及对所使用包的熟悉程度，因此还是那句老话“<strong>实践出真知</strong>”。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;数据简介&quot;&gt;&lt;a href=&quot;#数据简介&quot; class=&quot;headerlink&quot; title=&quot;数据简介&quot;&gt;&lt;/a&gt;数据简介&lt;/h3&gt;&lt;p&gt;该数据集最初来自糖尿病&amp;#x2F;消化&amp;#x2F;肾脏疾病研究所，此&lt;a href=&quot;https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database&quot;&gt;数据集&lt;/a&gt;的目标是基于数据集中包含的某些身体指标来诊断性的预测患者是否患有糖尿病。&lt;br&gt;数据集由多个医学指标和一个目标变量 &lt;code&gt;Outcome&lt;/code&gt; 组成，医学指标包含患者的怀孕次数、&lt;code&gt;BMI&lt;/code&gt; 指数、胰岛素水平、年龄、血压等。&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Docker install jupyter-notebook</title>
    <link href="https://blog.vgbhfive.cn/Docker-install-jupyter-notebook/"/>
    <id>https://blog.vgbhfive.cn/Docker-install-jupyter-notebook/</id>
    <published>2023-06-23T03:31:59.000Z</published>
    <updated>2023-06-23T04:13:23.268Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><code>Jupyter Notebook</code> 是一个开源的 <code>Web</code> 应用程序，允许用户创建和共享包含代码、方程式、可视化和文本的文档。主要用于 <strong>数据清理和转换</strong>、<strong>数值模拟</strong>、<strong>统计建模</strong>、<strong>数据可视化</strong>、<strong>机器学习</strong> 等等。<br>具有以下优势：</p><ul><li>可选择语言：支持超过 <code>40</code> 种编程语言，包括 <code>Python</code>、<code>R</code>、<code>Julia</code>、<code>Scala</code> 等。</li><li>分享笔记本：可以使用电子邮件、<code>Dropbox</code>、<code>GitHub</code> 和 <code>Jupyter Notebook Viewer</code> 与他人共享。</li><li>交互式输出：代码可以生成丰富的交互式输出，包括 <code>HTML</code>、图像、视频、<code>LaTeX</code> 等等。</li><li>大数据整合：通过 <code>Python</code>、<code>R</code>、<code>Scala</code> 编程语言使用 <code>Apache Spark</code> 等大数据框架工具。支持使用 <code>pandas</code>、<code>scikit-learn</code>、<code>ggplot2</code>、<code>TensorFlow</code> 来探索同一份数据。</li></ul><span id="more"></span><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker search jupyter</span></span><br><span class="line">NAME                                 DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">jupyter/scipy-notebook               Scientific Jupyter Notebook Python Stack fro…   404</span><br><span class="line">jupyter/tensorflow-notebook          Scientific Jupyter Notebook Python Stack w/ …   344</span><br><span class="line">jupyter/all-spark-notebook           Python, Scala, R and Spark Jupyter Notebook …   417</span><br><span class="line">jupyter/pyspark-notebook             Python and Spark Jupyter Notebook Stack from…   277</span><br><span class="line">jupyter/datascience-notebook         Data Science Jupyter Notebook Python Stack f…   1027</span><br><span class="line">jupyterhub/singleuser                single-user docker images for use with Jupyt…   45                   [OK]</span><br><span class="line">jupyterhub/jupyterhub                JupyterHub: multi-user Jupyter notebook serv…   326                  [OK]</span><br><span class="line">jupyter/minimal-notebook             Minimal Jupyter Notebook Python Stack from h…   183</span><br><span class="line">jupyter/base-notebook                Base image for Jupyter Notebook stacks from …   203</span><br><span class="line">jupyterhub/k8s-hub                                                                   22</span><br><span class="line">jupyterhub/k8s-network-tools                                                         2</span><br><span class="line">jupyterhub/configurable-http-proxy   node-http-proxy + REST API                      6                    [OK]</span><br><span class="line">jupyter/nbviewer                     Jupyter Notebook Viewer                         32                   [OK]</span><br><span class="line">jupyterhub/k8s-singleuser-sample                                                     10</span><br><span class="line">jupyter/r-notebook                   R Jupyter Notebook Stack from https://github…   54</span><br><span class="line">jupyterhub/k8s-image-awaiter                                                         2</span><br><span class="line">jupyter/repo2docker                  Turn git repositories into Jupyter enabled D…   21</span><br><span class="line">jupyterhub/k8s-secret-sync                                                           1</span><br><span class="line">jupyterhub/jupyterhub-onbuild        onbuild version of JupyterHub images            6</span><br><span class="line">jupyter/demo                         (DEPRECATED) Demo of the IPython/Jupyter Not…   16</span><br><span class="line">bitnami/jupyter-base-notebook                                                        39</span><br><span class="line">jupyterhub/k8s-image-cleaner                                                         1</span><br><span class="line">jupyterhub/k8s-binderhub                                                             3</span><br><span class="line">jupyterhub/k8s-pre-puller                                                            1</span><br><span class="line">bitnami/jupyterhub                                                                   18</span><br></pre></td></tr></table></figure><h4 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">dockcer pull jupyter/datascience-notebook</span></span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from jupyter/datascience-notebook</span><br><span class="line">d5fd17ec1767: Pull complete </span><br><span class="line">9288915018bc: Pull complete </span><br><span class="line">ad895732ee5c: Pull complete </span><br><span class="line">4f4fb700ef54: Pull complete </span><br><span class="line">7b053b0d567d: Pull complete </span><br><span class="line">577d6f3bb6f4: Pull complete </span><br><span class="line">1d18b5a5f242: Pull complete </span><br><span class="line">ffc9ad0a0b36: Pull complete </span><br><span class="line">75f3e04b1547: Pull complete </span><br><span class="line">e9036ae1aec3: Pull complete </span><br><span class="line">8016e50184c6: Pull complete </span><br><span class="line">55f4c93ee7b8: Pull complete </span><br><span class="line">17c3e54db24b: Pull complete </span><br><span class="line">e8c81a9b6c9a: Pull complete</span><br><span class="line">530f1db1e9d7: Pull complete </span><br><span class="line">44fa9360bdc5: Pull complete </span><br><span class="line">6f59df66069f: Pull complete </span><br><span class="line">a8c1c1bcf1d4: Pull complete </span><br><span class="line">5784e3ca1d66: Pull complete </span><br><span class="line">60e2c9b0e0a4: Pull complete </span><br><span class="line">4866b0f6598a: Pull complete </span><br><span class="line">613fc67c0714: Pull complete </span><br><span class="line">2a41639ceb55: Pull complete </span><br><span class="line">fa391f2a4b79: Pull complete </span><br><span class="line">Digest: sha256:acd52864dd364e2e5c494ccefa661e6f58f551c9006ec7263d7b5afd5d1852e9</span><br><span class="line">Status: Downloaded newer image for jupyter/datascience-notebook:latest</span><br><span class="line">docker.io/jupyter/datascience-notebook:latest</span><br></pre></td></tr></table></figure><h4 id="启动镜像"><a href="#启动镜像" class="headerlink" title="启动镜像"></a>启动镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run -d -p 8000:8888 –name jupyter-notebook jupyter/datascience-notebook</span></span><br><span class="line">xxxxxxxxx</span><br></pre></td></tr></table></figure><h4 id="部署代理"><a href="#部署代理" class="headerlink" title="部署代理"></a>部署代理</h4><p>服务部署完毕后，通过 <code>Nginx</code> 反向代理到公网访问，下面是示例配置。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen          80;</span><br><span class="line">    server_name     jupyter.vgbhfive.com;</span><br><span class="line">    index           index.html;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass      http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header Host http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ /api/kernels/ &#123;</span><br><span class="line">        proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">        proxy_http_version    1.1;  # websocket support</span><br><span class="line">        proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">        proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">        proxy_read_timeout    86400;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ /terminals/ &#123;</span><br><span class="line">        proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">        proxy_http_version    1.1;  # websocket support</span><br><span class="line">        proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">        proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">        proxy_read_timeout    86400;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="页面访问"><a href="#页面访问" class="headerlink" title="页面访问"></a>页面访问</h4><img src="/Docker-install-jupyter-notebook/docker-1111.png" class="" title="docker-1111"><p>第一次访问建议通过 <code>Token</code> 访问并设置，之后就只需要密码即可进入。</p><hr><h3 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h3><h4 id="403-GET-api-kernels"><a href="#403-GET-api-kernels" class="headerlink" title="403 GET /api/kernels"></a><code>403 GET /api/kernels</code></h4><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[W 2023-06-21 08:28:50.567 ServerApp] 403 GET /api/contents/demo.ipynb/checkpoints?1687335545679 (@172.17.0.1) 1.28ms referer=http://jupyter.vgbhfive.com/lab/tree/demo.ipynb</span><br><span class="line">[I 2023-06-21 08:32:19.956 ServerApp] Connecting to kernel 66e74bf7-4e19-40cc-8437-1eb43b35e208.</span><br><span class="line">[I 2023-06-21 08:32:19.956 ServerApp] Restoring connection for 66e74bf7-4e19-40cc-8437-1eb43b35e208:35bb8ace-627e-4574-b476-08fae23ccaa5</span><br><span class="line">[W 2023-06-21 08:33:49.957 ServerApp] WebSocket ping timeout after 90000 ms.</span><br><span class="line">[I 2023-06-21 08:33:54.961 ServerApp] Starting buffering for 66e74bf7-4e19-40cc-8437-1eb43b35e208:35bb8ace-627e-4574-b476-08fae23ccaa5</span><br><span class="line">[W 2023-06-21 08:34:04.639 ServerApp] wrote error: &#x27;Forbidden&#x27;</span><br><span class="line">    Traceback (most recent call last):</span><br><span class="line">      File &quot;/opt/conda/lib/python3.11/site-packages/tornado/web.py&quot;, line 1784, in _execute</span><br><span class="line">        result = method(*self.path_args, **self.path_kwargs)</span><br><span class="line">                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span><br><span class="line">      File &quot;/opt/conda/lib/python3.11/site-packages/tornado/web.py&quot;, line 3278, in wrapper</span><br><span class="line">        url = self.get_login_url()</span><br><span class="line">              ^^^^^^^^^^^^^^^^^^^^</span><br><span class="line">      File &quot;/opt/conda/lib/python3.11/site-packages/jupyter_server/base/handlers.py&quot;, line 753, in get_login_url</span><br><span class="line">        raise web.HTTPError(403)</span><br><span class="line">    tornado.web.HTTPError: HTTP 403: Forbidden</span><br></pre></td></tr></table></figure><p>根据 <code>403</code> 状态码的解释，服务器已经解析请求但没有权限访问资源，那么问题就来了，在查了无数资料之后，看到了这个东西：</p><img src="/Docker-install-jupyter-notebook/docker-2222.png" class="" title="docker-2222"><p>该问题是由于在 <code>Nginx</code> 代理请求时，将所有的请求代理为 <code>HTTP</code> 请求，而 <code>Jupyter Notebook</code> 的部分请求为 <code>WebSocket</code>，从而导致请求异常，修改 <code>Nginx</code> 代理之后问题就迎刃而解。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">location ~ /api/kernels/ &#123;</span><br><span class="line">    proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">    proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">    proxy_http_version    1.1;  # websocket support</span><br><span class="line">    proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">    proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">    proxy_read_timeout    86400;</span><br><span class="line">&#125;</span><br><span class="line">location ~ /terminals/ &#123;</span><br><span class="line">    proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">    proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">    proxy_http_version    1.1;  # websocket support</span><br><span class="line">    proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">    proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">    proxy_read_timeout    86400;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="https://stackoverflow.com/questions/76094881/jupyter-docker-image-kernel-disconnected-400-apache-proxy">Jupyter Docker Image Kernel Disconnected 400 Apache Proxy</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Jupyter Notebook&lt;/code&gt; 是一个开源的 &lt;code&gt;Web&lt;/code&gt; 应用程序，允许用户创建和共享包含代码、方程式、可视化和文本的文档。主要用于 &lt;strong&gt;数据清理和转换&lt;/strong&gt;、&lt;strong&gt;数值模拟&lt;/strong&gt;、&lt;strong&gt;统计建模&lt;/strong&gt;、&lt;strong&gt;数据可视化&lt;/strong&gt;、&lt;strong&gt;机器学习&lt;/strong&gt; 等等。&lt;br&gt;具有以下优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可选择语言：支持超过 &lt;code&gt;40&lt;/code&gt; 种编程语言，包括 &lt;code&gt;Python&lt;/code&gt;、&lt;code&gt;R&lt;/code&gt;、&lt;code&gt;Julia&lt;/code&gt;、&lt;code&gt;Scala&lt;/code&gt; 等。&lt;/li&gt;
&lt;li&gt;分享笔记本：可以使用电子邮件、&lt;code&gt;Dropbox&lt;/code&gt;、&lt;code&gt;GitHub&lt;/code&gt; 和 &lt;code&gt;Jupyter Notebook Viewer&lt;/code&gt; 与他人共享。&lt;/li&gt;
&lt;li&gt;交互式输出：代码可以生成丰富的交互式输出，包括 &lt;code&gt;HTML&lt;/code&gt;、图像、视频、&lt;code&gt;LaTeX&lt;/code&gt; 等等。&lt;/li&gt;
&lt;li&gt;大数据整合：通过 &lt;code&gt;Python&lt;/code&gt;、&lt;code&gt;R&lt;/code&gt;、&lt;code&gt;Scala&lt;/code&gt; 编程语言使用 &lt;code&gt;Apache Spark&lt;/code&gt; 等大数据框架工具。支持使用 &lt;code&gt;pandas&lt;/code&gt;、&lt;code&gt;scikit-learn&lt;/code&gt;、&lt;code&gt;ggplot2&lt;/code&gt;、&lt;code&gt;TensorFlow&lt;/code&gt; 来探索同一份数据。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Docker" scheme="https://blog.vgbhfive.cn/tags/Docker/"/>
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>北京同仁医院验光攻略</title>
    <link href="https://blog.vgbhfive.cn/%E5%8C%97%E4%BA%AC%E5%90%8C%E4%BB%81%E5%8C%BB%E9%99%A2%E9%AA%8C%E5%85%89%E6%94%BB%E7%95%A5/"/>
    <id>https://blog.vgbhfive.cn/%E5%8C%97%E4%BA%AC%E5%90%8C%E4%BB%81%E5%8C%BB%E9%99%A2%E9%AA%8C%E5%85%89%E6%94%BB%E7%95%A5/</id>
    <published>2023-06-22T04:33:02.000Z</published>
    <updated>2023-06-23T03:31:28.743Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前要简介"><a href="#前要简介" class="headerlink" title="前要简介"></a>前要简介</h3><ol><li>首都医科大学附属北京同仁医院始建于 <code>1886</code> 年，是一所以眼科学、耳鼻咽喉科学为国家重点学科的大型综合三甲医院，对于眼科相关绝对是权威专业。</li><li>一般的眼镜店对于只验光不配镜的顾客有多多少少的抵触。</li><li>镜片和镜框需要单独在网上购买，接着找眼镜店帮忙组装，可以最大化保证钱花在刀刃上。</li></ol><span id="more"></span><hr><h3 id="预约挂号"><a href="#预约挂号" class="headerlink" title="预约挂号"></a>预约挂号</h3><ol><li><p>微信公众号搜索 <strong>北京同仁验光配镜中心服务号</strong> 并关注，该机构是同仁医院下属的营业性机构，其专业性和规范性由同仁医院保证，可以放心。</p></li><li><p>点击公众号的右下角 <strong>验光服务</strong>，首次预约会先填一个预约人的信息，填写完毕后选择对应的预约人，接下来就是选择店铺，推荐 <strong>中心店</strong> 即可。</p></li><li><p>紧跟着根据自己的日期规划选择对应的 <strong>视光服务费W</strong>，在完成缴费后即预约成功。</p></li></ol><hr><h3 id="行程"><a href="#行程" class="headerlink" title="行程"></a>行程</h3><p>目的地位于 <code>2</code> 号线和 <code>5</code> 号线的换乘地铁站 <strong>崇文门地铁站</strong>，到站后从 <strong><code>E</code></strong> 口出战，出站后 <strong>站口方向过马路（不过天桥）直行 <code>100</code> 米</strong> 就是目的地。</p><img src="/%E5%8C%97%E4%BA%AC%E5%90%8C%E4%BB%81%E5%8C%BB%E9%99%A2%E9%AA%8C%E5%85%89%E6%94%BB%E7%95%A5/111.png" class="" title="111.png"><hr><h3 id="验光流程"><a href="#验光流程" class="headerlink" title="验光流程"></a>验光流程</h3><ol><li><p>取号<br>进门之后像是一个眼镜店，此时不要慌，在门口左侧和正面均有一台 <strong>取号机</strong>，使用公众号发给你的二维码取号即可。</p></li><li><p>初步测试<br>在测试完成后，在进门的左侧有一个 <strong>服务台</strong>，到达服务台将号码给工作人员，对方会让你去验一下 <strong>眼镜度数</strong> 和 <strong>初步机器验光测试</strong>，这里根据流程走就可以。</p></li><li><p>人工验光<br>在完成上述测试后，会有工作人员给你一个号码，拿着号码到验光处等到叫号即可。<br><strong>人工验光</strong> 会持续大概 <code>20~30</code> 分钟左右，会简单询问下个人情况、用眼习惯、用眼场景等情况，按照个人情况如实说明即可，如果眼睛有相关就诊经历也可以说明。</p></li><li><p>完成<br>在验光完成后，对方会给你一个单子，然后告知你配镜的注意事项，在拿到单子后就可以走人了。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;前要简介&quot;&gt;&lt;a href=&quot;#前要简介&quot; class=&quot;headerlink&quot; title=&quot;前要简介&quot;&gt;&lt;/a&gt;前要简介&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;首都医科大学附属北京同仁医院始建于 &lt;code&gt;1886&lt;/code&gt; 年，是一所以眼科学、耳鼻咽喉科学为国家重点学科的大型综合三甲医院，对于眼科相关绝对是权威专业。&lt;/li&gt;
&lt;li&gt;一般的眼镜店对于只验光不配镜的顾客有多多少少的抵触。&lt;/li&gt;
&lt;li&gt;镜片和镜框需要单独在网上购买，接着找眼镜店帮忙组装，可以最大化保证钱花在刀刃上。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-HBase</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-HBase/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-HBase/</id>
    <published>2023-05-28T04:31:42.000Z</published>
    <updated>2023-06-21T14:56:16.038Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><code>HBase</code> 是一个在 <code>HDFS</code> 上开发的<strong>面向列</strong>的<strong>分布式数据库</strong>，如果你需要实时访问超大规模的数据集，那么使用 <code>HBase</code> 就对了。</p><p><code>HBase</code> <strong>自底而上</strong>地进行构建，可以简单的通过<strong>增加节点来线性扩展</strong>。其并不是关系型数据库，并且也不支持 <code>SQL</code>，在特定的空间里，能够做 <code>RDBMS</code> 不能做的事，即在廉价的硬件构成的集群上管理超大规模的稀疏表。</p><span id="more"></span><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>应用将数据放到带有标签的表中。而表有行和列组成，表的 <em>单元格</em> 由行和列的交叉决定，其是有版本的。默认情况下，版本号自动分配，为 <code>HBase</code> 插入单元格时的时间戳，单元格的内容是未解释的字节数组。</p><p>表中的行也是字节数组，因此理论上任何东西都可以通过表示成字节数组或者将二进制形式转换为长整型或直接对数据结构进行序列化来作为键值。表中的行根据行的键值进行排序，排序根据字节序进行，所有对表的访问都需要通过表的主键。</p><img src="/Hadoop-HBase/hadoop-HBase-2.jpg" class="" title="Hadoop-HBase"><p>行中的列被分为<strong>列族（<code>column family</code>）</strong>，同一个列族的所有成员具有相同的前缀。列族的前缀必须由 <em>可打印的</em> 字符组成，而修饰性的结尾字符，即列族修饰符，可以为任意字节。列族和修饰符之间始终以冒号（<code>:</code>）分隔。<br>一个表的列族必须作为表模式定义的一部分预先给出，但是新的列族成员可以随后按需加入。</p><p>在物理上所有的列族成员都一起存放在文件系统中，因此结合前面的将 <code>HBase</code> 描述为一个面向列的存储器，其本质上更准确的是面向列族的存储器。<br>但由于调优和存储都是在列族这个维度上进行，所以所有的列族成员具有相同的访问模式和大小特征是最优解。对于存储较大字节的数据和较小字节的数据最好分别存储在不同的列族中。</p><p>简而言之，<code>HBase</code> 的表和 <code>RDBMS</code> 的表类似，只不过其单元格有版本；行是排序的；只要列族是预先存在的，客户端随时可以将列添加进去。</p><h5 id="区域"><a href="#区域" class="headerlink" title="区域"></a>区域</h5><p><code>HBase</code> 自动将表水平划分为<strong>区域（<code>region</code>）</strong>，每个区域由表中行的子集构成，每个区域由所属表、所包含的第一行及最后一行来表示。<br>从表建立之初，一个表只有一个区域，但随着数据增多，其区域也会增大，知道区域超过阈值的界限，就会在某行的边界上将表区分为两个大小相同的新分区。</p><p>区域是 <code>HBase</code> 集群上分布数据的最小单位，使用这种方式不会因为数据太大而无法放置在单个机器上的表会被放到集群中，其中集群中的每个机器负责管理表所有区域的一个子集。而表的加载也是使用这种方法将数据分布到各个节点，集群中所有的机器上的节点按次序排列也就构成了表的所有内容。</p><h5 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h5><p>无论对行进行访问的事务牵涉到多少行，对行的更新都是 <em>原子级别</em>。</p><h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><ul><li><strong>没有真正的索引</strong><br>  行是顺序存储的，每行中的列也是，所以不存在索引膨胀的问题，而且插入性能和表的大小无关。</li><li><strong>自动分区</strong><br>  在表增长的时候，表会自动分裂成区域，并分布到可用的节点上。</li><li><strong>线性扩展和对于新节点的自动处理</strong><br>  增加一个节点将它指向现有集群并运行 <code>regionserver</code>。区域会自动重新进行平衡，负载均匀分布。</li><li>普通商用硬件支持</li><li><strong>容错</strong><br>  大量节点意味着每个节点的重要性并不突出，不用担心单个节点失效。</li><li><strong>批处理</strong><br>  <code>MapReduce</code> 集成功能使用全并行的分布式作业根据数据位置来处理它们。</li></ul><h4 id="与传统-RDBMS-比较"><a href="#与传统-RDBMS-比较" class="headerlink" title="与传统 RDBMS 比较"></a>与传统 <code>RDBMS</code> 比较</h4><p><code>HBase</code> 是一个分布式的、面向列的数据存储系统，通过在 <code>HDFS</code> 上提供随机读&#x2F;写来解决 <code>Hadoop</code> 不能处理的问题。<code>HBase</code> 自底层设计开始即聚集于各种可伸缩性问题：表可以很<em>高</em>（数十亿个数据行）；表可以很<em>宽</em>（数百万个列）；水平分区并在上千个普通机器上自动复制。表的模式是物理存储的直接反映，使系统有可能提供高效的数据结构的序列化、存储和检索，因此程序的开发者就必须选择以正确的方式使用这种存储和检索方式。</p><p>严格来说 <code>RDBMS</code> 是一个遵循 <strong><code>Codd</code> 的 <code>12</code> 条准则</strong>的数据库。标准的 <code>RDBMS</code> 是模式固定、面向行的数据库且具有 <code>ACID</code> 性质和复杂的 <code>SQL</code> 查询处理引擎。<code>RDBMS</code> 强调事务的<em>强一致性</em>、参照完整性、数据抽象和物理存储层相对独立，以及基于 <code>SQL</code> 语言的复杂查询支持。在 <code>RDBMS</code> 中，可以非常容易地建立 <em>二级索引</em>，执行复杂的内连接和外连接，执行计数、求和、排序、分组等操作，或对表、行和列中的数据进行分页存放。</p><hr><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>正如 <code>HDFS</code> 和 <code>YARN</code> 是由客户端、<code>slave</code> 和 <code>master</code> 组成，<code>HBase</code> 也采用了相同的架构，使用一个 <code>master</code> 节点协调管理一个或多个 <code>regionserver</code>。<code>HBase</code> 的 <code>master</code> 节点负责启动一个新的集群，将区域分配个注册的 <code>regionserver</code>，恢复 <code>regionserver</code> 的故障。<br><code>regionserver</code> 负责零个或多个区域的管理以及相应客户端的读写请求，还负责区域的划分并通知 <code>master</code> 有了新的子区域，因此 <code>master</code> 就可以将父区域设置为离线，并用子区域替代父区域。</p><img src="/Hadoop-HBase/hadoop-hbase.jpeg" class="" title="Hadoop-HBase"><p><code>HBase</code> 依赖于 <code>ZooKeeper</code>，默认情况下管理着一个 <code>ZooKeeper</code> 实例，作为集群的 <em>权威机构</em>，其还负责管理 <code>hbase:meta</code> 目录表的位置以及当前集群主控地址等重要信息。<br>如果在区域的分配过程中有服务器崩溃，就可以通过 <code>ZooKeeper</code> 来进行分配的协调。<br>在启动一个客户端到 <code>HBase</code> 集群的连接时，客户端必须至少拿到集群所传递的 <code>ZooKeeper</code> 集合体的位置，这样客户端才能访问 <code>ZooKeeper</code> 的层次结构，从而了解集群的属性。</p><p><code>regionserver</code> 从节点列表可以在 <code>HBase</code> 的 <code>conf/regionservers</code> 文件中看到，与 <code>Hadoop</code> 的一致。<br>集群的站点配置在 <code>HBase</code> 的 <code>conf/hbase-site.xml</code> 和 <code>conf/hbase-env.sh</code> 中，他们的格式和 <code>Hadoop</code> 父项目中对应的格式相同。</p><p>大部分人都是使用 <code>HDFS</code> 来运行 <code>HBase</code>，而 <code>HBase</code> 通过 <code>Hadoop</code> 文件系统 <code>API</code> 来持久化存储数据。但在默认情况下，<code>HBase</code> 会将存储写入本地文件系统。</p><h4 id="使用中的-HBase"><a href="#使用中的-HBase" class="headerlink" title="使用中的 HBase"></a>使用中的 <code>HBase</code></h4><p><code>HBase</code> 内部保留着名为 <strong><code>hbase:meta</code> 的特殊目录表（<code>catalog table</code>）</strong>，他们维护着当前集群上所有区域的列表、状态和位置。<code>hbase:meta</code> 表中的项使用区域名作为键，区域名由 <em>所属的表名</em>、<em>区域的起始行</em>、<em>区域的创建时间</em> 以及对其整体进行的 <em><code>MD5</code> 哈希值（即对表名、起始行、创建的时间戳进行哈希后的结果）</em> 组成。示例如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 表名、起始行、时间戳使用逗号隔开，而 MD5 哈希值则使用两个句号包围</span><br><span class="line">TestTable,xyz,1279753346289.8hjffu7fhsis6f638hf6sjf67jd9sj3g.</span><br></pre></td></tr></table></figure><p>如前所属，行的键都是排序的，因此想要查找一个特定行所在的区域只要在目录表中找到第一个键大于或等于给定行键的项即可。而当区域变化时，目录表也会进行对应的更新，集群上所有区域的状态信息都能保持更新。<br>每个行操作可能要访问三次远程节点，而为了节省代价，通常都会缓存 <code>hbase:meta</code> 信息，其中缓存的内容包含位置信息、用户空间区域的开始行和结束行。当使用缓存中的数据发生异常时，即区域被移动了，客户端会去查看 <code>hbase:meta</code> 获取区域的新位置，如果 <code>hbase:meta</code> 也被移动了，那客户端也会重新寻找。<br>当新的客户端连接到 <code>ZooKeeper</code> 时会首先查找 <code>hbase:meta</code> 的位置，然后通过寻找合适的区域来获取用户空间区域所在的节点和位置，接下来就是客户端直接与管理对应区域的 <code>regionserver</code> 进行交互。</p><p>到达 <code>Regionserver</code> 的写操作首先追加到 <em>提交日志（<code>commit log</code>）</em> 中，然后加入内存中的 <code>metastore</code>，若是 <code>metastore</code> 已满，则会将内容 <em>刷入（<code>flash</code>）</em> 文件系统。<br>提交日志存放在 <code>HDFS</code> 中，当发现某个 <code>Regionserver</code> 崩溃时，<code>master</code> 节点会根据区域对崩溃的 <code>Regionserver</code> 的提交日志进行分割，找到还没有被持久化存储的更新，然后这部分被 <em>重做（<code>replay</code>）</em> 以使区域恢复到崩溃之前的状态。<br>而在读的时候会检查区域的 <code>metastore</code>，如果在其中找到了需要的版本则查询至此结束。否则就需要按照次序从新道旧检查 <em>刷新文件（<code>flash</code>）</em>，直到找到们组查询的版本，或者所有刷新文件都处理完为止。</p><p>之前说到的 <em>刷新文件（<code>flash</code>）</em> 会有一个后台进程在其个数达到阈值时压缩他们，将多个文件重新写入一个文件，在执行压缩操作时，进程会清理掉超出模式所设最大值的版本以及删除单元格或标识单元格为过期。<br>而在 <code>Regionserver</code> 上会有另外一个进程监控着刷新文件的大小，一旦大小超过预先设定的最大值，便会对区域进行分割。</p><hr><h3 id="日常问题"><a href="#日常问题" class="headerlink" title="日常问题"></a>日常问题</h3><h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a><code>HDFS</code></h4><p><code>HBase</code> 使用 <code>HDFS</code> 的方式与 <code>MapReduce</code> 的使用方式截然不同。在 <code>HBase</code> 中数据文件在启动时就被打卡，并在处理过程中始终保持打开状态，这是为了节省每次访问操作打开文件所需的代价。</p><ol><li><p>文件描述符用完<br>由于在连接的集群上始终保持文件的打开状态，因此可能会达到系统和 <code>HDFS</code> 设定的限制。<br>其中一个进程默认的文件描述符限制是 <code>1024</code>，当使用的描述符个数超过文件系统的 <code>ulimit</code> 值，就会在日志中看到异常信息 <code>Too many open files</code>，不过在出现异常信息之前，往往 <code>HBase</code> 就已经出现问题了。</p></li><li><p><code>datanode</code> 中的线程用完<br>与上述的情况类似，<code>datanode</code> 上限制运行的线程数不能超过 <code>256</code>，可以通过在 <code>hdfs-site.xml</code> 中配置 <code>dfs.datanode.max.transfer.threads</code> 来更改设置。</p></li></ol><h4 id="用户界面"><a href="#用户界面" class="headerlink" title="用户界面"></a>用户界面</h4><p><code>HBase</code> 在 <code>master</code> 节点机器上运行了 <code>Web</code> 服务，提供了运行中集群的状态视图。默认情况下，其监听 <code>60010</code> 端口，主界面显示基本的属性（包含软件版本、集群负载、请求频率、集群表的列表）和加入的 <code>Regionserver</code> 等。<br>在主界面上点击选中的 <code>Regionserver</code> 就会展示对应的 <code>Web</code> 服务器，展示该服务器上所有区域的列表及其他基本的属性值。</p><h4 id="度量-Metric"><a href="#度量-Metric" class="headerlink" title="度量 Metric"></a>度量 <code>Metric</code></h4><p><code>Hadoop</code> 有一个<strong>度量 <code>Metric</code></strong> 系统，会在每隔一段时间获取系统重要组件的信息，并将其输出到上下文中。启用该系统可以将信息导出到 <code>JMX</code>，从而展示集群上正在和过去的请求视图。</p><h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p><code>Htable</code> 提供的 <strong><code>incrementColumnValue()</code></strong> 方法可以实现计数器每秒数千次的更新，解决之前计数器存储在 <code>MySQL</code> 中的更新频繁问题。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h3&gt;&lt;p&gt;&lt;code&gt;HBase&lt;/code&gt; 是一个在 &lt;code&gt;HDFS&lt;/code&gt; 上开发的&lt;strong&gt;面向列&lt;/strong&gt;的&lt;strong&gt;分布式数据库&lt;/strong&gt;，如果你需要实时访问超大规模的数据集，那么使用 &lt;code&gt;HBase&lt;/code&gt; 就对了。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;HBase&lt;/code&gt; &lt;strong&gt;自底而上&lt;/strong&gt;地进行构建，可以简单的通过&lt;strong&gt;增加节点来线性扩展&lt;/strong&gt;。其并不是关系型数据库，并且也不支持 &lt;code&gt;SQL&lt;/code&gt;，在特定的空间里，能够做 &lt;code&gt;RDBMS&lt;/code&gt; 不能做的事，即在廉价的硬件构成的集群上管理超大规模的稀疏表。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>to_2023-05-15</title>
    <link href="https://blog.vgbhfive.cn/to-2023-05-15/"/>
    <id>https://blog.vgbhfive.cn/to-2023-05-15/</id>
    <published>2023-05-15T13:02:52.000Z</published>
    <updated>2023-06-21T14:57:29.330Z</updated>
    
    <content type="html"><![CDATA[<p>今天是 <code>2023</code> 年 <code>5</code> 月 <code>15</code> 日，距离我第一次来北京工作的时间刚好跨过了整整三年，关于这个时间我也是在跟朋友的闲聊中才发现原来我已经来北京北漂三年了。</p><span id="more"></span><p>回想起三年前的自己，刚才学校毕业不到一年，工作在西安的一家表面上还看得过去的公司，挣扎在与前女友的回忆中，身体与记忆相互拉扯，不知道明天的路该怎么走，既不敢辞职，也不敢和朋友说，幸亏有大学的舍友在同一家公司上班，不高不低的续命到毕业的第二年。<br>领导在特别突然的一天询问我的工作规划和以后的安排，谈话中我感觉到了不妙的气氛，果不其然我要被安排长期驻场出差了，去原本就定好的北京，领导给了两天的时间让我收拾家里的事情，其实我明白这是领导给我的考虑时间，不同意那就离职，同意就去北京出差。<br>我回到自己组的房子里，思考了一晚，我说服了自己，我要跟之前的生活告别，那最好的方式就是去一个陌生的地方，开启一段陌生的生活。</p><p>两天后，在大学舍友的送别之下，我踏上了开往北京的动车，从西安开往北京，时间花费了 <code>5</code> 小时 <code>35</code> 分钟，全程我没有睡觉，没有玩手机，我就静静地看着窗外，我的内心告诉我新的生活在向我招手，而我也即将开始新的生活。<br>正如预测的那样，刚到北京的一个月住在公司租的房子里，开始跟同事了解驻场开发的所有相关内容，一个月后我开始自己独立租房子，买了锅碗瓢盆自己做饭。幸而遇到一个还算不错的组长，在工作完成之后，他还允许我自己看书学习；半夜熬夜切换演练后，第二天就给我一天假期，让我好好休息；其他项目组的朋友，我们会一起买水果，一起在水房边聊天边吃。合租的室友遇到特别好的两个饭搭子、扑克牌搭子、游玩搭子，那是一段特别开心的日子，有时候竟然可以聊到凌晨的两点钟，哈哈哈哈哈哈哈哈。<br>这些快乐的时光让我逐渐找回自己，每天都在学习新的技术、尝试做新的菜、交往新的朋友、幻想骑着自己买的摩托车，到现在不知不觉来到北京已经三年了，基本上都是快乐的日子，希望在未来的日子里可以多多挣钱，哈哈哈哈哈哈哈哈。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天是 &lt;code&gt;2023&lt;/code&gt; 年 &lt;code&gt;5&lt;/code&gt; 月 &lt;code&gt;15&lt;/code&gt; 日，距离我第一次来北京工作的时间刚好跨过了整整三年，关于这个时间我也是在跟朋友的闲聊中才发现原来我已经来北京北漂三年了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Myx&amp;Lyl" scheme="https://blog.vgbhfive.cn/tags/Myx-Lyl/"/>
    
  </entry>
  
  <entry>
    <title>2023 低生产力 PC 装机报告</title>
    <link href="https://blog.vgbhfive.cn/2023-%E4%BD%8E%E7%94%9F%E4%BA%A7%E5%8A%9B-PC-%E8%A3%85%E6%9C%BA%E6%8A%A5%E5%91%8A/"/>
    <id>https://blog.vgbhfive.cn/2023-%E4%BD%8E%E7%94%9F%E4%BA%A7%E5%8A%9B-PC-%E8%A3%85%E6%9C%BA%E6%8A%A5%E5%91%8A/</id>
    <published>2023-05-11T13:17:18.000Z</published>
    <updated>2023-06-14T14:03:40.400Z</updated>
    
    <content type="html"><![CDATA[<h3 id="配置列表"><a href="#配置列表" class="headerlink" title="配置列表"></a>配置列表</h3><p><code>CPU</code>：<code>Intel i5-12400</code> 散片<br>主板：微星 <code>MAG B660 MORTAR WIFI DDR4</code><br>内存：光威 天策系列 <code>16G * 2</code> 套条<br>固态：宏基掠夺者 <code>GM7000 PCIe4.0 NVMe</code><br>电源：长城 <code>650w</code> 金牌全模<br>散热：九州风神 玄冰 <code>400V5</code>（四热管）<br>机箱：先马 平头哥 <code>M2</code>（五风扇位，侧头玻璃）<br>系统：<code>Windows 10</code> 专业版</p><span id="more"></span><p>上面这台机器从购买组装完再到现在，已经有了半个月的时间，可以基本上满足类似 <code>DNF</code>、 原神、 <code>IDEA</code>、 <code>VS Code</code> 这类低生产、低游戏的日常 PC 机需求。</p><hr><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>没有显卡，使用核显，对于非游戏玩家来说完美适配。后续若想加显卡，电源、主板均可以完美适配。</li><li>整体配置中规中矩，也就意味着不出错、不完美。</li><li>主板和机箱支持多磁盘扩展，散热、颜值在线。</li><li>支持国产品牌，发挥自己的一份力。</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li>没有独显，对于中轻度游戏玩家不是很友好，游戏体验折扣较大。</li><li>没有光污染，对于常年不关机玩家有较好的体验。</li><li>如果没有后续扩展的需求，可以使用 <code>MATX-mini</code> 机箱进一步压缩占用面积。</li></ol><hr><h3 id="优化配置"><a href="#优化配置" class="headerlink" title="优化配置"></a>优化配置</h3><p>机箱：小喆优品 <code>C2P</code><br>散热：乔思伯<code> CR-1400</code><br>机箱风扇：利民 <code>C12C * 3</code></p><p><small>这里的优化配置倾向于压缩机箱占用面积，而不是减少花费的金额，毕竟一分钱一分货。</small></p><hr><h3 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h3><p>上面详细客观的描述了这台机器的情况和需要优化的地方，那下面就是吐槽的内容。<br>因为一直是 <code>JD</code> 的深度用户，基本上大大小小的电子产品都是在官方直营买的，但是我却在这次买主板的时候遇到了<strong>“大坑”</strong> 。店铺客服告诉我他们是官方客服，第一时间我还没有理解这个意思，然后我就问了一嘴，那是京东官方吗？（我想很多人在京东买电子产品都是对京东官方售后有保障才去的吧）<br>反转来了，客服说：他们不是！！！他们是官方、官方、官方客服，这是打着京东自营的招牌，而是主板官方旗舰店？？？</p><p>至于我为什么会跟客服聊这么多呢？是因为装机之后，板子的蓝牙时断时续，用一会就自己断开连接了，我重新连接再过一会就又会断开；自己尝试过各种各样的解决方案：刷 <code>BIOS</code>、抠 <code>BIOS</code> 电池、重装蓝牙驱动，自己无法解决，就去找了店铺客服。<br>此时我看到一行小字<strong>“180天只换不修”</strong> ，我就问客服啊，是真的只换不修吗？我这情况可以只换不修吗？客服说：行，那你寄回来吧。<br>我又是一通折腾，主板拆了下来，我还专门录像了，特别是重要的地方：<code>CPU</code> 接口、零配件、盖板都搞的好好的，自我感觉都可以二次销售了（录像是担心有扯皮的事情出现）。第二天官方收到板子，速度很快另一块板子就上路了，好家伙！这速度杠杠的。第三天早上快递就打电话喊我去取快递，不过当时在上班就喊舍友帮忙取了一下，晚上我回家又是一通折腾，装机、整线、重装系统，一直搞到凌晨一点多。<br>这其中更有意思的来了，因为我录了开箱视频，就发现了几个问题：新的主板外包装没有塑封，感觉像是维修后的；我在找说明书的时候，发现他的保修卡被人撕了一角，这让我更加觉得是维修后的主板。当时由于当时着急装机，我就没管这些，板子能用就好啊，其他的都是小事。那么更大的反转又来了！<br>就在我写这篇博客的今天，感觉之前买的内存条不是很好，想看看板子支不支持 <code>DDR5</code>，我记得之前问过客服，就去翻了之前的聊天记录，此时我发现了<strong>大 <code>BUG</code></strong> ，店铺官方客服说<strong>“180天只换不修”</strong> ，更换的是<strong>良品</strong>；而我去咨询京东官方客服，她们说<strong>“180天只换不修”</strong> ，更换的是<strong>新品</strong>。<br>你品品这其中的差距，一字之差，就是天翻地覆的差距！而我现在呢，被客服留了联系方式，说是 <code>24</code>小时内给我回电，<code>emmmm....</code> 怎么说呢？<br>我好像无话可说，反正我是基本告别 <code>JD</code> 了。</p><hr><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>今天是 <code>2023-06-15</code>，上述的吐槽经过 <code>12315</code> 没有任何解决方案，就是一直拖着。<br>无奈，我接受了解决方案：售后维修再加两百块的补偿。说实话真的很无奈，得不到任何的解释，也没有任何说明，反正就是拿钱办事，对此我只能说<strong>这事办的漂亮</strong>。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;配置列表&quot;&gt;&lt;a href=&quot;#配置列表&quot; class=&quot;headerlink&quot; title=&quot;配置列表&quot;&gt;&lt;/a&gt;配置列表&lt;/h3&gt;&lt;p&gt;&lt;code&gt;CPU&lt;/code&gt;：&lt;code&gt;Intel i5-12400&lt;/code&gt; 散片&lt;br&gt;主板：微星 &lt;code&gt;MAG B660 MORTAR WIFI DDR4&lt;/code&gt;&lt;br&gt;内存：光威 天策系列 &lt;code&gt;16G * 2&lt;/code&gt; 套条&lt;br&gt;固态：宏基掠夺者 &lt;code&gt;GM7000 PCIe4.0 NVMe&lt;/code&gt;&lt;br&gt;电源：长城 &lt;code&gt;650w&lt;/code&gt; 金牌全模&lt;br&gt;散热：九州风神 玄冰 &lt;code&gt;400V5&lt;/code&gt;（四热管）&lt;br&gt;机箱：先马 平头哥 &lt;code&gt;M2&lt;/code&gt;（五风扇位，侧头玻璃）&lt;br&gt;系统：&lt;code&gt;Windows 10&lt;/code&gt; 专业版&lt;/p&gt;</summary>
    
    
    
    
    <category term="Electron" scheme="https://blog.vgbhfive.cn/tags/Electron/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-Hive</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-Hive/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-Hive/</id>
    <published>2023-04-12T14:31:45.000Z</published>
    <updated>2023-05-28T04:36:06.235Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><strong><code>Hive</code></strong> 是一个构建在 <code>Hadoop</code> 之上的<strong>数据仓库框架</strong>，其设计目的在于让精通 <code>SQL</code> 但编程技能较弱的运营人员能够对存放在 <code>HDFS</code> 中的大规模数据集执行查询。<br>但是由于其底层依赖的 <code>Hadoop</code> 和 <code>HDFS</code> 设计本身约束和局限性，限制 <code>Hive</code> 不支持记录级别的更新、插入或者删除操作，不过可以通过查询生成新表或将查询结果导入文件中来实现。同时由于 <code>MapReduce</code> 任务的启动过程需要消耗较长的时间，所以查询延时比较严重。</p><span id="more"></span><img src="/Hadoop-Hive/mmexport1684155627591.jpg" class="" title="mmexport1684155627591"><p><code>Hive</code> 发行版本中包含 <strong><code>CLI</code></strong> 、 <strong><code>HWI</code></strong> （一个简单的网页界面）以及可通过 <strong><code>JDBC</code></strong> 、 <strong><code>ODBC</code></strong> 和一个 <strong><code>Thrift</code> 服务器</strong>进行编程访问的几个模块。<br>所有的命令和查询都会进入 <strong><code>Driver</code>（驱动模块）</strong>，通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行（通常是启动多个 <code>MapReduce</code> 任务 <code>job</code> 来执行）。当需要启动启动 <code>MapReduce</code> 任务（<code>job</code>）时，<code>Hive</code> 本身是不会生成 <code>MapReduce</code> 算法程序，相反 <code>Hive</code> 会通过一个 <code>XML</code> 文件的 <strong><code>job</code> 执行计划</strong>驱动执行内置的、原生的 <code>Mapper</code> 和 <code>Reduce</code> 模块，即这些通用的模块函数类似于微型的语言翻译程序，而驱动此翻译程序的就是 <code>XML</code> 文件。<br><code>Hive</code> 通过和 <strong><code>JobTracker</code> 通信来初始化 <code>MapReduce</code> 任务</strong>，而不必部署在 <code>JobTracker</code> 所在的管理节点上执行。<br><strong><code>Metastore</code>（元数据存储）</strong>是一个独立的关系型数据库，<code>Hive</code> 会在其中保存表模式和其他系统元数据。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p><code>Hive</code> 一般运行在工作站上，将 <code>SQL</code> 查询转换为一系列在 <code>Hadoop</code> 集群上运行的作业。<code>Hive</code> 把数据组织为表，通过这种方式为存储在 <code>HDFS</code> 上的数据结构赋予结构，元数据（表模式等）存储在 <code>metastore</code> 数据库中。</p><p><code>Hive</code> 的安装非常简单，首先必须安装相同版本的 <code>Hadoop</code>。 接下来下载相同版本的 <code>Hive</code>，然后把压缩包解压缩到合适的目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">tar xzf apache-hive-x.y.z-bin.tar.gz</span></span><br></pre></td></tr></table></figure><p>接下来就是将 <code>Hive</code> 加入到全局文件中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash"><span class="built_in">export</span> HIVE_HOME=~/apache-hive-x.y.z-bin</span></span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span></span><br></pre></td></tr></table></figure><p>最后就是启动 <code>Hive</code> 的 <code>shell</code> 环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">hive</span></span><br><span class="line"><span class="meta prompt_">hive&gt;</span></span><br></pre></td></tr></table></figure><h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><p>与 <code>Hadoop</code> 类似，<code>Hive</code> 用 <code>XML</code> 配置进行设置，配置文件为 <code>hive-site.xml</code>，位于在 <code>Hive</code> 的 <code>conf</code> 目录下。通过该文件可以设置每次运行 <code>Hive</code> 时希望使用的配置项，该目录下还包含 <code>hive-default.xml</code> （其中记录着 <code>Hive</code> 的选项及默认值）。</p><p><code>hive-site.xml</code> 文件最适合存放详细的集群连接信息，可以使用 <code>Hadoop</code> 属性 <code>fa.defaultFS</code> 和 <code>yarn.resourcemanager.address</code> 来指定文件系统和资源管理器。默认值为本地文件系统和本地作业运行器（<code>job runner</code>）。</p><p>传递 <code>--config</code> 选项参数给 <code>hive</code> 命令，可以通过这种方式重新定义 <code>Hive</code> 查找 <code>hive-site.xml</code> 文件的目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">hive --config /Users/home/hive-conf</span></span><br></pre></td></tr></table></figure><p>传递 <code>-hiveconf</code> 选项来为单个会话（<code>pre-session</code>）设置属性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">hive -hiveconf fs.defaultFS=hdfs://localhost -hiveconf mapper.framework.name=yarn</span></span><br></pre></td></tr></table></figure><p>还可以在一个会话中使用 <code>SET</code> 命令更改设置，这对于某个特定的查询修改 <code>Hive</code> 设置非常有用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">SET mapper.framework.name=yarn</span></span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">SET mapper.framework.name</span></span><br><span class="line">mapper.framework.name=yarn</span><br></pre></td></tr></table></figure><p>设置属性有一个优先级层次，越小的值表示优先级越高：</p><ul><li><strong><code>Hive SET</code> 命令</strong> 。</li><li><strong>命令行 <code>-hiveconf</code> 选项</strong> 。</li><li><strong><code>hive-site.xml</code> 和 <code>Hadoop</code> 站点文件</strong>（<code>core-site.xml</code>、<code>hdfs-site.xml</code>、<code>mapper-site.xml</code>、<code>yarn-site.xml</code>）。</li><li><strong><code>Hive</code> 默认值和 <code>Hadoop</code> 默认文件</strong>（<code>core-default.xml</code>、<code>hadfs-default.xml</code>、<code>mapper-default.xml</code>、<code>yarn-default.xml</code>）。</li></ul><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><h5 id="用户接口"><a href="#用户接口" class="headerlink" title="用户接口"></a>用户接口</h5><ol><li><p><strong><code>CLI</code></strong><br><code>Shell</code> 终端命令行（<code>Command Line Interface</code>），交互形式使用 <code>Hive</code> 命令行与 <code>Hive</code> 进行交互。</p></li><li><p><strong><code>JDBC/ODBC</code></strong><br><code>Hive</code> 基于 <code>JDBC</code> 操作提供的客户端，用户可以通过连接来访问 <code>Hive Server</code> 服务。</p></li><li><p><strong><code>Web UI</code></strong><br>通过浏览器访问 <code>Hive</code>。</p></li></ol><h5 id="Thrift-Server"><a href="#Thrift-Server" class="headerlink" title="Thrift Server"></a><code>Thrift Server</code></h5><p>轻量级、跨语言的远程服务调用框架，<code>Hive</code> 集成该服务便于不同的编程语言调用 <code>Hive</code> 的接口。</p><h5 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a><code>Metastore</code></h5><p><code>Metastore</code> 是 <code>Hive</code> 元数据的集中存放地，通常包含两部分：服务和元数据的存储。元数据包含：<strong>表的名字</strong>、<strong>表的列和分区及属性</strong>、<strong>表的属性（内部表和外部表）</strong>、<strong>表数据所在目录</strong>。默认情况下，<code>metastore</code> 服务和 <code>Hive</code> 服务运行在同一个 <code>JVM</code> 中，包含一个内嵌的以本地磁盘作为存储的 <code>Derby</code> 数据库实例，被称为内嵌 <code>metastore</code> 配置（<code>embedded metastore configuration</code>）。</p><p>如果要支持多会话（以及多用户），需要使用一个独立的数据库。这是因为 <code>MetaStore</code> 通常存储在其自带的 <code>Derby</code> 数据库中，缺点是跟随 <code>Hive</code> 部署，数据目录不固定，且不支持多用户操作。另外现在支持外部 <code>MySQL</code> 与 <code>Hive</code> 交互用于存储元数据信息。<br>可以通过把 <code>hive.metastore.uris</code> 设为 <code>metastore</code> 服务器 <code>URI</code>（如果有多个服务器，各个 <code>URI</code> 之间使用逗号分隔），把 <code>Hive</code> 服务设为使用远程 <code>metastore</code>。<code>metastore</code> 服务器 <code>URI</code> 的形式为 <code>thrift://host:port</code>。</p><p><img src="/Hadoop-Hive/hadoop_metastore_1.jpg" alt="hadoop_metastore_1"></p><p>重要的 <code>metastore</code> 配置属性：</p><table><thead><tr><th>属性名称</th><th>类型 <div style="width: 70px"></th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td><code>hive.metastore.warehouse.dir</code></td><td><code>URI</code></td><td><code>/user/hive/warehouse</code></td><td>相当于 <code>fs.default.name</code> 的目录，托管表就存储在这里</td></tr><tr><td><code>hive.metastore.uris</code></td><td>逗号分隔的 <code>URI</code></td><td>未设定</td><td>如果未设置则使用当前的 <code>metastore</code>，否则连接到由 <code>URI</code> 列表指定要连接的远程 <code>metastore</code> 服务器。</td></tr><tr><td><code>javax.jddo.option.ConnectionURL</code></td><td><code>URI</code></td><td><code>jdbc:derby:;databaseName=metastoredb;create=true</code></td><td><code>metastore</code> 数据库的 <code>JDBC URL</code></td></tr><tr><td><code>javax.jddo.option.ConnectionDriveName</code></td><td>字符串</td><td><code>org.apache.derby.jdbc.EmbeddedDriver</code></td><td><code>JDBC</code> 驱动器的类名</td></tr><tr><td><code>javax.jddo.option.ConnectionUserName</code></td><td>字符串</td><td><code>APP</code></td><td><code>JDBC</code> 用户名</td></tr><tr><td><code>javax.jddo.option.ConnectionPassword</code></td><td>字符串</td><td><code>mine</code></td><td><code>JDBC</code> 密码</td></tr></tbody></table><hr><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><h4 id="计算引擎"><a href="#计算引擎" class="headerlink" title="计算引擎"></a>计算引擎</h4><p>目前 <code>Hive</code> 支持 <strong><code>MapReduce</code></strong> 、 <strong><code>Tez</code></strong> 和 <strong><code>Spark</code></strong> 三种计算引擎。</p><ol><li><p><strong><code>MapReduce</code></strong> 计算引擎<br>请参考之前的博客内容。</p></li><li><p><strong><code>Spark</code></strong> 计算引擎<br>请参考之前的博客内容。</p></li><li><p><strong><code>Tez</code></strong> 计算引擎<br><code>Apache Tez</code> 是进行大规模数据处理且支持 <code>DAG</code> 作业的计算框架，它直接源于 <code>MapReduce</code> 框架，除了能够支持 <code>MapReduce</code> 特性之外，还支持新的作业形式，并允许不同类型的作业能够在一个集群中运行。</p><p> <code>Tez</code> 将原有的 <code>Map</code> 和 <code>Reduce</code> 两个操作简化为一个新的概念 **<code>Vertex</code>**，并将原有的计算处理节点拆分成多个组成部分： <strong><code>Vertex Input</code></strong> 、 <strong><code>Vertex Output</code></strong> 、 <strong><code>Sorting</code></strong>  <strong><code>Shuffling</code></strong> 和 <strong><code>Merging</code></strong> 。计算节点之间的数据通信被统称为 <strong><code>Edge</code></strong> ，这些分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的 <strong><code>DAG</code></strong> 作业。<br> 通过允许 <code>Apache Hive</code> 运行更加复杂的 <code>DAG</code> 任务，之前需要多个 <code>MR jobs</code>，但现在运行一个 <code>Tez</code> 任务中。</p> <img src="/Hadoop-Hive/hadoop_mapreduce-tez.jpg" class="" title="hadoop_mapreduce-tez"><p> <code>Tez</code> 和 <code>MapReduce</code> 作业的比较：</p><ul><li><code>Tez</code> 绕过 <code>MapReduce</code> 很多不必要的中间的数据存储和读取的过程，直接在一个作业中表达了 <code>MapReduce</code> 需要多个作业共同协作才能完成的事情。</li><li><code>Tez</code> 和 <code>MapReduce</code> 一样都使用 <code>YARN</code> 作为资源调度和管理。但与 <code>MapReduce on YARN</code> 不同，<code>Tez on YARN</code> 并不是将作业提交到 <code>ResourceManager</code>，而是提交到 <code>AMPoolServer</code> 的服务上，<code>AMPoolServer</code> 存放着若干个已经预先启动的 <code>ApplicationMaster</code> 服务。</li><li>当用户提交一个 <code>Tez</code> 作业上来后，<code>AMPoolServer</code> 从中选择一个 <code>ApplicationMaster</code> 用于管理用户提交上来的作业，这样既可以节省 <code>ResourceManager</code> 创建 <code>ApplicationMaster</code> 的时间，而又能够重用每个 <code>ApplicationMaster</code> 的资源，节省了资源释放和创建时间。</li></ul><p> <code>Tez</code> 相比于 <code>MapReduce</code> 有以下几点重大改进：</p><ul><li>当查询需要有多个 <code>Reduce</code> 逻辑时，<code>Hive</code> 的 <code>MapReduce</code> 引擎会将计划分解，每个 <code>Redcue</code> 提交一个 <code>MapReduce</code> 作业。这个链中的所有 <code>MR</code> 作业都需要逐个调度，每个作业都必须从 <code>HDFS</code> 中重新读取上一个作业的输出并重新洗牌。而在 <code>Tez</code> 任务中，几个 <code>Reduce</code> 接收器可以直接连接，数据可以流水线传输，而不需要临时 <code>HDFS</code> 文件，这种模式称为 <strong><code>MRR（Map-reduce-reduce）</code></strong> 。</li><li><code>Tez</code> 还允许一次发送整个查询计划，实现应用程序动态规划，从而使框架能够更智能地分配资源，并通过各个阶段流水线传输数据。对于更复杂的查询来说，这是一个巨大的改进，因为它消除了 <code>IO/sync</code> 障碍和各个阶段之间的调度开销。</li><li>在 <code>MapReduce</code> 计算引擎中，无论数据大小，在洗牌阶段都以相同的方式执行，将数据序列化到磁盘，再由下游的程序去拉取，并反序列化。<code>Tez</code> 可以允许小数据集完全在内存中处理，而 <code>MapReduce</code> 中没有这样的优化。仓库查询经常需要在处理完大量的数据后对小型数据集进行排序或聚合，因此 <code>Tez</code> 的优化也能极大地提升效率。</li></ul></li></ol><h4 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a>存储格式</h4><p><code>Hive</code> 支持的存储数的格式主要有： <strong><code>TEXTFILE</code>（行式存储）</strong>、 <strong><code>SEQUENCEFILE</code>（行式存储）</strong>、 <strong><code>ORC</code>（列式存储）</strong>、 <strong><code>PARQUET</code>（列式存储）</strong>。</p><p>行存储的特点： 查询满足条件的<strong>一整行数据</strong>的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><p>列存储的特点： 因为每个字段的数据聚集存储，在查询只需要<strong>少数几个字段</strong>的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><h5 id="TEXTFILE"><a href="#TEXTFILE" class="headerlink" title="TEXTFILE"></a><code>TEXTFILE</code></h5><p>默认格式，数据不做压缩，磁盘开销大，数据解析也开销大。可结合 <code>Gzip</code>、<code>Bzip2</code> 使用（系统自动检查，执行查询时自动解压），但使用这种方式，<code>Hive</code> 就不会对数据进行切分，从而无法对数据进行并行操作。</p><h5 id="ORC-格式"><a href="#ORC-格式" class="headerlink" title="ORC 格式"></a><code>ORC</code> 格式</h5><p><code>ORC (Optimized Row Columnar)</code> 是 <code>Hive 0.11</code> 引入的新的存储格式。其可以看到每个 <code>ORC</code> 文件由 <code>1</code> 个或多个 <code>Stripe</code> 组成，每个 <code>stripe</code> 为 <code>250MB</code> 大小。<br><small><code>Stripe</code> 实际相当于 <code>RowGroup</code> 概念，不过大小由 <code>4MB-&gt;250MB</code>，这样能提升顺序读的吞吐率。</small><br>每个 <code>Stripe</code> 里有三部分组成，分别是：</p><ul><li><code>Index Data</code><br> 一个轻量级的 <code>index</code>，默认是每隔 <code>1W</code> 行做一个索引。这里做的索引只是记录某行的各字段在 <code>Row Data</code> 中的 <code>offset</code> 偏移量。</li><li><code>RowData</code><br> 存储的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 <code>Stream</code> 来存储。</li><li><code>Stripe Footer</code><br> 存的是各个stripe的元数据信息。每个文件有一个 <code>File Footer</code>，这里面存的是每个 <code>Stripe</code> 的行数，每个<code>Column</code> 的数据类型信息等；每个文件的尾部是一个 <code>PostScript</code>，记录了整个文件的压缩类型以及 <code>FileFooter</code> 的长度信息等。<br> 在读取文件时，会 <code>seek</code> 到文件尾部读 <code>PostScript</code>，从里面解析到 <code>File Footer</code> 长度，再读 <code>FileFooter</code>，从里面解析到各个 <code>Stripe</code> 信息，再读各个 <code>Stripe</code>，即从后往前读。</li></ul><h5 id="PARQUET-格式"><a href="#PARQUET-格式" class="headerlink" title="PARQUET 格式"></a><code>PARQUET</code> 格式</h5><p><code>Parquet</code> 是面向分析型业务的列式存储格式，以二进制方式存储的，因此是不可以直接读取的，文件中包括该文件的数据和元数据，所以 <code>Parquet</code> 格式文件是自解析的。</p><p>通常情况下，在存储 <code>Parquet</code> 数据的时候会按照 <code>Block</code> 大小设置行组的大小，由于一般情况下每一个 <code>Mapper</code> 任务处理数据的最小单位是一个 <code>Block</code>，这样可以把每一个行组由一个 <code>Mapper</code> 任务处理，增大任务执行并行度。</p><p>该 <code>Parquet</code> 文件的内容中：一个文件中可以存储多个行组，文件的首位都是该文件的 <code>Magic Code</code>，用于校验它是否是一个 <code>Parquet</code>文件；<code>Footer length</code> 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量；文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 <code>Schema</code> 信息；除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据。</p><p>在Parquet中，有三种类型的页：</p><ul><li><strong>数据页</strong><br>  数据页用于存储当前行组中该列的值。</li><li><strong>字典页</strong><br>  字典页存储该列值的编码字典，每一个列块中最多包含一个字典页。</li><li><strong>索引页</strong><br>  索引页用来存储当前行组下该列的索引，目前 <code>Parquet</code> 中还不支持索引页。</li></ul><h4 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h4><p>在 <code>Hive</code> 中处理数据，一般都需要经过压缩，通过压缩来节省网络带宽。</p><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>对应的编解码器</th></tr></thead><tbody><tr><td><code>DEFAULT</code></td><td>无</td><td><code>DEFAULT</code></td><td><code>.deflate</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.DefaultCodec</code></td></tr><tr><td><code>Gzip</code></td><td><code>gzip</code></td><td><code>DEFAULT</code></td><td><code>.gz</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.GzipCodec</code></td></tr><tr><td><code>bzip2</code></td><td><code>bzip2</code></td><td><code>bzip2</code></td><td><code>.bz2</code></td><td>是</td><td><code>org.apache.hadoop.io.compress.BZip2Codec</code></td></tr><tr><td><code>LZO</code></td><td><code>lzop</code></td><td><code>LZO</code></td><td><code>.lzo</code></td><td>否</td><td><code>com.hadoop.compression.lzo.LzopCodec</code></td></tr><tr><td><code>LZ4</code></td><td>无</td><td><code>LZ4</code></td><td><code>.lz4</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.Lz4Codec</code></td></tr><tr><td><code>Snappy</code></td><td>无</td><td><code>Snappy</code></td><td><code>.snappy</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.SnappyCodec</code></td></tr></tbody></table><h4 id="底层执行原理"><a href="#底层执行原理" class="headerlink" title="底层执行原理"></a>底层执行原理</h4><img src="/Hadoop-Hive/mmexport1684155646915.jpg" class="" title="mmexport1684155646915"><h5 id="Driver-运行器"><a href="#Driver-运行器" class="headerlink" title="Driver 运行器"></a><code>Driver</code> 运行器</h5><p><code>Driver</code> 组件完成 <code>HQL</code> 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行计划的生成。生成的逻辑执行计划存储在 <code>HDFS</code> 中，并随后由 <code>MapReduce</code> 调用执行。<br><code>Hive</code> 的核心是驱动引擎， 驱动引擎由四部分组成：</p><ul><li><strong>解释器</strong>：解释器的作用是将 <code>HiveSQL</code> 语句转换为抽象语法树（<code>AST</code>）。</li><li><strong>编译器</strong>：编译器是将语法树编译为逻辑执行计划。</li><li><strong>优化器</strong>：优化器是对逻辑执行计划进行优化。</li><li><strong>执行器</strong>：执行器是调用底层的运行框架执行逻辑执行计划。</li></ul><h5 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h5><p><code>HiveQL</code> 通过命令行或者客户端提交，经过 <code>Compiler</code> 编译器，运用 <code>MetaStore</code> 中的元数据进行类型检测和语法分析，生成一个逻辑方案(<code>Logical Plan</code>)，然后通过的优化处理，产生一个 <code>MapReduce</code> 任务。</p><img src="/Hadoop-Hive/hadoop_hive_driver.jpg" class="" title="hadoop_hive_driver"><h4 id="与传统数据库比较"><a href="#与传统数据库比较" class="headerlink" title="与传统数据库比较"></a>与传统数据库比较</h4><p><code>Hive</code> 在很多方面与传统的数据库类似，但由于需要支持 <code>MapReduce</code> 和 <code>HDFS</code> 就意味着其体系结构有别于传统数据库，而这些区别又影响着 <code>Hive</code> 所支持的特性。</p><h5 id="读时模式和写时模式"><a href="#读时模式和写时模式" class="headerlink" title="读时模式和写时模式"></a>读时模式和写时模式</h5><p>在加载时发现数据不符合模式，则拒绝加载数据，因为数据是在写入数据库时对照模式进行检查，因此这一设计又被称为<strong>“写时模式”</strong> 。而 <code>Hive</code> 对数据的验证并不在加载数据时进行，而是在查询时进行，被称为<strong>“读时模式”</strong> 。</p><p><strong>读时模式</strong>不需要读取数据来进行“解析”，再进行序列化并以数据库内部格式存入磁盘，因此可以使数据加载非常迅速。<strong>写时模式</strong>可以对列进行索引，并对数据进行压缩，但这也会导致加载数据会额外耗时，由此有利于提升查询性能。</p><h5 id="更新、事务和索引"><a href="#更新、事务和索引" class="headerlink" title="更新、事务和索引"></a>更新、事务和索引</h5><p>更新、索引和事务这些是传统数据库最要的特性，但 <code>Hive</code> 并不支持这些，因为需要支持 <code>MapReduce</code> 操作 <code>HDFS</code> 数据，因此 <strong>“全表扫描”</strong> 是常态化操作，而表更新则是将数据变换后放入新表实现。</p><ol><li><p>更新<br><code>HDFS</code> 不提供就地文件更新，因此插入、更新、删除等一系列引起数据变化的操作都会被保存在一个较小的增量文件中，由 <code>metastore</code> 在后台运行的 <code>MapReduce</code> 任务定期将这些增量文件合并到基表文件中。<br><small>上述功能的支持必须启用事务，以保证对表进行读取操作时可以看到表的一致性快照。</small></p></li><li><p>锁<br><code>Hive</code> 引入了表级（<code>table-level</code>）和分区级（<code>partition-level</code>）的锁，因此可以防止一个进程删除正在被另一个进程读取的表。该锁由 <code>ZooKeeper</code> 透明管理，因此用户不必执行获取和释放锁的操作，但可以通过 <code>SHOW LOCKS</code> 语句获取已经获得了哪些锁的信息。默认情况下，未启用锁的功能。</p></li><li><p>索引<br><code>Hive</code> 的索引目前被分为两类：<strong>紧凑索引（<code>compact index</code>）</strong>和<strong>位图索引（<code>bitmap index</code>）</strong>。<br>紧凑索引存储每个值的 <code>HDFS</code> 块号，而不是存储文件内偏移量，因此存储不会占用过多的磁盘空间，并且对于值被聚簇（<code>clustered</code>）存储于相近行的情况，索引仍然能够有效。<br>位图索引使用压缩的位集合（<code>bitset</code>）来高效存储某个特殊值的行，而这种索引一般适用于较少取值的列（例如性别和国家）。</p></li></ol><h5 id="其他-SQL-on-hadoop"><a href="#其他-SQL-on-hadoop" class="headerlink" title="其他 SQL-on-hadoop"></a>其他 <code>SQL-on-hadoop</code></h5><p>针对 <code>Hive</code> 的局限性，也有其他的 <code>SQL-on-Hadoop</code> 技术出现，那么 <strong><code>Cloudera Impala</code></strong> 就是其中的佼佼者，他是开源交互式 <code>SQL</code> 引擎，<code>Impala</code> 在性能上要强于 <code>MapReduce</code> 的 <code>Hive</code> 高一个数量级。<br><code>Impala</code> 使用专门的守护进程，这些守护进程运行在集群中的每个数据节点上，当客户端发起查询时，会首先联系任意一个运行了 <code>Impala</code> 守护进程的节点，该节点会被当作该查询的协调（<code>coordination</code>）节点。协调节点向集群中的其他 <code>Impala</code> 守护进程分发工作，并收集结果以形成该查询的完整结果集。<code>Impala</code> 使用 <code>Hive</code> 的 <code>Metastore</code> 并支持 <code>Hive</code> 格式和绝大多数的 <code>HiveQL</code> 结构，因此在实际中这两者可以直观地相互移植，或者运行在同一个集群上。</p><p>当然也有 <code>Hortonworks</code> 的 <strong><code>Stinger</code></strong> 计划支持 <code>Tez</code> 作为执行引擎，再加上矢量化查询引擎等其他改进技术，使 <code>Hive</code> 在性能上得到很大的提升。</p><p><strong><code>Apache phoenix</code></strong> 则采取了另一种完全不同的方式，提供基于 <code>HBase</code> 的 <code>SQL</code>，通过 <code>JDBC</code> 驱动实现 <code>SQL</code> 访问，<code>JDBC</code> 驱动将查询转换为 <code>HBase</code> 扫描，并利用 <code>HBase</code> 协同处理器来执行服务器端的聚合，当然数据也存储在 <code>HBase</code> 中。 </p><hr> <h3 id="HiveQL"><a href="#HiveQL" class="headerlink" title="HiveQL"></a><code>HiveQL</code></h3><p><code>Hive</code> 的 <code>SQL</code> 被称为 <code>HiveQL</code>，是 <code>SQL-92</code>、 <code>MySQL</code>、 <code>Oracle SQL</code> 的混合体。其概要比较如下：</p><table><thead><tr><th>特性</th><th><code>SQL</code></th><th><code>HiveQL</code></th></tr></thead><tbody><tr><td>更新</td><td><code>UPDATE</code>、 <code>INSERT</code>、 <code>DELETE</code></td><td><code>UPDATE</code>、 <code>INSERT</code>、 <code>DELETE</code></td></tr><tr><td>事务</td><td>支持</td><td>有限支持</td></tr><tr><td>索引</td><td>支持</td><td>支持</td></tr><tr><td>延迟</td><td>亚秒级</td><td>分钟级</td></tr><tr><td>数据类型</td><td>整数、浮点数、定点数、文本和二进制串、时间</td><td>布尔型、整数、浮点数、文本和二进制串、时间戳、数组、映射、结构</td></tr><tr><td>函数</td><td>数百个内置函数</td><td>数百个内置函数</td></tr><tr><td>多表插入</td><td>不支持</td><td>支持</td></tr><tr><td><code>CREATE TABLE AS SELECT</code></td><td><code>SQL-92</code> 中不支持，但有些数据库支持</td><td>支持</td></tr><tr><td><code>SELECT</code></td><td><code>SQL-92</code></td><td><code>SQL-92</code>。支持偏序的 <code>SORT BY</code>，可限制返回数量的 <code>LIMIT</code></td></tr><tr><td>连接</td><td><code>SQL-92</code> 支持或变相支持</td><td>内连接、外连接、半连接、映射连接、交叉连接</td></tr><tr><td>子查询</td><td>在任何子句中支持的或不相关的</td><td>只能在 <code>FROM</code>、 <code>WHERE</code> 或 <code>HAVING</code> 子句中</td></tr><tr><td>视图</td><td>可更新</td><td>只读</td></tr><tr><td>扩展点</td><td>用户定义函数、存储过程</td><td>用户定义函数、<code>MapReduce</code> 脚本</td></tr></tbody></table><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p><code>Hive</code> 支持原子和复杂数据类型。原子数据类型包括<strong>数值型</strong>、<strong>布尔型</strong>、<strong>字符串类型</strong>和<strong>时间戳类型</strong>。复杂数据类型包括<strong>数组</strong>、<strong>映射</strong>和<strong>结构</strong>。</p><p><code>Hive</code> 提供了普通 <code>SQL</code> 操作符，包括：<strong>关系操作符</strong>、<strong>算术操作符</strong>和<strong>逻辑操作符</strong>。</p><h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><p>用户自定义函数（<code>UDF</code>）是一个允许用户通过扩展 <code>HiveQL</code> 的强大功能，用户通过 <code>Java</code> 编写自己的 <code>UDF</code>，在将自定义函数加入用户会话后，就可以跟内置函数一样使用。<br><code>Hive</code> 提供了多种类型的用户自定义函数，每一种都会针对输入数据执行特定的转换过程，具体类型包含以下三种：</p><ul><li><strong><code>UDF (User Defined Function)</code></strong> ：一进一出。传入一个值，逻辑运算后返回一个值，如内置函数的 <code>floor</code>、<code>round</code> 等。</li><li><strong><code>UDAF (User Defined Aggregation Funtion)</code></strong> ：多进一出。传入多行数据，根据选定的值 <code>group by</code> 后返回一行结果，类似 <code>sum</code>、<code>count</code>。</li><li><strong><code>UDTF (User Defined Table Generating Functions)</code></strong> ：一进多出。基于特定的一行值输入，返回展开多行输出，类似内置函数 <code>explode</code>。</li></ul><p>创建自定义函数步骤如下：</p><ul><li><strong>编写自定义函数</strong><br>  引入依赖  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><strong>编译部署</strong><br>  自定义函数代码编写完成后，编译打包为 <code>jar</code> 文件。<br>  部署 <code>jar</code> 文件要根据部署模式进行调整，本地模式则是将 <code>jar</code> 文件采用本地模式部署，而非本地模式则是将 <code>jar</code> 文件放置到共享存储（<code>HHDFS</code>）上。  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地模式</span></span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">ADD jar hive-jar-test-1.0.0.jar</span></span><br><span class="line">Added hive-jar-test-1.0.0.jar to class path</span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">list jars;</span></span><br><span class="line">hive-jar-test-1.0.0.jar</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">非本地模式</span></span><br><span class="line">hadoop fs -put hive-jar-test-1.0.0.jar /usr/home/hive-jar-test-1.0.0.jar</span><br></pre></td></tr></table></figure></li><li><strong><code>Hive</code> 中注册函数</strong><br>  注册函数也被分为两种：<strong>临时函数</strong>和<strong>永久函数</strong>，临时注册函数用于解决一些临时特殊的业务需求开发的函数，<code>Hive</code> 注册的临时函数只在当前会话中可用，注册函数时需要使用 <code>temporary</code> 关键字声明。注册函数时未使用临时关键字 <code>temporary</code> 的都为永久函数，在所有会话中都可用。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE [temporary] FUNCTION [dbname.]function_name AS class_name [USING jar | file | archive &#x27;file_url&#x27;]</span><br></pre></td></tr></table></figure></li><li><strong>使用自定义函数</strong><br>  函数全名使用 <code>dbname.function_name</code> 表示，使用的时候可以直接用函数全名，但查询如果在当前库下操作，则使用函数名即可。  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> id, test_udf_lower(content) <span class="keyword">FROM</span> test_table_1 LIMIT <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><strong>销毁自定义函数</strong>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP [temporary] FUNCTION [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure></li></ul><h5 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a><code>UDF</code></h5><p>编写 <code>UDF</code>，需要继承 <code>org.apache.hadoop.hive.ql.exec.UDF</code> 并重写 <code>evaluate</code> 函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestUDF</span> <span class="keyword">extends</span> <span class="title class_">UDF</span> &#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里接受参数的类型必须是 Hadoop 支持的输入输出类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Test <span class="title function_">evaluate</span><span class="params">(<span class="keyword">final</span> Test x)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Text</span>(x.toString()).toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><small><code>UDF</code> 必须有返回类型，可以返回 <code>null</code>，但返回类型不能为 <code>void</code>。</small></p><h5 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a><code>UDAF</code></h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestUDAF</span> <span class="keyword">extends</span> <span class="title class_">AbstractGenericUDAFResolver</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> GenericUDAFEvaluator <span class="title function_">getEvaluator</span><span class="params">(TypeInfo[] parameters)</span></span><br><span class="line">            <span class="keyword">throws</span> SemanticException &#123;</span><br><span class="line">        <span class="keyword">if</span> (parameters.length != <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(parameters.length - <span class="number">1</span>, <span class="string">&quot;Exactly one argument is expected.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">ObjectInspector</span> <span class="variable">oi</span> <span class="operator">=</span> TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">if</span> (oi.getCategory() != ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(<span class="number">0</span>, <span class="string">&quot;Argument must be PRIMITIVE, but &quot;</span> + oi.getCategory().name() + <span class="string">&quot; was passed.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">PrimitiveObjectInspector</span> <span class="variable">inputOI</span> <span class="operator">=</span> (PrimitiveObjectInspector) oi;</span><br><span class="line">        <span class="keyword">if</span> (inputOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(<span class="number">0</span>, <span class="string">&quot;Argument must be String, but &quot;</span> + inputOI.getPrimitiveCategory().name() + <span class="string">&quot; was passed.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TotalNumOfLettersEvaluator</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TotalNumOfLettersEvaluator</span> <span class="keyword">extends</span> <span class="title class_">GenericUDAFEvaluator</span> &#123;</span><br><span class="line">        PrimitiveObjectInspector inputOI;</span><br><span class="line">        ObjectInspector outputOI;</span><br><span class="line">        PrimitiveObjectInspector integerOI;</span><br><span class="line">        <span class="type">int</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> ObjectInspector <span class="title function_">init</span><span class="params">(Mode m, ObjectInspector[] parameters)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="keyword">assert</span>(parameters.length == <span class="number">1</span>);</span><br><span class="line">            <span class="built_in">super</span>.init(m, parameters);</span><br><span class="line">           </span><br><span class="line">            <span class="comment">//map阶段读取sql列，输入为String基础数据格式</span></span><br><span class="line">            <span class="keyword">if</span> (m == Mode.PARTIAL1 || m == Mode.COMPLETE) &#123;</span><br><span class="line">                inputOI = (PrimitiveObjectInspector) parameters[<span class="number">0</span>];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//其余阶段，输入为Integer基础数据格式</span></span><br><span class="line">            integerOI = (PrimitiveObjectInspector) parameters[<span class="number">0</span>];</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">            <span class="comment">// 指定各个阶段输出数据格式都为Integer类型</span></span><br><span class="line">            outputOI = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class, ObjectInspectorOptions.JAVA);</span><br><span class="line">            <span class="keyword">return</span> outputOI;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 存储当前字符总数的类</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">LetterSumAgg</span> <span class="keyword">implements</span> <span class="title class_">AggregationBuffer</span> &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> num)</span>&#123;</span><br><span class="line">            sum += num;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> AggregationBuffer <span class="title function_">getNewAggregationBuffer</span><span class="params">()</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="type">LetterSumAgg</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LetterSumAgg</span>();</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reset</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">        <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LetterSumAgg</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">warned</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">iterate</span><span class="params">(AggregationBuffer agg, Object[] parameters)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="keyword">assert</span> (parameters.length == <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (parameters[<span class="number">0</span>] != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">                <span class="type">Object</span> <span class="variable">p1</span> <span class="operator">=</span> ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[<span class="number">0</span>]);</span><br><span class="line">                myagg.add(String.valueOf(p1).length());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Object <span class="title function_">terminatePartial</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">            total += myagg.sum;</span><br><span class="line">            <span class="keyword">return</span> total;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">merge</span><span class="params">(AggregationBuffer agg, Object partial)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="keyword">if</span> (partial != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="type">LetterSumAgg</span> <span class="variable">myagg1</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">                <span class="type">Integer</span> <span class="variable">partialSum</span> <span class="operator">=</span> (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class="line">                <span class="type">LetterSumAgg</span> <span class="variable">myagg2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LetterSumAgg</span>();</span><br><span class="line">                </span><br><span class="line">                myagg2.add(partialSum);</span><br><span class="line">                myagg1.add(myagg2.sum);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Object <span class="title function_">terminate</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">            total = myagg.sum;</span><br><span class="line">            <span class="keyword">return</span> myagg.sum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数执行过程：</p><ul><li><code>PARTIAL1</code>：从原始数据到部分聚合数据的过程，会调用 <code>iterate()</code> 和 <code>terminatePartial()</code> 方法。<code>iterate()</code> 函数负责解析输入数据，而 <code>terminatePartial()</code> 负责输出当前临时聚合结果。该阶段可以理解为对应 <code>MapReduce</code> 过程中的 <code>Map</code> 阶段。</li><li><code>PARTIAL2</code>：从部分聚合数据到部分聚合数据的过程（多次聚合），会调用 <code>merge()</code> 和 <code>terminatePartial()</code> 方法。<code>merge()</code> 函数负责聚合 <code>Map</code> 阶段 <code>terminatePartial()</code> 函数输出的部分聚合结果，<code>terminatePartial()</code> 负责输出当前临时聚合结果。阶段可以理解为对应 <code>MapReduce</code> 过程中的 <code>Combine</code> 阶段。</li><li><code>FINAL</code>: 从部分聚合数据到全部聚合数据的过程，会调用 <code>merge()</code> 和 <code>terminate()</code> 方法。<code>merge()</code> 函数负责聚合 <code>Map</code> 阶段或者 <code>Combine</code> 阶段 <code>terminatePartial()</code> 函数输出的部分聚合结果。<code>terminate()</code> 方法负责输出 <code>Reduce</code> 阶段最终的聚合结果。该阶段可以理解为对应 <code>MapReduce</code> 过程中的 <code>Reduce</code> 阶段。</li><li><code>COMPLETE</code>: 从原始数据直接到全部聚合数据的过程，会调用 <code>iterate()</code> 和 <code>terminate()</code> 方法。可以理解为 <code>MapReduce</code> 过程中的直接 <code>Map</code> 输出阶段，没有 <code>Reduce</code> 阶段。</li></ul><p><small>还有另外一种实现方式是继承 <code>org.apache.hadoop.hive.ql.exec.UDAF</code>，并且包含一个或多个嵌套的、实现了 <code>org.apache.hadoop.hive.ql.UDAFEvaluator</code> 的静态类。</small></p><hr> <h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><h4 id="表"><a href="#表" class="headerlink" title="表"></a>表</h4><p><code>Hive</code> 的表在逻辑上由存储的数据和描述表中数据形式的相关元数据组成。数据一般存放在 <code>HDFS</code> 中，但它也可以放在其他任何 <code>Hadoop</code> 文件系统中，包括本地文件系统或 <code>S3</code>。<code>Hive</code> 把元数据存放在关系型数据库中，而不是放在 <code>HDFS</code> 中。</p><h5 id="托管表和外部表"><a href="#托管表和外部表" class="headerlink" title="托管表和外部表"></a>托管表和外部表</h5><p>在 <code>Hive</code> 中创建表时，默认情况下由 <code>Hive</code> 负责管理数据，这也就意味着要将数据移入仓库目录，被称为<strong>内部表</strong>。而另外一种则是<strong>外部表</strong>，数据存放在仓库目录以外的地方。</p><p>这两种表的区别在于 <code>LOAD</code> 和 <code>DROP</code> 命令的语义上：</p><ul><li><code>LOAD</code><br>  托管表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_1(content STRING);</span><br><span class="line">LOAD DATA INPATH <span class="string">&#x27;/usr/home/data.txt&#x27;</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> test_table_1;</span><br></pre></td></tr></table></figure>  加载数据到托管表时，会将数据文件移入到仓库目录下。<br>  外部表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> test_table_2 (content STRING) LOCATION <span class="string">&#x27;/usr/home/data.txt&#x27;</span>;</span><br><span class="line">LOAD DATA INPATH <span class="string">&#x27;/usr/home/data.txt&#x27;</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> test_table_2;</span><br></pre></td></tr></table></figure></li><li><code>DROP</code><br>  托管表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> test_table_1;</span><br></pre></td></tr></table></figure>  执行上述操作，会将其元数据和数据一起被删除，这也就是 <strong>“托管数据”</strong> 的含义。<br>  外部表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> test_table_2;</span><br></pre></td></tr></table></figure>  删除外部表并不会删除数据，仅会删除元数据。</li></ul><p>那么如何选择并使用这两种类型的表呢？<br>有一个经验法则就是，如果需要所有的处理都由 <code>Hive</code> 完成，那么应该使用托管表。如果要使用其他的工具来处理数据集则使用外部表。</p><h5 id="分区和桶"><a href="#分区和桶" class="headerlink" title="分区和桶"></a>分区和桶</h5><p><code>Hive</code> 将表组织成<strong>分区（<code>partition</code>）</strong>，这是一种根据<strong>分区列（<code>partition column</code>）</strong>的值对表进行粗略划分的机制。使用分区可以加快数据分片（<code>slice</code>）的查询速度。</p><p>表或分区可以进一步分为<strong>桶（<code>bucket</code>）</strong>，会为数据提供额外的结构以获得更高效的查询处理。</p><ol><li><p>分区<br>一个表可以通过多个维度来进行分区。分区是在创建表的时候用 <code>PARTITIONED BY</code> 子句定义的，该子句需要定义列的列表。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_3 (ts <span class="type">INT</span>, line STRING)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (dt STRING, country STRING);</span><br></pre></td></tr></table></figure><p>而在文件系统级别，分区只是表目录下嵌套的子目录，将更多的文件加载到表目录之后，目录结构如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/usr/home/warehouse/test_table_3</span><br><span class="line">dt=2023-05-16</span><br><span class="line">country=CN</span><br><span class="line">file1</span><br><span class="line">file2</span><br><span class="line">country=EU</span><br><span class="line">file5</span><br><span class="line">file6</span><br><span class="line">dt=2023-05-17</span><br><span class="line">country=CN</span><br><span class="line">file3</span><br><span class="line">file4</span><br></pre></td></tr></table></figure><p>之后使用 <code>SHOW PARTITIONS</code> 命令显示表中的分区列表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">SHOW</span> PARTITIONS test_table_3</span><br><span class="line">dt<span class="operator">=</span><span class="number">2023</span><span class="number">-05</span><span class="number">-16</span><span class="operator">/</span>country<span class="operator">=</span>CN</span><br><span class="line">dt<span class="operator">=</span><span class="number">2023</span><span class="number">-05</span><span class="number">-16</span><span class="operator">/</span>country<span class="operator">=</span>EU</span><br><span class="line">dt<span class="operator">=</span><span class="number">2023</span><span class="number">-05</span><span class="number">-17</span><span class="operator">/</span>country<span class="operator">=</span>CN</span><br></pre></td></tr></table></figure><p><small><code>PARTITIONED BY</code> 子句中的列定义是表中正式的列，称为<strong>分区列（<code>partition column</code>）</strong>，但数据中并不包含这些列的值，而是源于目录名。</small><br>在日常的查询中以通常的方式使用分区列，<code>Hive</code> 会对输入进行修剪，从而只扫描相关分区。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ts <span class="keyword">FROM</span> test_table_3 <span class="keyword">where</span> country <span class="operator">=</span> <span class="string">&#x27;CN&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>桶<br>将表（或分区）组织成<strong>桶（<code>bucket</code>）</strong>有以下优点：</p><ul><li>获得更高的查询处理效率。桶为表增加了额外的结构，在处理查询时能够利用这个结构，具体为连接两个在相同列上划分了桶的表，可以使用 <code>map</code> 端连接高效地实现。</li><li>使取样更加高效。在处理大规模数据集时，能使用数据的一部分进行查询，会带来很多方便。</li></ul><p> <code>Hive</code> 使用 <code>CLUSTERED BY</code> 子句来指定划分桶所用的列和要划分的桶的个数：<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_4 (ts <span class="type">INT</span>, line STRING)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (id) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS;</span><br></pre></td></tr></table></figure></p><p> 在实际物理存储上，每个桶就是表（或分区）目录里的一个文件，其文件名并不重要，但桶 <code>n</code> 是按照字典序排列的第 <code>n</code> 个文件。事实上桶对应于 <code>MapReduce</code> 的输出文件分区：一个作业产生的桶和 <code>reduce</code> 任务个数相同。<br> 可以通过查看刚才创建的 <code>bucketed_users</code> 表的布局来了解这一情况：<br> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">dfs -<span class="built_in">ls</span> /usr/home/warehouse/bucketed_users;</span></span><br><span class="line">000000_0</span><br><span class="line">000001_0</span><br><span class="line">000002_0</span><br><span class="line">000003_0</span><br></pre></td></tr></table></figure></p><p> 使用 <code>TABLESAMPLE</code> 子句对表进行取样，可以获得相同的结果，该子句会查询限定在表的一部分桶内，而不是整个表：<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> bucketed_users <span class="keyword">TABLESAMPLE</span>(BUCKET <span class="number">1</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">4</span> <span class="keyword">ON</span> id);</span><br><span class="line"><span class="number">4</span> Ann</span><br><span class="line"><span class="number">0</span> Nat</span><br><span class="line"><span class="number">2</span> Joe</span><br></pre></td></tr></table></figure></p></li></ol><h5 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h5><p>前面已经使用 <code>LOAD DATA</code> 操作，通过把文件复制或移动到表的目录中，从而把数据导入到 <code>Hive</code> 的表（或分区）。也可以使用 <code>INSERT</code> 语句把数据从一个 <code>Hive</code> 表填充到另一个，或在新建表时使用 <strong><code>CTAS</code> （<code>CREATE TABLE ... AS SELECT</code>）</strong>结构。</p><ol><li><p><code>INSERT</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> target [<span class="keyword">PARTITION</span>(dt)] <span class="keyword">SELECT</span> col1, col2 <span class="keyword">FROM</span> source;</span><br></pre></td></tr></table></figure><p><code>OVERWRITE</code> 关键字意味着目标表（分区）中的内容会被替换掉。</p></li><li><p>多表插入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> source</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> table_by_year <span class="keyword">SELECT</span> <span class="keyword">year</span>, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">table</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">year</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> table_by_month <span class="keyword">SELECT</span> <span class="keyword">month</span>, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">table</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span></span><br></pre></td></tr></table></figure><p>这种<strong>多表插入</strong>比使用单独的 <code>INSERT</code> 效率更高，因为只需要扫描一遍源表就可以生成多个不相交的输出。</p></li><li><p><code>CTAS</code> 语句<br>将 <code>Hive</code> 查询的结果输出到一个新的表内是非常方便的，新表的列的定义是从 <code>SELECT</code> 子句所检索的列导出的。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> target</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> col1, col2 <span class="keyword">FROM</span> source;</span><br></pre></td></tr></table></figure></li></ol><h5 id="表的修改和删除"><a href="#表的修改和删除" class="headerlink" title="表的修改和删除"></a>表的修改和删除</h5><p>由于 <code>Hive</code> 使用<em>读时模式（<code>schema on read</code>）</em>，所以表在创建之后可以非常灵活地支持对表定义的修改。使用 <code>ALTER TABLE</code> 语句来重命名表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> source RENAME <span class="keyword">TO</span> target;</span><br></pre></td></tr></table></figure><p><small>此命令除更新元数据外，还会将表目录移动到对应的目录下。</small><br>另外也支持修改列的定义，添加新的列，甚至用一组新的列替换表内已有的列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> target <span class="keyword">ADD</span> COLUMNS (col3 STRING);</span><br></pre></td></tr></table></figure><p><code>DROP TABLE</code> 语句用于删除表的数据和元数据。如果是外部表则只删除元数据，数据不会受到影响。<br>但若是需要仅删除表内的数据，保留表的定义，则需要使用 <code>TRUNCATE TABLE</code> 语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> target;</span><br></pre></td></tr></table></figure><h4 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h4><h5 id="排序和聚集"><a href="#排序和聚集" class="headerlink" title="排序和聚集"></a>排序和聚集</h5><p><code>Hive</code> 中可以使用标准的 <code>ORDER BY</code> 子句对数据进行排序， <strong><code>ORDER BY</code></strong> 将对输入执行并行全排序。</p><p>但是在很多情况下，并不需要对结果全局排序，那么可以使用 <code>Hive</code> 的非标准的扩展 <strong><code>SORT BY</code></strong> ，<code>SORT BY</code> 为每一个 <code>reducer</code> 文件产生一个排序文件。<br><strong><code>DISTRIBUTE BY</code></strong> 子句可以控制某个特定列到 <code>reducer</code>，通常是为了后续的聚集操作。如果 <code>SORT BY</code> 和 <code>DISTRIBUTE BY</code> 中所使用的列相同，可以缩写为 <strong><code>CLUSTER BY</code></strong> 以便同时指定两者所用的列。</p><h5 id="MapReduce-脚本"><a href="#MapReduce-脚本" class="headerlink" title="MapReduce 脚本"></a><code>MapReduce</code> 脚本</h5><p>与 <code>Hadoop Streaming</code> 类似，<code>TRANSFORM</code>、 <code>MAP</code> 和 <code>REDUCE</code> 子句可以在 <code>Hive</code> 中调用外部脚本或程序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">(year, temp, q) = line.strip().split()</span><br><span class="line"><span class="keyword">if</span> (temp != <span class="string">&quot;9999&quot;</span> <span class="keyword">and</span> re.<span class="keyword">match</span>(<span class="string">&quot;[01459]&quot;</span>, q)):</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;%s\t%s&quot;</span> % (year, temp)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">ADD FILE /usr/home/ ./is_year.py</span></span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">FROM record SELECT TRANSFORM(year, temp, name) USING <span class="string">&#x27;is_year.py&#x27;</span> AS year, temp;</span></span><br><span class="line">1950 0</span><br><span class="line">1950 22</span><br><span class="line">1949 111</span><br></pre></td></tr></table></figure><h5 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h5><p>子查询是内嵌在另一个 <code>SQL</code> 语句中的 <code>SELECT</code> 语句。<code>Hive</code> 对子查询的支持很有限，他只允许子查询出现在 <code>SELECT</code> 语句的 <code>FROM</code> 子句中 ，或者某些情况下的 <code>WHERE</code> 子句中。</p><h5 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h5><p>视图是一种用 <code>SELECT</code> 语句定义的<strong>虚表（<code>virtual table</code>）</strong>。视图可以用来以一种不同于磁盘实际存储的形式把数据呈现给用户，视图也可以用来限制用户，使其只能访问被授权的可以看到的子表。</p><p><code>Hive</code> 创建视图时并不把视图物化存储在磁盘上，相反视图的 <code>SELECT</code> 语句只是在执行引用视图的语句时才执行。<code>SHOW TABLES</code> 命令的输出结果里包含视图。还可以使用 <code>DESCRIBE EXTENDED view_name</code> 命令来查看某个视图的详细信息，包括用于定义它的那个查询。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> valid_table</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> col1, col3 <span class="keyword">FROM</span> target <span class="keyword">WHERE</span> <span class="keyword">year</span> <span class="operator">!=</span> <span class="string">&#x27;2022&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="EXPLAIN"><a href="#EXPLAIN" class="headerlink" title="EXPLAIN"></a><code>EXPLAIN</code></h5><p>在查询语句之前加上 <code>EXPLAIN</code> 关键字，就可以了解到查询计划和其他的信息来更加直观的展示 <code>Hive</code> 是如何将查询任务转化为 <code>MapReduce</code> 任务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">EXPLAIN SELECT <span class="built_in">sum</span>(col1) FROM target;</span></span><br></pre></td></tr></table></figure><p>首先会输出抽象语法树。其表明 <code>Hive</code> 是如何将查询解析为 <code>token</code>（符号）和 <code>literal</code>（字面值）的，是将查询转化到最终结果的一部分。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ABSTRACT SYNTAX TREE:</span><br><span class="line">(TOK_QUERY</span><br><span class="line">    (TOK_FROM (TOK_TABREF (TOK_TABNAME target)))</span><br><span class="line">    (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))</span><br><span class="line">    (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION sum (TOK_TABLE_OR_COL number))))))</span><br></pre></td></tr></table></figure><p>接下来可以看到列明 <code>col1</code>、 表明 <code>target</code> 还有 <code>sum</code> 函数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">    Stage-1 is a root stage</span><br><span class="line">    Stage-0 is a root stage</span><br></pre></td></tr></table></figure><p><small>一个 <code>Hive</code> 任务会包含一个或多个 <code>stage</code> 阶段，不同的 <code>stage</code> 阶段间会存在着依赖关系。一个 <code>stage</code> 可以是一个 <code>MapReduce</code> 任务，也可以是一个抽样阶段，或者一个合并阶段，还可以是一个 <code>limit</code> 阶段，以及 <code>Hive</code> 需要的其他任务的一个阶段。</small><br><small>默认情况下，<code>Hive</code> 一次只执行一个 <code>stage</code>（阶段）。</small></p><p>除此之外还可以使用 <code>EXPLAIN EXTENDED</code> 语句产生更多的输出信息。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;Hive&lt;/code&gt;&lt;/strong&gt; 是一个构建在 &lt;code&gt;Hadoop&lt;/code&gt; 之上的&lt;strong&gt;数据仓库框架&lt;/strong&gt;，其设计目的在于让精通 &lt;code&gt;SQL&lt;/code&gt; 但编程技能较弱的运营人员能够对存放在 &lt;code&gt;HDFS&lt;/code&gt; 中的大规模数据集执行查询。&lt;br&gt;但是由于其底层依赖的 &lt;code&gt;Hadoop&lt;/code&gt; 和 &lt;code&gt;HDFS&lt;/code&gt; 设计本身约束和局限性，限制 &lt;code&gt;Hive&lt;/code&gt; 不支持记录级别的更新、插入或者删除操作，不过可以通过查询生成新表或将查询结果导入文件中来实现。同时由于 &lt;code&gt;MapReduce&lt;/code&gt; 任务的启动过程需要消耗较长的时间，所以查询延时比较严重。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-Could not find first log file name in binary log index file</title>
    <link href="https://blog.vgbhfive.cn/MySQL-Could-not-find-first-log-file-name-in-binary-log-index-file/"/>
    <id>https://blog.vgbhfive.cn/MySQL-Could-not-find-first-log-file-name-in-binary-log-index-file/</id>
    <published>2023-04-02T12:32:07.000Z</published>
    <updated>2023-04-02T12:41:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>在之前的博客中说明过，我负责的业务有数据同步的需求，是从 <code>MySQL</code> 实时同步数据到 <code>ClickHouse</code>，为此我们使用了一个工具 <code>clickhouse-mysql-data-reader</code>，该工具的底层是通过监听 <code>MySQL</code> 的 <code>bin log</code> 来实现实时同步数据。</p><p>就在今早，数据同步不知为何停止了，当发现问题重新拉起同步任务时，就发现同步脚本出现了异常：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Could not find first log file name in binary log index file</span><br></pre></td></tr></table></figure><span id="more"></span><hr><h3 id="思考问题"><a href="#思考问题" class="headerlink" title="思考问题"></a>思考问题</h3><p>在看过异常报错信息之后，其大致是因为同步脚本停止了同步任务，之后就没有更新本地的 <code>bin log</code> 索引，此时等待我再拉起同步任务时，同步脚本使用本地未修改的 <code>bin log</code> 索引去 <code>MySQL</code> 拉取数据时，<code>MySQL</code> 的 <code>bin log</code> 索引经过业务数据的写入已经覆盖了之前的索引，同步脚本没有找到对应的索引就抛出异常。</p><hr><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>那么就下来的问题就是如何查找正确的 <code>bin log</code> 索引，然后修改同步脚本的 <code>bin log</code> 索引即可恢复同步数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show binary logs;</span><br></pre></td></tr></table></figure><p>上述命令可以帮助你查看到 <code>MySQL</code> 最新的 <code>bin log</code> 索引，之后同步修改脚本的 <code>bin log</code> 索引即可。</p><p><small>那么关于两次索引之间的数据如何同步呢？可以通过添加 <code>where case</code> 采用 <code>select</code> 的形式同步数据，更多的操作可以参考同步工具的使用文档。</small></p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;复现&quot;&gt;&lt;a href=&quot;#复现&quot; class=&quot;headerlink&quot; title=&quot;复现&quot;&gt;&lt;/a&gt;复现&lt;/h3&gt;&lt;p&gt;在之前的博客中说明过，我负责的业务有数据同步的需求，是从 &lt;code&gt;MySQL&lt;/code&gt; 实时同步数据到 &lt;code&gt;ClickHouse&lt;/code&gt;，为此我们使用了一个工具 &lt;code&gt;clickhouse-mysql-data-reader&lt;/code&gt;，该工具的底层是通过监听 &lt;code&gt;MySQL&lt;/code&gt; 的 &lt;code&gt;bin log&lt;/code&gt; 来实现实时同步数据。&lt;/p&gt;
&lt;p&gt;就在今早，数据同步不知为何停止了，当发现问题重新拉起同步任务时，就发现同步脚本出现了异常：&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Could not find first log file name in binary log index file&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="https://blog.vgbhfive.cn/tags/MySQL/"/>
    
    <category term="ClickHouse" scheme="https://blog.vgbhfive.cn/tags/ClickHouse/"/>
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-Spark</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-Spark/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-Spark/</id>
    <published>2023-03-12T03:06:30.000Z</published>
    <updated>2023-04-18T14:06:05.388Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p><code>Spark</code> 是用于<strong>处理大数据的集群计算框架</strong> ，与其他大多数数据处理框架不同之处在于 <code>Spark</code> 没有以 <code>MapReduce</code> 作为执行引擎，而是使用它自己的<strong>分布式运行环境</strong>在集群上执行工作。另外 <code>Spark</code> 与 <code>Hadoop</code> 又紧密集成，<code>Spark</code> 可以在 <code>YARN</code> 上运行，并支持 <code>Hadoop</code> 文件格式及其存储后端（例如 <code>HDFS</code>）。</p><p><code>Spark</code> 最突出的表现在于其能将 <strong>作业与作业之间的大规模的工作数据集存储在内存中</strong>。这种能力使得在性能上远超 <code>MapReduce</code> 好几个数量级，原因就在于 <code>MapReduce</code> 数据都是从磁盘上加载。根据 <code>Spark</code> 的处理模型有两类应用获益最大，分别是 <strong>迭代算法（即对一个数据集重复应用某个函数，直至满足退出条件）</strong>和 <strong>交互式分析（用户向数据集发出一系列专用的探索性查询）</strong> 。<br>另外 <code>Spark</code> 还因为其具有的 <strong><code>DAG</code> 引擎</strong>更具吸引力，原因在于 <code>DAG</code> 引擎可以处理任意操作流水线，并为用户将其转化为单个任务。</p><span id="more"></span><hr><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>从官方页面下载一个稳定版本的 <code>Spark</code> 二进制发行包（选择与当前使用 <code>Hadoop</code> 匹配的版本），然后在合适的位置解压文件包，并将 <code>Spark</code> 的解压路径添加到 <code>PATH</code> 中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tar -xzvf spark-x.y.z.tgz</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> PATH=~/spark-x.y.z/bin:<span class="variable">$PATH</span></span></span><br></pre></td></tr></table></figure><h4 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h4><p><code>Spark</code> 像 <code>MapReduce</code> 一样也有 <strong>作业（<code>job</code>）</strong>的概念，只不过 <code>Spark</code> 的作业相比 <code>MapReduce</code> 更加通用，因为 <code>Spark</code> 作业可以由任意的<strong>多阶段（<code>stages</code>）有向无环图（<code>DAG</code>）</strong>构成，其中每个阶段大致相当于 <code>MapReduce</code> 中的 <code>map</code> 阶段或 <code>reduce</code> 阶段。<br>这些阶段又被 <code>Spark</code> 运行环境分解为多个任务（<code>tash</code>），任务并行运行在分布式集群中的 <code>RDD</code> 分区上，类似于 <code>MapReduce</code> 中的任务。</p><p><code>Spark</code> 作业始终运行在应用（<code>application</code>）上下文中，它提供了 <code>RDD</code> 分组以及共享变量。一个应用可以串行或并行地运行多个作业，并为这些作业提供访问由同一应用的先前作业所缓存的 <code>RDD</code> 的机制。</p><h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a><code>RDD</code></h4><p><code>RDD</code> 又称<strong>弹性分布式数据集（<code>Resilient Distributed Dataset</code>，简称 <code>RDD</code>）</strong>是 <code>Spark</code> 最核心的概念，他是在<strong>集群中跨多个机器分区存储的一个只读的对象集合</strong> ，在 <code>Spark</code> 应用中首先会加载一个或多个 <code>RDD</code>，他们作为输入通过一系列转换得到一组目标 <code>RDD</code>，然后对这些目标 <code>RDD</code> 执行一个动作。<br><small>弹性分布式数据集中的“弹性”是指 <code>Spark</code> 可以通过重新安排计算来自动重建丢失的分区。</small></p><p><code>RDD</code> 是 <code>Spark</code> 最基本的抽象，<code>RDD</code> 关联着三个至关重要的属性：</p><ul><li>依赖关系。</li><li>分区（包括一些位置信息）</li><li>计算函数：<code>Partition =&gt; Iterator[T]</code></li></ul><p>这三大属性对于简单的 <code>RDD</code> 是不可或缺的，其他更高层的功能也都是基于这套模型构建的。首先，依赖关系列表会告诉 <code>Spark</code> 如何从必要的输入构建 <code>RDD</code>。其次，分区允许 <code>Spark</code> 将工作以分区为单位，分配到多个执行器上并行计算。最后，每个 <code>RDD</code> 都是一个计算函数 <code>compute</code>，可用于生成 <code>RDD</code> 所表示数据的 <code>Iterator[T]</code> 对象。</p><p>创建 <code>RDD</code> 共有三种方式：</p><ul><li>来自一个内存中的对象集合（也称为并行化一个集合）。适用于对少量的输入数据进行并行的 <code>CPU</code> 密集型计算。</li><li>使用外部存储器（例如 <code>HDFS</code>）中的数据集。创建一个外部数据集的引用。</li><li>对现有的 <code>RDD</code> 进行转换。</li></ul><h4 id="窄依赖和宽依赖"><a href="#窄依赖和宽依赖" class="headerlink" title="窄依赖和宽依赖"></a>窄依赖和宽依赖</h4><p>在对每一个 <code>RDD</code> 操作时，都会得到一个新的 <code>RDD</code>，那么前后的两个 <code>RDD</code> 就有了某种联系，即新的 <code>child RDD</code> 会依赖旧的 <code>parent RDD</code>。目前这些依赖关系被分为： <strong>窄依赖（<code>NarrowDependency</code>）</strong>和 <strong>宽依赖（<code>ShuffleDependency</code>）</strong>。</p><ol><li><p><strong>窄依赖</strong><br>官方解释为：<code>child RDD</code> 中的每个分区都依赖 <code>parent RDD</code> 中的一小部分分区。</p><p> <img src="https://s2.loli.net/2023/04/17/OyXb2IrHGMF3z5J.jpg" alt="hadoop_spark_10.jpg"></p><p> 上图包括了有关窄依赖的各种依赖情况：</p><ul><li><strong>一对一依赖</strong>：<code>child RDD</code> 中的每个分区都只依赖 <code>parent RDD</code> 中的一个分区，并且 <code>child RDD</code> 的分区数和 <code>parent RDD</code> 的分区数相同。属于这种依赖关系的转换算子有 <code>map()</code>、<code>flatMap()</code>、<code>filter()</code> 等。</li><li><strong>范围依赖</strong>：<code>child RDD</code> 和 <code>parent RDD</code> 的分区经过划分，每个范围内的父子 <code>RDD</code> 的分区都为一一对应的关系。属于这种依赖关系的转换算子有 <code>union()</code> 等。</li><li><strong>窄依赖</strong>：窄依赖可以理解为一对一依赖和范围依赖的组合使用。属于这种依赖关系的转换算子有 <code>join()</code>、<code>cartesian()</code>、<code>cogroup()</code> 等。</li></ul></li><li><p><strong>宽依赖</strong><br>宽依赖官方解释为需要两个 <code>shuffle</code> 的两个 <code>stage</code> 的依赖。</p><p> <img src="https://s2.loli.net/2023/04/17/BzWSR42FvDLtMql.jpg" alt="hadoop_spark_11.jpg"></p><p> <code>child RDD</code> 的一个分区依赖的是 <code>parent RDD</code> 中各个分区的某一部分，即 <code>child RDD</code> 的两个分区分别只依赖 <code>parent RDD</code> 中的部分，而计算出某个部分的过程，以及 <code>child RDD</code> 分别读取某个部分的过程（<code>shuffle write/shuffle read</code>），此过程正是 <code>shuffle</code> 开销所在。</p></li></ol><h4 id="转换和动作"><a href="#转换和动作" class="headerlink" title="转换和动作"></a>转换和动作</h4><p><code>Spark</code> 对 <code>RDD</code> 提供了两大操作：<strong>转换（<code>transformation</code>）</strong> 和 <strong>动作（<code>action</code>）</strong> 。转换是从现有 <code>RDD</code> 生成新的 <code>RDD</code>，而动作则触发对 <code>RDD</code> 的计算并对计算结果执行某种操作，要么返回给用户，要么保存到外部存储器中。</p><p>加载 <code>RDD</code> 或者执行转换并不会立即触发任何数据处理的操作，只不过是创建了一个计划，只有当对 <code>RDD</code> 执行某个动作时才会触发真正的计算。<br><small>如果想判断一个操作是转换还是动作，可以通过观察其返回类型：如果返回的类型是 <code>RDD</code>，那么他是一个转换否则就是一个动作。</small></p><p>在 <code>Spark</code>库中包含了丰富的操作，包含映射、分组、聚合、重新分区、采样、连接 <code>RDD</code> 以及把 <code>RDD</code> 作为集合来处理的各种转换，同时还包括将 <code>RDD</code> 物化为集合；对 <code>RDD</code> 进行统计数据的计算；从一个 <code>RDD</code> 中采样固定数量的元素；以及将 <code>RDD</code> 保存到外部存储器等各种动作。</p><h4 id="Lineage-机制"><a href="#Lineage-机制" class="headerlink" title="Lineage 机制"></a><code>Lineage</code> 机制</h4><p>相比其他系统的细颗粒度的内存数据更新级别的备份或者 <code>LOG</code> 机制，<code>RDD</code> 的 <strong><code>Lineage</code> 记录的是粗颗粒度的特定数据的 <code>Transformation</code> 操作</strong>（如 <code>filter</code>、<code>map</code>、<code>join</code> 等）行为。<br>当某个 <code>RDD</code> 的部分分区数据丢失时，它可以通过 <code>Lineage</code> 记录获取足够的信息来重新运算和恢复丢失的数据分区，该记录的内容就是前面提到的 <code>RDD</code> 之间的依赖关系。</p><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p><code>Spark</code> 可以在跨集群的内存中<strong>缓存数据</strong> ，也就意味着对数据集所做的任何计算都会非常快。相比较而言，<code>MapReduce</code> 在执行另一个计算时必须从磁盘重新加载输入数据集，即使他们可以使用中间数据集作为输入，但也无法摆脱始终从磁盘加载数据的事实，也必然影响其执行速度。<br><small>被缓存的 <code>RDD</code> 只能由同一应用的作业来读取。同理，应用终止时，作业所缓存的 <code>RDD</code> 都会被销毁，除非这些 <code>RDD</code> 已经被持久化保存，否则无法访问。</small></p><p>默认的持久化级别共分为两类： <strong><code>MEMORY_ONLY</code></strong> 是默认持久化级别，使用对象在内存中的常规表示方式； <strong><code>MEMORY_ONLY_SER</code></strong> 是一种更加紧凑的表示方法，通过把分区中的元素序列化为字节数组来实现的。<br><code>MEMORY_ONLY_SER</code> 相比 <code>MEMORY_ONLY</code> 多了一笔 <code>CPU</code> 的开销，但若是生成的序列化 <code>RDD</code> 分区的大小适合被保存在内存中，而默认的持久化方式无法做到，那就说明额外的开销是值得。另外 <code>MEMORY_ONLY_SER</code> 还可以减少垃圾回收的压力，因为每个 <code>RDD</code> 被存储为一个字节数组而不是大量的对象。</p><p>默认情况下，<code>RDD</code> 分区的序列化使用的是 <code>Kryo</code> 序列化方法，通过压缩序列化分区可以进一步节省空间，而这通常是更好的选择。<br><small>将 <code>spark.rdd.compress</code> 属性设置为 <code>true</code>，并且可选地设置 <code>spark.io.compression.codec</code> 属性。</small></p><h4 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h4><p>在使用 <code>Spark</code> 的序列化时，需要从两个方面来考虑：</p><ul><li><strong>数据序列化</strong></li><li><strong>函数序列化（闭包函数）</strong></li></ul><h5 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h5><p>数据序列化在默认情况下，<code>Spark</code> 在通过网络将数据从一个 <code>executor</code> 发送到另一个 <code>executor</code> 时，或者以序列化的形式缓存（持久化）数据时，所使用的都是 <code>Java</code> 序列化机制。<br>使用 <code>Kryo</code> 序列化机制对于大多数 <code>Spark</code> 应用都是更好的选择，<code>Kryo</code> 是一个高效的通用 <code>Java</code> 序列化库，要想使用 <code>Kryo</code> 序列化机制，需要在应用中的 <code>SparkConf</code> 中设置 <code>spark.serializer</code> 属性，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br></pre></td></tr></table></figure><p><strong><code>Kryo</code></strong> 不要求被序列化的类实现某个特性的的接口，因此如果旧的对象需要使用 <code>Kryo</code> 序列化也是可以的，在配置启用 <code>Kryo</code> 序列化之后就可以使用了，不过话虽如此，若是使用前可以在 <code>Kryo</code> 中对这些类进行注册，那么就可以提高其性能。这是因为 <code>Kryo</code> 需要写被序列化对象的类的引用，如果已经引用已经注册的类，那么引用标识就只是一个整数，否则就是完整的类名。<br>在 <code>Kryo</code> 注册类很简单，创建一个 <code>KryoRegistrator</code> 子类，重写 <code>registerClasses()</code> 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomKryoRegistrator</span> <span class="keyword">extends</span> <span class="title class_">KryoRegistrator</span> &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">registerClasses</span><span class="params">(Kryo kryo)</span> &#123;</span><br><span class="line">      kryo.register(Object.class);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后在 <code>driver</code> 应用中将 <code>spark.kryo.registrator</code> 属性设置为你的 <code>KryoRegistrator</code> 实现的完全限定类名：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;spark.kryo.registrator&quot;, &quot;CustomKryoRegistrator&quot;)</span><br></pre></td></tr></table></figure><h5 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h5><p>通常函数的序列化都会<strong>谨守本分</strong>，对于 <code>Spark</code> 来说，即使在本地模式下，也需要序列化函数，假若引入一个不可序列化的函数，那么应该在开发期间就应该发现。</p><h4 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h4><p><code>Spark</code> 应用可能会使用一些不属于 <code>RDD</code> 的数据，这些数据会被作为闭包函数的一部分被序列化后传递给下一个动作，这可以保证应用正常执行，但使用广播变量可以跟高效的完成相同的工作。</p><h5 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h5><p><strong>广播变量（<code>broadcast variable</code>）</strong>在经过序列化后被发送给各个 <code>executor</code>，然后缓存在那里，以便后期任务可以在需要时访问它。它与常规变量不同，常规变量是作为闭包函数的一部分被序列化的，因此他们在每个任务中都要通过网络被传输一次。<br>广播变量的作用类似于 <code>MapReduce</code> 中的分布式缓存，两者的不同之处在于 <code>Spark</code> 将数据保存到内存中，只有在内存耗尽时才会溢出到磁盘上。</p><h5 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h5><p><strong>累加器（<code>accumulator</code>）</strong>是在任务中只能对它做加法的共享变量，类似于 <code>MapReduce</code> 中的计数器。当作业完成后 <code>driver</code> 程序可以检索累加器的最终值。</p><hr><h3 id="作业运行机制"><a href="#作业运行机制" class="headerlink" title="作业运行机制"></a>作业运行机制</h3><p>在 <code>Spark</code> 作业的最高层，他有两个独立的实体： <strong><code>driver</code></strong> 和 <strong><code>executor</code></strong> 。<code>driver</code> 负责托管应用（<code>SparkContext</code>）并为作业调度任务。<code>executor</code> 专属于应用，他在应用运行期间运行，并执行该应用的任务。通常 <code>driver</code> 作为一个不由集群管理器（<code>cluster manager</code>）管理的客户端来运行，而 <code>executor</code> 则运行在集群的计算机上。 </p><h4 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h4><p>当对 <code>RDD</code> 执行一个动作时，会自动提交一个 <code>Spark</code> 作业。从内部看导致对 <code>SparkContext</code> 调用 <code>runJob()</code> 方法，然后将调用传递给作为 <code>driver</code> 的一部分运行的调度程序。调度程序由两部分组成： <strong><code>DAG</code> 调用程序</strong> 和 <strong>任务调度程序</strong> 。<code>DAG</code> 调度程序把作业分解为若干阶段，并由这些阶段组成一个 <code>DAG</code>。任务调度程序则负责把每个阶段中的任务提交到集群。</p><p><img src="https://s2.loli.net/2023/04/10/z1HoGR53NuTWdbI.jpg" alt="hadoop_spark_1.jpg"></p><h4 id="DAG-构建"><a href="#DAG-构建" class="headerlink" title="DAG 构建"></a><code>DAG</code> 构建</h4><p>要想了解一个作业如何被划分为阶段，首先需要了解在阶段中运行的任务的类型。有两种类型的任务： <strong><code>shuffle map</code> 任务</strong> 和 <strong><code>result</code> 任务</strong> ，从任务类型的名称可以看出 <code>Spark</code> 会怎样处理任务的输出。</p><ul><li><code>shuffle map</code> 任务。<br> 顾名思义 <code>shuffle map</code> 任务类似于 <code>MapReduce</code> 中 <code>shuffle</code> 的 <code>map</code> 端部分。每个 <code>shuffle map</code> 任务在一个 <code>RDD</code> 分区上运行计算，并根据分区函数把输出写入到一组新的分区中，以允许在后面的阶段中取用（后面的阶段可能由 <code>shuffle map</code> 任务组成，也可能由 <code>result</code> 任务组成），<code>shuffle map</code> 任务运行在除最终阶段之外的其他所有阶段中。</li><li><code>result</code> 任务。<br> <code>result</code> 任务运行在最终阶段，并将结果返回给用户程序。每个 <code>result</code> 任务在他自己的 <code>RDD</code> 分区上运行计算，然后把结果发送回 <code>driver</code>，再由 <code>driver</code> 将每个分区的计算结果汇集成最终结果。</li></ul><p>最简单的 <code>Spark</code> 作业不需要使用 <code>shuffle</code>，因此它只有一个由 <code>result</code> 任务构成阶段，就像是 <code>MapReduce</code> 中仅有 <code>map</code> 一样。而比较复杂的作业要涉及到分组操作，并且要求一个或多个 <code>shuffle</code> 阶段。</p><p>如果 <code>RDD</code> 已经被同一应用（<code>SparkContext</code>）中先前的作业持久化保存，那么 <code>DAG</code> 调度程序将会省掉一些任务，不会再创建一些阶段来重新计算（或者它的父 <code>RDD</code>）。</p><p><code>DAG</code> 调度程序负责将一个阶段分解为若干任务以提交给任务调度程序。另外 <code>DAG</code> 调度程序会为每个任务赋予一个位置偏好（<code>placement preference</code>），以允许任务调度程序充分利用数据本地化（<code>data locality</code>）。</p><h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><p>当任务集合被发送到任务调度程序后，任务调度程序用该应用运行的 <code>executor</code> 的列表，在斟酌位置偏好的同时构建任务到 <code>executor</code> 的映射。接着任务调度程序将任务分配给具有内核的 <code>executor</code>，并且在 <code>executor</code> 完成运行任务时继续分配更多的任务，直到任务集合全部完成。默认情况下，每个任务到分配一个内核，不过也可以通过设置 <code>spark.task.cpus</code> 来更改。<br><small>任务调度程序在为某个 <code>executor</code> 分配任务时，首先分配的是进程本地化（<code>process-local</code>）任务，再分配节点本地（<code>node-local</code>）任务，然后分配机架本地（<code>rack-local</code>）任务，最后分配任意（非本地）任务或者推测任务（<code>speculative task</code>）。</small></p><p>这些被分配的任务通过调度程序后端启动。调度程序后端向 <code>executor</code> 后端发送远程启动任务的消息，以告知 <code>executor</code> 开始运行任务。</p><p>当任务成功完成或者失败时，<code>executor</code> 都会向 <code>driver</code> 发送状态更新信息。如果失败任务调度程序将在另一个 <code>executor</code> 上重新提交任务。若是启用推测任务（默认情况下不启用），它还会为运行缓慢的任务启动推测任务。</p><h4 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h4><p><code>executor</code> 首先确保任务的 <code>JAR</code> 包和文件依赖关系都是最新的，<code>executor</code> 在本地高速缓存中保留了先前任务已使用的所有依赖，因此只有在更新的情况下才会重新下载。接下来由于任务代码是以启动任务消息的一部分而发送的序列化字节，因此需要反序列化任务代码（包括用户自己的函数）。最后执行任务代码，不过需要注意的是因为运行任务在于 <code>executor</code> 相同的 <code>JVM</code> 中，因此任务的启动没有进程开销。</p><p>任务可以向 <code>driver</code> 返回执行结果，这些执行结果被序列化并发送到 <code>executor</code> 后端，然后以状态更新消息的形式返回 <code>driver</code>。<code>shuffle map</code> 任务返回的是一些可以让下一个阶段检索其输出分区的消息，而 <code>result</code> 任务则返回其运行的分区的结果值，<code>driver</code> 将这些结果值收集起来，并把最终结果返回给用户的程序。</p><h4 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h4><p><code>Spark</code> 如何依靠 <code>executor</code> 来运行构成 <code>Spark</code> 作业的任务，负责管理 <code>executor</code> 生命周期的是集群管理器（<code>cluster manager</code>），同时 <code>Spark</code> 提供了多种具有不同特性的集群管理器：</p><ul><li>本地模式<br> 在使用本地模式时，有一个 <code>executor</code> 与 <code>driver</code> 运行在同一个 <code>JVM</code> 中。此模式对于测试或运行小规模作业非常有用。</li><li>独立模式<br> 独立模式的集群管理器是一个简单的分布式实现，它运行了一个 <code>master</code> 以及一个或多个 <code>worker</code>。当 <code>Spark</code> 应用启动时，<code>master</code> 要求 <code>worker</code> 代表应用生成多个 <code>executor</code> 进程，这种模式的主 <code>URL</code> 为 <code>spark://host:port</code>。</li><li><strong><code>Mesos</code> 模式</strong><br> <code>Apache Mesos</code> 是一个通用的集群资源管理器，它允许根据组织策略在不同的应用之间细化资源共享。默认情况下（细粒度模式）每个 <code>Spark</code> 任务被当作是一个 <code>Mesos</code> 任务运行，这样做可以更有效地使用集群资源，但是以额外的进程启动开销为代价。在粗粒度模式下 <code>executor</code> 在进程中运行任务，因此在 <code>Spark</code> 应用运行期间的集群资源由 <code>executor</code> 进程来掌管，这种模式的主 <code>URL</code> 为 <code>mesos://host:port</code>。</li><li><strong><code>YARN</code> 模式</strong><br> <code>YARN</code> 是 <code>Hadoop</code> 中使用的资源管理器，每个运行的 <code>Spark</code> 应用对应于一个 <code>YARN</code> 应用实例，每个 <code>executor</code> 在自己的 <code>YARN</code> 容器中运行，这种模式的主 <code>URL</code> 为 <code>yarn-client</code> 或 <code>yarn-cluster</code>。<br> <small><code>YARN</code> 是唯一一个能够与 <code>Hadoop</code> 的 <code>Kerberos</code> 安全机制集成的集群管理器。</small></li></ul><h5 id="运行在-YARN-上的-Spark"><a href="#运行在-YARN-上的-Spark" class="headerlink" title="运行在 YARN 上的 Spark"></a>运行在 <code>YARN</code> 上的 <code>Spark</code></h5><p>在 <code>YARN</code> 上运行 <code>Spark</code> 提供了与其他 <code>Hadoop</code> 组件最紧密的集成，为了在 <code>YARN</code> 上运行，<code>Spark</code> 提供了两种部署模式： <strong><code>YARN</code> 客户端模式</strong>和 <strong><code>YARN</code> 集群模式</strong>。<code>YARN</code> 客户端模式的 <code>driver</code> 在客户端运行，而 <code>YARN</code> 集群模式的 <code>driver</code> 在 <code>YARN</code> 的 <code>application master</code> 集群上运行。</p><p>对于具有任何交互式组件的程序都必须使用 <code>YARN</code> 客户端模式，在交互式组件上的任何调试都是立即可见的。<br>另一方面 <code>YARN</code> 集群模式适用于生成作业（<code>production job</code>），因为整个应用在集群上运行，这样更易于保留日志文件（包括来自 <code>driver</code> 的日志文件）以供稍后检查。如果 <code>application master</code> 出现故障，<code>YARN</code> 还可以尝试重新运行该应用。</p><ol><li><p><code>YARN</code> 客户端模式<br>在 <code>YARN</code> 客户端模式下，当 <code>driver</code> 构建新的 <code>SparkContext</code> 实例时就会启动与 <code>YARN</code> 之间的交互，该 <code>SparkContext</code> 向 <code>YARN</code> 资源管理器提交一个 <code>YARN</code> 应用，而 <code>YARN</code> 资源管理器则启动集群节点管理器上的 <code>YARN</code> 容器，并在其中运行一个名为 <code>SparkExecutorLauncher</code> 的 <code>application master</code>。该 <code>ExecutorLauncher</code> 的工作是启动 <code>YARN</code> 容器中的 <code>executor</code>，为了做到这一点 <code>ExecutorLauncher</code> 要向资源管理器请求资源，然后启动 <code>ExecutorBackend</code> 进程作为分配给它的容器。</p><p><img src="https://s2.loli.net/2023/04/10/A4U8BWLM21ix9a5.jpg" alt="hadoop_spark_2.jpg"></p><p>每个 <code>executor</code> 在启动时都会连接回 <code>SparkContext</code>，并注册自身。因此这就向 <code>SparkContext</code> 提供了关于可用于运行任务的 <code>executor</code> 的数量及其位置的信息，之后这些信息会被用在任务的位置偏好策路中。</p><p><code>YARN</code> 资源管理器的地址并没有在主 <code>URL</code> 中指定(这与使用独立模式或 <code>Mesos</code> 模式的集群管理器不同)，而是从 <code>HADOOP_CONF_DIR</code> 环境变量指定的目录中的 <code>Hadoop</code> 配置中选取。</p></li><li><p><code>YARN</code> 集群模式<br>在 <code>YARN</code> 集群模式下，用户的 <code>driver</code> 程序在 <code>YARN</code> 的 <code>application master</code> 进程中运行，<code>spark-submit</code> 客户端将会启动 <code>YARN</code> 应用，但是它不会运行任何用户代码。剩余过程与客户端模式相同，除了 <code>application master</code> 在为 <code>executor</code> 分配资源之前先启动 <code>driver</code> 程序外。</p><p><img src="https://s2.loli.net/2023/04/10/sMCpmWhGg5qeiV1.jpg" alt="hadoop_spark_3.jpg"></p></li></ol><p>在这两种 <code>YARN</code> 模式下，<code>executor</code> 都是在还没有任何本地数据位置信息之前先启动的，因此最终有可能会导致 <code>executor</code> 与存有作业所希望访同文件的 <code>datanode</code> 不在一起。而这些对于交互式会话是可以接受的，特别是因为会话开始之前可能开不知道需要访问哪些数据集。但是对于生成作业来说情况并非如此，所以 <code>Spark</code> 提供了一种方法，可以在 <code>YARN</code> 群集模式下运行时提供一些有关位置的提示，以提高数据本地性。</p><p><code>sparkContext</code> 构造函数可以使用第二个参数来传递一个优选位置，该优选位置是利用 <code>InputFormatInfo</code> 辅助类根据输人格式和路径计算得到的。因此当向资源管理器请求分配时 <code>application master</code> 需要用到这个优选位置。</p><hr><h3 id="数据结构化-DataFrame"><a href="#数据结构化-DataFrame" class="headerlink" title="数据结构化 DataFrame"></a>数据结构化 <code>DataFrame</code></h3><p><code>Spark</code> 的 <strong><code>DataFrame</code> 是结构化的、有格式的</strong> ，且支持一些特定的操作，就像分布式内存中的表那样，每列都有名字，有表结构定义，每列都有特定的数据类型：整数、字符串型、数组、映射表、实数、日期、时间戳等。另外 <code>DataFrame</code> 中的数据是不可变的，<code>Spark</code> 记录着所有转化操作的血缘关系。可以添加列或者改变已有列的名字和数据类型，这些操作都会创建新的 <code>DataFrame</code> ，原有的 <code>DataFrame</code> 则会保留。在 <code>DataFrame</code> 中，一列与其名字和对应的 <code>Spark</code> 数据类型都在表结构中定义。</p><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p>基本数据类型：</p><ul><li><code>ByteType</code></li><li><code>ShortType</code></li><li><code>IntegerType</code></li><li><code>LongType</code></li><li><code>FloatType</code></li><li><code>DoubleType</code></li><li><code>StringType</code></li><li><code>BooleanType</code></li><li><code>DecimalType</code></li></ul><p>复杂数据类型：</p><ul><li><code>BinaryType</code></li><li><code>TimestampType</code></li><li><code>DateType</code></li><li><code>ArrayType</code></li><li><code>MapType</code></li><li><code>StructType</code></li><li><code>StructField</code></li></ul><h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><p><code>Spark</code> 中的表结构为 <code>DataFrame</code> 定义了各列的名字和对应的数据类型。从外部数据源读取结构化数据时，表结构就会派上用场。相较于在读取数据时确定数据结构，提前定义表结构有如下优点：</p><ul><li>可以避免 <code>Spark</code> <strong>推断数据类型的额外开销</strong>。</li><li>可以防止 <code>Spark</code> 为决定表结构而单独创建一个作业来从数据文件读取很大一部分内容，对于较大的数据文件而言，其<strong>耗时相当长</strong> 。</li><li>可以<strong>尽早发现数据与表结构不匹配</strong> 。</li></ul><p>定义表结构有两种方式：</p><ul><li>编程的方式。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">schema = StructType([StructField(<span class="string">&quot;author&quot;</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">   StructField(<span class="string">&quot;title&quot;</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">   StructField(<span class="string">&quot;pages&quot;</span>, IntegerType(), <span class="literal">False</span>)])</span><br></pre></td></tr></table></figure></li><li>使用数据定义语言（<code>data definition language, DDL</code>）。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schema = &quot;author STRING, title STRING, pages INT&quot;</span><br></pre></td></tr></table></figure></li></ul><h4 id="行与列"><a href="#行与列" class="headerlink" title="行与列"></a>行与列</h4><p><code>DataFrame</code> 中的具名列与 <code>Pandas</code> 中的 <code>DataFrame</code> 对象的具名列，以及关系数据库表的列的概念上是类似的：描述的都是一种字段。<br><small><code>Spark</code> 的文档对列有 <code>col</code> 和 <code>Column</code> 两种表示。<code>Column</code> 是列对象的类名，而 <code>col()</code> 是标准的内建函数，返回一个 <code>Column</code> 对象。</small><br><code>DataFrame</code> 中的 <code>Column</code> 对象不能单独存在，在一条记录中，每一列都是行的一部分，所有的行共同组成整个 <code>DataFrame</code>。</p><p><code>Spark</code> 中的行是用 <code>Row</code> 对象来表示的，它包含一列或多列，各列既可以是相同的类型，也可以是不同的类型。由于 <code>Row</code> 是 <code>Spark</code> 中的对象，表示一系列字段的有序集合，因此可以在编程中很容易的实例化 <code>Row</code> 对象，并用自 <code>0</code> 开始的下标访问该对象的各字段。</p><h4 id="表与视图"><a href="#表与视图" class="headerlink" title="表与视图"></a>表与视图</h4><p>表存放数据，<code>Spark</code> 中的每张表都关联有相应的元数据，而这些元数据是表及其数据的一些信息，包括表结构、描述、表名、数据库名、列名、分区、实际数据所在的物理位置等，这些全都存放在中心化的元数据库中。<br><code>Spark</code> 没有专门的元数据库，<strong>默认使用 <code>Apache Hive</code> 的元数据库来保存表的所有数据</strong> ，仓库路径位于 <code>/user/hive/warehouse</code>。如果想要想要修改默认路径，可以修改 <code>Spark</code> 配置变量 <code>spark.sql.warehouse.dir</code> 为别的路径，这个路径既可以是本地路径，也可以是外部的分布式存储。</p><h5 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h5><p><code>Spark</code> 允许创建两种表：</p><ul><li><strong>有管理表</strong><br><code>Spark</code> 既管理元数据，也管理文件存储上的数据。这里的文件存储可以理解为本地文件系统或 <code>HDFS</code>，也可以是外部的对象存储系统。</li><li><strong>无管理表</strong><br><code>Spark</code> 只管理元数据，需要自行管理外部数据源中的数据。</li></ul><h5 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h5><ol><li><p>创建数据库和表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE demo_db;</span><br><span class="line">USE demo_db;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> demo_table_1 (<span class="type">date</span> STRING, delay <span class="type">INT</span>, distance <span class="type">INT</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> demo_table_2 (<span class="type">date</span> STRING, delay <span class="type">INT</span>, distance <span class="type">INT</span>) <span class="keyword">USING</span> csv OPTIONS (PATH <span class="string">&#x27;/data/learing/data.csv&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>新增视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> REPLACE TEMP <span class="keyword">VIEW</span> [global_temp.]test_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="type">date</span>, delay, distance <span class="keyword">FROM</span> demo_table_1 <span class="keyword">WHERE</span> distance <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure></li><li><p>缓存 <code>SQL</code> 表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CACHE [LAZY] <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span></span><br><span class="line">UNCACHE <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h4><h5 id="DataFrameReader"><a href="#DataFrameReader" class="headerlink" title="DataFrameReader"></a><code>DataFrameReader</code></h5><p><code>DataFrameReader</code> 是从数据源读取数据到 <code>DataFrame</code> 所用到的核心结构。用法有固定的格式和推荐的使用模式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrameReader.format(args).option(&quot;key&quot;, &quot;value&quot;).schema(args).load()</span><br></pre></td></tr></table></figure><p>这种将一串方法串联起来使用的模式在 <code>Spark</code> 中很常见，可读性也不错。</p><p>需要注意的是只能通过 <code>SparkSession</code> 实例访问 <code>DataFrameReader</code>，也就是说不能自行创建 <code>DataFrameReader</code> 实例，获取该实例的方法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.read // 返回 DataFrameReader 从静态数据源读取 DataFrame</span><br><span class="line">SparkSession.readStream // 返回的实例用于读取流式数据源</span><br></pre></td></tr></table></figure><p><code>DataFrameReader</code> 的公有方法如下：</p><table><thead><tr><th>方法</th><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>format()</code></td><td><code>&quot;parquet&quot;</code>、 <code>&quot;csv&quot;</code>、 <code>&quot;txt&quot;</code>、 <code>&quot;json&quot;</code>、 <code>&quot;orc&quot;</code>、 <code>&quot;avro&quot;</code></td><td>如果不指定方法的格式，则使用 <code>spark.sql.sources.default</code> 所指定的默认格式</td></tr><tr><td><code>option()</code></td><td><code>(&quot;mode&quot;, [PERMISSIVE FAILFAST DROPMALFORMAD])</code> 、 <code>(&quot;inferSchema&quot;, [true false])</code>、<code>(&quot;path&quot;, &quot;path_file_data_source&quot;)</code></td><td>一系列键值对，<code>Spark</code> 文档中解释了不同模式下的对应行为</td></tr><tr><td><code>schema()</code></td><td><code>DDL</code> 字符串或 <code>StructType</code> 对象</td><td>对于 <code>JSON</code> 或者 <code>CSV</code> 格式，可以使用 <code>option()</code> 方法自行推断表结构</td></tr><tr><td><code>load()</code></td><td><code>/path/source</code></td><td>要读取的数据源路径</td></tr></tbody></table><p><small>从静态的 <code>Parquet</code> 数据源读取数据不需要提供数据结构，因为 <code>Parquet</code> 文件的元数据通常包含表结构信息。不过对于流式数据源，表结构信息是需要提供的。</small></p><h5 id="DataFrameWriter"><a href="#DataFrameWriter" class="headerlink" title="DataFrameWriter"></a><code>DataFrameWriter</code></h5><p><code>DataFrameWriter</code> 是 <code>DataFrameReader</code> 的反面，将数据保存或写入特定的数据源。与 <code>DataFrameReader</code> 不同，<code>DataFrameWriter</code> 需要从保存的 <code>DataFrame</code> 获取，推荐的使用模式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataFrameWriter.format(args).option(args).bucketBy(args).partitionBy(args).save(path)</span><br><span class="line">DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)</span><br></pre></td></tr></table></figure><p><code>DataFrameWriter</code> 的公有方法如下：</p><table><thead><tr><th>方法</th><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>format()</code></td><td><code>&quot;parquet&quot;</code>、 <code>&quot;csv&quot;</code>、 <code>&quot;txt&quot;</code>、 <code>&quot;json&quot;</code>、 <code>&quot;orc&quot;</code>、 <code>&quot;avro&quot;</code></td><td>如果不指定方法的格式，则使用 <code>spark.sql.sources.default</code> 所指定的默认格式</td></tr><tr><td><code>option()</code></td><td><code>(&quot;mode&quot;, append [overwrite ignore error])</code>、 <code>(&quot;path&quot;, &quot;path_to_write_to&quot;)</code></td><td>一系列键值对，<code>Spark</code> 文档中解释了不同模式下的对应行为</td></tr><tr><td><code>bucketBy()</code></td><td><code>(numBuckets, col, ..., coln)</code></td><td>按桶写入时，指定桶数量和分桶所依据字段的名字列表</td></tr><tr><td><code>save()</code></td><td><code>&quot;/path/source&quot;</code></td><td>写入的路径</td></tr><tr><td><code>saveAsTable()</code></td><td><code>&quot;table_name&quot;</code></td><td>写入的表名</td></tr></tbody></table><h5 id="文件类型"><a href="#文件类型" class="headerlink" title="文件类型"></a>文件类型</h5><ol><li><p><strong><code>Parquet</code></strong><br><code>Spark</code> 的默认数据源，很多大数据框架和平台都支持，它是一种开源的列式存储文件格式，提供多种 <code>I/O</code> 优化措施（比如压缩，以节省存储空间，支持快速访问数据列）。</p></li><li><p><code>JSON</code><br><code>JSON</code> 的全称为 <code>JavaScript Object Notation</code>，它 是一种常见的数据格式。<code>JSON</code> 有两种表示格式：单行模式和多行模式。</p></li><li><p><code>CSV</code><br><code>CSV</code> 格式应用非常官方，是一种将所有数据字段用逗号隔开的文本文件格式。在这些用逗号隔开的字段中，每行表示一条记录。（这里的逗号分隔符号是可以被修改的）</p></li><li><p><strong><code>Avro</code></strong><br><code>Avro</code> 格式有很多优点，包括直接映射 <code>JSON</code>、快速且高效、支持多种编程语言。</p></li><li><p><strong><code>ORC</code></strong><br>作为另一种优化后的列式存储文件格式，<code>Spark</code> 支持 <code>ORC</code> 的向量化读。向量化读通常会成块（）读入数据，而不俗一次读一行，同时操作会串起来，降低扫描、过滤、聚合、连接等集中操作时的 <code>CPU</code> 使用率。</p></li></ol><h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><p>复杂数据类型由简单数据类型组合而成，而实际则是经常直接操作复杂数据类型，操作复杂数据类型的方式有以下两种:</p><ul><li>将嵌套的结构打散到多行，调用某个函数，然后重建嵌套结构。</li><li>构建用户自定义函数。</li></ul><p>这两种方式都有助于以表格格式处理问题，一般会涉及到 <code>get_json_object()</code>、<code>from_json()</code>、<code>to_json()</code>、<code>explode()</code> 和 <code>selectExpr()</code> 等工具函数。</p><h5 id="打散再重组"><a href="#打散再重组" class="headerlink" title="打散再重组"></a>打散再重组</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT id, collect_list(value + 1) AS values</span><br><span class="line">FROM (</span><br><span class="line">   SELECT id, EXPLODE(values) AS value</span><br><span class="line">   FROM table</span><br><span class="line">) x</span><br><span class="line">GROUP BY id</span><br></pre></td></tr></table></figure><p>上述的嵌套的 <code>SQL</code> 语句中，先执行 <code>EXPLODE(values)</code>，会为每一个 <code>value</code> 创建新的一行（包括 <code>id</code> 字段）。<code>collect_list()</code> 返回的是未去重的对象列表，由于 <code>GROUP BY</code> 语句会触发数据混洗操作，因此重新组合的数组顺序和原数组不一定相同。 </p><h5 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h5><p>要想自行上述等价的任务，也可以创建 <code>UDF</code>，用 <code>map()</code> 迭代各个元素并执行额外的操作。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val = plusOneInt = (values: Array[Int] =&gt; &#123;</span><br><span class="line">   values.map(value =&gt; value + 1)</span><br><span class="line">&#125;)</span><br><span class="line">spark.udf.register(&quot;plusOneInt&quot;, plusOneInt)</span><br></pre></td></tr></table></figure><p>然后可以在 <code>Spark SQL</code> 中使用这个 <code>UDF</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;SELECT id, plusOneInt(values) AS values FROM table&quot;).show()</span><br></pre></td></tr></table></figure><p>由于没有顺序问题，这种方法比使用 <code>explode()</code> 和 <code>collect_list()</code> 好一些，但序列化和反序列化过程本身开销很大。</p><h5 id="复杂类型的内建函数"><a href="#复杂类型的内建函数" class="headerlink" title="复杂类型的内建函数"></a>复杂类型的内建函数</h5><p><code>Spark</code> 专门为复杂数据类型准备的内建函数，完整列表可以参考官方文档。</p><h5 id="高阶函数-1"><a href="#高阶函数-1" class="headerlink" title="高阶函数"></a>高阶函数</h5><p>除了上述的内建函数外，还有部分高阶函数接受匿名 <code>lambda</code> 函数作为参数，示例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># transform 函数接受一个数组和匿名函数作为输入，通过对数组的每个元素应用匿名函数，该函数将结果赋值到输出数组，透明地创建出一个新数组。</span><br><span class="line">transform(<span class="keyword">values</span>, <span class="keyword">values</span> <span class="operator">-</span><span class="operator">&gt;</span> lambda expression)</span><br></pre></td></tr></table></figure><ol><li><p><code>transform()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform(array&lt;T&gt;, function&lt;T, U&gt;): array&lt;U&gt;</span><br></pre></td></tr></table></figure><p>通过对输入数组的每个元素使用一个函数，<code>transform()</code> 函数会生成新的数组。 </p></li><li><p><code>filter()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter(array&lt;T&gt;, function&lt;T, Boolean&gt;): array&lt;T&gt;</span><br></pre></td></tr></table></figure><p><code>filter()</code> 函数输出的数组仅包含输入数组中让布尔表达式结果为 <code>true</code> 的元素。</p></li><li><p><code>exists()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exists(array&lt;T&gt;, function&lt;T, V, Boolean&gt;): Boolean</span><br></pre></td></tr></table></figure><p>当输入数组中有任意一元素满足布尔函数时，<code>exists()</code> 函数返回 <code>true</code>。</p></li><li><p><code>reduce()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(array&lt;T&gt;, B, function&lt;B, T, B&gt;, function&lt;B, R&gt;)</span><br></pre></td></tr></table></figure><p>通过函数 <code>function&lt;B, T, B&gt;</code>，<code>reduce()</code> 函数可以将数组的元素合并到缓冲区 <code>B</code>，最后对最终缓冲区使用最终函数 <code>function&lt;B, R&gt;</code>，并将数组归约为单个值。</p></li></ol><h5 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h5><ol><li><p>联合<br>将具有相同表结构的 <code>DataFrame</code> 联合起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 联合两种表</span></span><br><span class="line">bar = deplays.union(foo)</span><br><span class="line">bar.createOrReplaceTempView(<span class="string">&quot;bar&quot;</span>)</span><br><span class="line"><span class="comment"># 展示联合结果</span></span><br><span class="line">bar.filtyer(expr(<span class="string">&quot;origin == &#x27;SEA&#x27; AND destination == &#x27;SFO&#x27; AND date LIKE &#x27;0010%&#x27; AND deplay &gt; 0&quot;</span>)).show()</span><br></pre></td></tr></table></figure></li><li><p>连接<br>连接两个 <code>DataFrame</code> 是常用操作之一。<br>默认情况下连接为 <code>inner join</code>，可选的种类包含 <code>inner</code>、 <code>cross</code>、 <code>outer</code>、 <code>full</code>、 <code>full_outer</code>、 <code>left</code>、 <code>left_outer</code>、 <code>right</code>、 <code>right_outer</code>、 <code>left_semi</code> 和 <code>left_anti</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foo.join(ports, ports.IATA == foo.origin).select(<span class="string">&quot;City&quot;</span>, <span class="string">&quot;date&quot;</span>, <span class="string">&quot;deplay&quot;</span>, <span class="string">&quot;distance&quot;</span>).show()</span><br></pre></td></tr></table></figure></li><li><p>窗口<br>窗口函数使用窗口（一个范围内的输入行）中各行的值计算出一组值来返回，返回的一般是新的一行。通过使用窗口函数可以在每次操作一组的同时，返回的行数仍然和输入行数一一对应。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> origin, destiation, TotalDelays, rank</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">SELECT</span> origin, destiation, TotalDelays, <span class="built_in">dense_rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> origin <span class="keyword">ORDER</span> <span class="keyword">BY</span> TotalDelays <span class="keyword">DESC</span>) <span class="keyword">as</span> rank) t</span><br><span class="line"><span class="keyword">WHERE</span> rank <span class="operator">&lt;=</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></li><li><p>修改<br>对 <code>DataFrame</code> 进行修改，<code>DataFrame</code> 本身不允许被修改，不过可以通过新建 <code>DataFrame</code> 的方式来实现修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加新列</span></span><br><span class="line">foo2 = (foo.withColumn(<span class="string">&quot;status&quot;</span>, expr(<span class="string">&quot;CASE WHEN delay &lt;= 10 THEN &#x27;On-time&#x27; ELSE &#x27;Delayed&#x27; END&quot;</span>)))</span><br><span class="line">foo2.show()</span><br><span class="line"><span class="comment"># 删除列</span></span><br><span class="line">foo3 = foo2.drop(<span class="string">&quot;delay&quot;</span>)</span><br><span class="line">foo3.show()</span><br><span class="line"><span class="comment"># 修改列名</span></span><br><span class="line">foo4 = foo3.withColumnRenamed(<span class="string">&quot;status&quot;</span>, <span class="string">&quot;flight_status&quot;</span>)</span><br><span class="line">foo4.show()</span><br><span class="line"><span class="comment"># 转置（将行与列数据互换）</span></span><br><span class="line">SELECT * FROM (</span><br><span class="line">   SELECT destination, CAST(SUBSTRING(date, <span class="number">0</span>, <span class="number">2</span>) AS <span class="built_in">int</span>) AS month, delay FROM departureDelays WHERE origin = <span class="string">&#x27;SEA&#x27;</span></span><br><span class="line">)</span><br><span class="line">PIVOT (</span><br><span class="line">   CAST(AVG(delay) AS DECIMAL(<span class="number">4</span>, <span class="number">2</span>)) AS AvgDelay, MAX(delay) AS MaxDelay FOR month IN (<span class="number">1</span> JAN, <span class="number">2</span> FEB)</span><br><span class="line">)</span><br><span class="line">ORDER BY destination</span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="数据结构化-Dataset"><a href="#数据结构化-Dataset" class="headerlink" title="数据结构化 Dataset"></a>数据结构化 <code>Dataset</code></h3><p>前面看过了 <code>DataFrame</code>，那么你基本上就理解了 <code>Dataset</code>，不过还是有那么一些差别，<code>Dataset</code> 主要区分两种特性： <strong>有类型</strong>和<strong>无类型</strong> 。</p><p><img src="https://s2.loli.net/2023/04/10/897iHPxjcUebNE5.jpg" alt="hadoop_spark_4.jpg"></p><p>从概念上看，可以将 <code>DataFrame</code> 看作是 <code>Dataset[Row]</code> 这种由普通对象组成的集合的一个别称，其中 <code>Row</code> 是普通的无类型对象，可以包含不同数据类型的字段。而 <code>Dataset</code> 则与之相反，是由同一类型的对象所组成的集合。正如官方文档中描述的那样：</p><blockquote><p>一种由领域专用对象组成的强类型集合，可以使用函数式或关系型的操作将其并行转化。</p></blockquote><table><thead><tr><th>语言</th><th>有类型和无类型的主要抽象结构</th><th>有类型或无类型</th></tr></thead><tbody><tr><td><code>Scala</code></td><td><code>Dataset[T]</code> 和 <code>DataFrame</code>（<code>Dataset[Row]</code> 的别命）</td><td>都有</td></tr><tr><td><code>Java</code></td><td><code>Dataset&lt;T&gt;</code></td><td>有类型</td></tr><tr><td><code>Python</code></td><td><code>DataFrame</code></td><td>普通 <code>Row</code> 对象，无类型</td></tr><tr><td><code>R</code></td><td><code>DataFrame</code></td><td>普通 <code>Row</code> 对象，无类型</td></tr></tbody></table><h4 id="转化数据"><a href="#转化数据" class="headerlink" title="转化数据"></a>转化数据</h4><p><code>Dataset</code> 是强类型的对象集合，这些对象可以使用函数式或关系型的算子并行转化。可用的转化操作包括 <code>map()</code>、<code>reduce()</code>、<code>filter()</code>、<code>select()</code> 和 <code>aggregate()</code>，这些方法都属于高阶函数，他们接受 <code>lambda</code> 表达式、闭包或函数作为参数，然后返回结果，因此这些操作非常适合函数式编程。</p><p>不过上述 <code>Dataset</code> 是有不足之处的，在使用高阶函数时，会产生从 <code>Spark</code> 内部的 <code>Tungsten</code> 格式反序列化为 <code>JVM</code> 对象的开销，那么避免多余的序列化和反序列化的策略有以下两种：</p><ul><li>在查询中使用 <code>DSL</code> 表达式，避免过多地使用 <code>lambda</code> 表达式的匿名函数作为高阶函数的参数。</li><li>将查询串联起来，以尽量减少序列化和反序列化。</li></ul><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p><strong>编码器将堆外内存中的数据从 <code>Tungsten</code> 格式转为 <code>JVM</code> 对象，即编码器承担着在 <code>Spark</code> 内部格式和 <code>JVM</code> 对象之间序列化和反序列化 <code>Dataset</code> 对象</strong> 。<br><code>Spark</code> 支持自动生成原生类型、<code>Scala</code> 样例类和 <code>JavaBean</code> 的编码器。比起 <code>Java</code> 和 <code>Kryo</code> 的序列化和反序列化，<code>Spark</code> 的编码器要快很多。</p><p><code>Spark</code> 不为 <code>Dataset</code> 或 <code>DataFrame</code> 创建基于 <code>JVM</code> 的对象，而会分配 <code>Java</code> 堆外内存来存储数据，并使用编码器将内存表示的数据转为 <code>JVM</code> 对象。<br>当数据以紧凑的方式存储并通过指针和偏移量访问时，编码器可以快速序列化和反序列化数据。</p><p><img src="https://s2.loli.net/2023/04/10/S5PKNrgB8GFZHRd.jpg" alt="hadoop_spark_5.jpg"></p><p>相比 <code>JVM</code> 自建的序列化和反序列化，<code>Dataset</code> 编码器的优点如下：</p><ul><li><code>Spark</code> 内部的 <code>Tungsten</code> 二进制格式将对象存储在 <code>Java</code> 的堆内存之外，存储的方式很紧凑，因此对象占用的空间更小。</li><li>通过使用指针和计算出的内存地址与偏移量来访问内存，编码器可以实现快速序列化。</li><li>在接收端，编码器能快速地将二进制格式反序列化为 <code>Spark</code> 内部的表示形式。编码器不受 <code>JVM</code> 垃圾回收暂停的影响。</li></ul><p><img src="https://s2.loli.net/2023/04/10/a6jyR9tQhSOGAEz.jpg" alt="hadoop_spark_6.jpg"></p><hr><h3 id="Spark-引擎"><a href="#Spark-引擎" class="headerlink" title="Spark 引擎"></a><code>Spark</code> 引擎</h3><p>在编程层面上，<code>Spark SQL</code> 允许开发人员对带有表结构的结构化数据发起兼容 <code>ANSI SQL:2003</code> 标准的查询。至此 <code>Spark SQL</code>  已经演变成一个非常重要的引擎，许多高层的结构化功能都是基于它构建出来的，除了可以对数据发起类似 <code>SQL</code> 的查询，<code>Spark SQL</code> 引擎还支持下列功能：</p><ul><li>统一 <code>Spark</code> 的各个组件，允许在 <code>Java</code>、<code>Scala</code>、<code>Python</code>、<code>R</code> 程序中将结构化数据集抽象为 <code>DataFrame</code> 或 <code>Dataset</code>，从而简化编程工作。</li><li>连接 <code>Apache Hive</code> 的元数据库和表。</li><li>从结构化的文件格式（<code>JSON</code>、<code>CSV</code>、<code>Text</code>、<code>Avro</code>、<code>Parquet</code>、<code>ORC</code> 等）使用给定的表结构读取结构化数据，并将数据转换为临时表。</li><li>为快速的数据探索提供交互式 <code>Spark SQL shell</code>。</li><li>通过标准的 <code>JDBS/ODBC</code> 连接器，提供与外部工具互相连接的纽带。</li><li>生成优化后的查询计划和紧凑的 <code>JVM</code> 二进制代码，用于最终执行。</li></ul><p><img src="https://s2.loli.net/2023/04/10/yilF4BnUgvVhQTC.jpg" alt="hadoop_spark_7.jpg"></p><p><code>Spark SQL</code> 引擎的核心是 <strong><code>Catalyst</code> 优化器</strong>和 <strong><code>Tungsten</code> 项目</strong> ，其两者共同支撑高层的 <code>DataFrame API</code> 和 <code>Dataset API</code>，以及 <code>SQL</code> 查询。</p><h4 id="Catalyst-优化器"><a href="#Catalyst-优化器" class="headerlink" title="Catalyst 优化器"></a><code>Catalyst</code> 优化器</h4><p><code>Catalyst</code> 优化器接受计算查询作为参数，并将查询转化为执行计划。共分为四个转换阶段：</p><ul><li><strong>解析</strong><br><code>Spark SQL</code> 引擎首先会为 <code>SQL</code> 或 <code>DataFrame</code> 查询生成相应的抽象语法树（<code>abstract synrax tree, AST</code>）。所有的列名和表名都会通过查询内部元数据而解析出来，全部解析完成后，会进入下一阶段。</li><li><strong>逻辑优化</strong><br>这个阶段在内部共分为两步。通过应用基于规则的优化策略，<code>Catalyst</code> 优化器会首先构建出多个计划，然后使用基于代价的优化器（<code>cost-based optimizer, CBO</code>）为每个计划计算出执行开销。这些计划以算子树的形式呈现，其优化过程包括常量折叠、谓词下推、列裁剪、布尔表达式简化等，最终获得的逻辑计划作为下一阶段的输入，用于生成物理计划。</li><li><strong>生成物理计划</strong>hexo<br><code>Spark SQL</code> 会为选中的逻辑计划生成最佳的物理计划，这个物理计划由 <code>Spark</code> 执行引擎可用的物理算子组成。</li><li><strong>生成二进制代码</strong><br>在查询优化的最终阶段，<code>Spark</code> 会最终生成高效的 <code>Java</code> 字节码，用于在各个机器上执行。而在这个过程中用到了 <code>Tungsten</code> 项目，实现了执行计划的全局代码生成。<br>那什么是全局代码生成呢？他是物理计划的一个优化阶段，将整个查询合并为一个函数，避免虚函数调用，利用 <code>CPU</code> 寄存器存放中间数据，而这种高效策略可以显著提升 <code>CPU</code> 效率和性能。</li></ul><p><img src="https://s2.loli.net/2023/04/10/uevImbGLT7N9rJK.jpg" alt="hadoop_spark_8.jpg"></p><p>通过上面的图示，可以发现只要执行过程相同，最终会生成相似的查询计划和一样的字节码用于执行，也就是说，无论使用什么编程语言，查询都会经过同样的过程，所生成的字节码很有可能都是一样的。</p><p>在经过最初的解析阶段之后，查询计划会被 <code>Catalyst</code> 优化器转化和重排。</p><p><img src="https://s2.loli.net/2023/04/10/cUXY3alpo6qZmrn.jpg" alt="hadoop_spark_9.jpg"></p><h4 id="Tungsten-项目"><a href="#Tungsten-项目" class="headerlink" title="Tungsten 项目"></a><code>Tungsten</code> 项目</h4><p><code>Tungsten</code> 项目致力于提升 <code>Spark</code> 应用对内存和 <code>CPU</code> 的利用率，使性能达到硬件的极限，主要包含以下内容：</p><ul><li><code>Memory Management and Binary Processing</code>: <code>off-heap</code> 管理内存，降低对象的开销和消除 <code>JVM GC</code> 带来的延时。</li><li><code>Cache-aware computation</code>: 优化存储，提升 <code>CPU L1/L2/L3</code> 缓存命中率。</li><li><code>Code generation</code>: 优化 <code>Spark SQL</code> 的代码生成部分，提升 <code>CPU</code> 利用率。</li></ul><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;引入&quot;&gt;&lt;a href=&quot;#引入&quot; class=&quot;headerlink&quot; title=&quot;引入&quot;&gt;&lt;/a&gt;引入&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Spark&lt;/code&gt; 是用于&lt;strong&gt;处理大数据的集群计算框架&lt;/strong&gt; ，与其他大多数数据处理框架不同之处在于 &lt;code&gt;Spark&lt;/code&gt; 没有以 &lt;code&gt;MapReduce&lt;/code&gt; 作为执行引擎，而是使用它自己的&lt;strong&gt;分布式运行环境&lt;/strong&gt;在集群上执行工作。另外 &lt;code&gt;Spark&lt;/code&gt; 与 &lt;code&gt;Hadoop&lt;/code&gt; 又紧密集成，&lt;code&gt;Spark&lt;/code&gt; 可以在 &lt;code&gt;YARN&lt;/code&gt; 上运行，并支持 &lt;code&gt;Hadoop&lt;/code&gt; 文件格式及其存储后端（例如 &lt;code&gt;HDFS&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Spark&lt;/code&gt; 最突出的表现在于其能将 &lt;strong&gt;作业与作业之间的大规模的工作数据集存储在内存中&lt;/strong&gt;。这种能力使得在性能上远超 &lt;code&gt;MapReduce&lt;/code&gt; 好几个数量级，原因就在于 &lt;code&gt;MapReduce&lt;/code&gt; 数据都是从磁盘上加载。根据 &lt;code&gt;Spark&lt;/code&gt; 的处理模型有两类应用获益最大，分别是 &lt;strong&gt;迭代算法（即对一个数据集重复应用某个函数，直至满足退出条件）&lt;/strong&gt;和 &lt;strong&gt;交互式分析（用户向数据集发出一系列专用的探索性查询）&lt;/strong&gt; 。&lt;br&gt;另外 &lt;code&gt;Spark&lt;/code&gt; 还因为其具有的 &lt;strong&gt;&lt;code&gt;DAG&lt;/code&gt; 引擎&lt;/strong&gt;更具吸引力，原因在于 &lt;code&gt;DAG&lt;/code&gt; 引擎可以处理任意操作流水线，并为用户将其转化为单个任务。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
</feed>
