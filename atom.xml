<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vgbhfive&#39;s Blog</title>
  
  
  <link href="https://blog.vgbhfive.cn/atom.xml" rel="self"/>
  
  <link href="https://blog.vgbhfive.cn/"/>
  <updated>2023-08-13T03:52:25.474Z</updated>
  <id>https://blog.vgbhfive.cn/</id>
  
  <author>
    <name>vgbhfive</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ML-决策树</title>
    <link href="https://blog.vgbhfive.cn/ML-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://blog.vgbhfive.cn/ML-%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2023-08-02T13:48:52.000Z</published>
    <updated>2023-08-13T03:52:25.474Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><p>决策树（<code>decision tree</code>）是一种基本的<strong>分类和回归方法</strong>。其主要呈现为<strong>树状结构</strong>，在分类问题中，表示基于特征对实例进行分类的过程，可以被认为是 <strong><code>if-then</code> 的规则集合</strong>，也可以被认为是定义在<strong>特征空间与类空间上的条件概率分布</strong>。</p><p>其优点主要有分类速度快、模型具有可读性，在学习时利用训练数据根据<strong>损失函数最小化</strong>的原则建立决策树模型；而在预测时对新的数据利用<strong>决策树模型</strong>进行分类。</p><p>决策树模型主要包含以下步骤：</p><ul><li><strong>特征选择</strong></li><li><strong>决策树的生成</strong></li><li><strong>决策树的修剪</strong></li></ul><span id="more"></span><hr><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h4><p>分类决策树模型是一种描述对实例进行分类的树状结构。决策树有<strong>结点（<code>node</code>）</strong>和<strong>有向边（<code>directed edge</code>）</strong>组成，结点有两种类型：<strong>内部结点（<code>internal node</code>）</strong>和<strong>叶结点（<code>leaf node</code>）</strong>，其中内部结点表示一个特征或属性，叶结点表示一个类。</p><p>用决策树分类，首先从根结点开始对实例的某一个特征进行测试，根据测试结果将实例分配到其子结点上，每个子结点对应着一个特征的取值。如此递归地对实例进行测试和分配，直至到达叶结点，最后将实例分配到叶结点的分类中。</p><p><img src="/ML-%E5%86%B3%E7%AD%96%E6%A0%91/decision_tree-1.jpeg" alt="decision_tree-1"></p><h4 id="分类过程"><a href="#分类过程" class="headerlink" title="分类过程"></a>分类过程</h4><ol><li><p><strong><code>if-then</code> 规则集合</strong><br>决策树可以看成是 <code>if-then</code> 规则的集合。将决策树转换为 <code>if-then</code> 规则的流程如下：由决策树的根结点到叶结点的每一条路径构建一个规则；路径上的内部结点的特征表示规则的条件，而叶结点的分类则对应的规则的结论。<br>决策树上的路径或其对应的 <code>if-then</code> 规则集合具有一个重要的性质：<strong>互斥且完备</strong>。也就是说每一个实例都被一条路径或对应的一条规则覆盖，而且仅被一条路径或一条规则所覆盖。<br><small>覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</small></p></li><li><p><strong>条件概率分布</strong><br>决策树还可以表示为给定特征条件下类的<strong>条件概率分布</strong>，该条件概率分布定义在特征空间的划分（<code>partition</code>）上。将特征空间划分为互不相交的单元或区域，并在每个单元或区域定义一个类的概率分布就可以构成一个条件概率分布。<br>决策树中的每一条路径对应花粉中的某个单元，决策树所表示的条件概率分布就是由各个单元在给定特征条件下类的条件概率分布组成。<br><img src="/ML-%E5%86%B3%E7%AD%96%E6%A0%91/decision_tree_3.jpg" alt="decision_tree-3"></p></li></ol><h4 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h4><p>决策树学习根据给定的训练数据集构建一个决策树模型，使之可以正确地进行分类，但其本质就是从训练数据集中归纳总结出一组分类规则，而最终所需要的是具有很好的泛化能力，既可以对训练数据有很好的拟合，也可以对未知数据有很好的预测。<br>那么如何对这一要求如何体现呢？决策树模型使用损失函数来表示这一目标，其通常是<strong>正则化的极大似然函数，策略是以损失函数为目标函数的最小化</strong>。</p><p>决策树学习的算法通常就是一个递归地选择特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类，这一过程对应特征空间的划分，也是决策树的构建。<br>在开始时构建根结点，将所有的训练数据都放在根结点，此时选择一个最优特征，根据这一特征将训练数据集划分为子集，使得各个子集在当前分类情况下是最好的分类；如果这些子集可以被基本正确分类，那就构建叶结点，并将子集分配到对应的叶结点中；如果这些子集不能被正确分类，则为这些子集重新选择新的最优特征，继续对其划分并构建相应的结点；如此递归下去直至训练数据子集都能被正确分类，或者没有合适的特征为止，至此每个子集都会被分配到对应的叶结点上，这就生成了一颗决策树。</p><p>以上方法生成的决策树可以很好的对训练数据集进行分类，但对于未知的数据集却未必有很好的分类能力，即可能发生<strong>过拟合现象</strong>。因此需要对生成的决策树进行<strong>剪枝</strong>，将决策树变得简单化，使其具有更好的<strong>泛化能力</strong>。</p><hr><h3 id="生成算法"><a href="#生成算法" class="headerlink" title="生成算法"></a>生成算法</h3><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a><code>ID3</code></h4><p><code>ID3</code> 算法的核心在于对决策树上的各个结点应用<strong>信息增益准则</strong>选择特征，递归地构建决策树。<br>具体方法是：</p><ul><li>从根结点（<code>root node</code>）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点。</li><li>再对子结点递归地调用以上方法，构建决策树。</li><li>直到所有特征的信息增益都很小或没有特征可以选择为止。</li><li>最终就会得到一个决策树，其本质是使用 <em>极大似然法</em> 进行概率模型的选择。</li></ul><p>示例：</p><blockquote><p>输入：训练数据集 <code>D</code>，特征集 <code>A</code>，阈值 $\omega$ <br><br>输出：决策树<br>过程：</p><ol><li>若 <code>D</code> 中所有实例属于同一类 $C_k$，则 <code>T</code> 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 <code>T</code>。</li><li>若 <code>A =</code> $\phi$，则 <code>T</code> 为单结点树，并将 <code>D</code> 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 <code>T</code>。</li><li>否则计算 <code>A</code> 中各个特征对 <code>D</code> 的信息增益，选择信息增益最大的特征 $A_g$。</li><li>如果 $A_g$ 小于阈值 $\omega$，则置 <code>T</code> 为单结点树，并将 <code>D</code> 中实例数最大的类 $C_k$ 作为该类的标记，返回 <code>T</code>。</li><li>否则对 $A_g$ 的每一个可能值 $a_i$，依据 $A_g$ &#x3D; $a_i$ 将 <code>D</code> 划分为若干个非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 <code>T</code>，返回 <code>T</code>。</li><li>对第 <code>i</code> 个子结点，以 $D_i$ 为训练集，以 <code>A - </code>$A_g$ 为特征集，递归地调用 <code>1, 2, 3, 4, 5</code> 得到子树 $T_i$，返回 $T_i$。</li></ol></blockquote><h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a><code>C4.5</code></h4><p><code>C4.5</code> 算法与 <code>ID3</code> 算法类似，<code>C4.5</code> 在生成过程中使用了<strong>信息增益比</strong>来选择特征。</p><h4 id="CART"><a href="#CART" class="headerlink" title="CART"></a><code>CART</code></h4><p><strong>分类与回归树（<code>classification and regression tree, CART</code>）模型</strong>是应用非常广泛的决策树学习方法，既可以用于分类，也可以用于回归，将用于分类和回归的树统称为决策树。</p><p><code>CART</code> 是在给定输入随机变量 <code>X</code> 条件下输出随机变量 <code>Y</code> 的条件概率分布的学习方法。解释其含义就是，<code>CART</code> 假设决策树是二叉树，内部结点特征取值为<strong>“是”</strong>或 <strong>“否”</strong>，即左分支是取值为“是”的分支，右分支是取值为“否”的分支，这样决策树就等价于递归地对每个特征进行二分，即将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p><p><code>CART</code> 算法由以下两步组成：</p><ul><li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽可能的大。</li><li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，此时<strong>使用损失函数最小化作为剪枝的标准</strong>。</li></ul><hr><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以大大提高决策树学习的效率。如果一个特征进行分类的结果和随机分类的结果没有太大的差别，则这个特征没有分类能力，实际中这样的特征对决策树学习的准确度影响不大。<br>特征选择的准则是：</p><ul><li><strong>信息增益</strong></li><li><strong>信息增益比</strong></li><li><strong>基尼系数</strong></li></ul><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>在信息论与概率统计中，<strong>熵（<code>entropy</code>）</strong>用来度量随机变量的不确定性。</p><p><strong>条件熵 <code>H(Y|X)</code></strong> 表示在已知随机变量 <code>X</code> 的条件下随机变量 <code>Y</code> 的不确定性，即定义为 <code>X</code> 给定条件下 <code>Y</code> 的条件概率分布的熵对 <code>X</code> 的数学期望。</p><p>$$ H(Y|X) &#x3D; \sum_{i&#x3D;1}^n p_i H (Y | X &#x3D; x_i) $$</p><p><small>其中 $p_i$ <code>= P(X = </code> $x_i$ <code>), i = 1, 2, ..., n</code></small></p><p>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别被称为<strong>经验熵（<code>empirical entropy</code>）</strong>和<strong>经验条件熵（<code>empirical conditional entropy</code>）</strong>。</p><p><strong>信息增益（<code>information gain</code>）</strong>表示得知特征 <code>X</code> 的信息而使得类 <code>Y</code> 的信息的不确定性减少的程度。<br>特征 <code>A</code> 对训练数据集 <code>D</code> 的信息增益 $g(D, A)$，定义为集合 <code>D</code> 的经验熵 $H(D)$ 与特征 <code>A</code> 给定条件下 <code>D</code> 的经验条件熵 $H(D|A)$ 之差，即</p><p>$$ g(D, A) &#x3D; H(D) - H(D|A) $$</p><p>给定训练数据集 <code>D</code> 与特征 <code>A</code>，<strong>经验熵 $H(D)$</strong> 表示数据集 <code>D</code> 进行分类的不确定性；而<strong>经验条件熵 $H(D|A)$</strong> 表示在特征 <code>A</code> 给定条件下对数据集 <code>D</code> 进行分类的不确定性，那么他们的差即信息增益，表示由于特征 <code>A</code> 而使得对数据集 <code>D</code> 的分类的不确定性减少的程度。由此可以得到信息增益依赖于特征，不同的特征具有不同的信息增益，而信息增益大的特征具有更强的分类能力。</p><p><small>一般地，熵 <code>H(Y)</code> 与条件熵 <code>H(Y|X)</code> 之差称为互信息（<code>mutual information</code>），决策树学习中的信息增益等价于训练数据集中类与特征的互信息，因此决策树学习应用信息增益准则选择特征。</small></p><p>在实际中根据信息增益准则选择特征的方式是：对训练数据集 <code>D</code>，计算其每个特征的信息增益，并比较其大小，择优选择信息增量最大的特征。</p><p>信息增益算法示例： </p><blockquote><p>输入：训练数据集 <code>D</code> 和特征 <code>A</code><br>输出：特征 <code>A</code> 对训练数据集 <code>D</code> 的信息增益 <code>g(D, A)</code><br>过程：</p><ol><li>计算数据集 <code>D</code> 的信息增益<br>$$ H(D) &#x3D; - \sum_{k&#x3D;1}^K {\frac {\vert {C_k}\vert} {\vert {D} \vert} } log_2 {\frac {\vert {C_k}\vert} {\vert {D} \vert} } $$</li><li>计算特征 <code>A</code> 对数据集 <code>D</code> 的经验条件熵<br>$$ H(D|A) &#x3D; \sum_{i&#x3D;1}^n {\frac {\vert {D_i} \vert} {\vert {D} \vert} H(D_i) } $$</li><li>计算信息增益<br>$$ g(D, A) &#x3D; H(D) - H(D|A) $$</li></ol></blockquote><h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>信息增益的大小是相对于训练数据而言的，并没有绝对意义，而在训练数据集的经验熵大的时候，信息增益也会偏大。反之信息增益值会偏小。因此使用信息增益比可以解决这一问题，也是特征选择的另一个准则。<br>特征 <code>A</code> 对训练数据集 <code>D</code> 的信息增益比 $g_r(D, A)$，定义为其信息增益 $g(D, A)$ 与训练数据集 <code>D</code> 的经验熵 $H(D)$ 之比：</p><p>$$ g_r(D, A) &#x3D; {\frac {g(D, A)} {H(D)} } $$</p><h4 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h4><p>决策树的生成是递归地构建二叉决策树的过程，对<strong>回归树使用平方误差最小化准则</strong>，对<strong>分类树使用基尼系数（<code>Gini index</code>）最小化准则</strong>，进行特征选择生成二叉树。</p><h5 id="生成回归树"><a href="#生成回归树" class="headerlink" title="生成回归树"></a>生成回归树</h5><p>假设 <code>X, Y</code> 分别表示输入和输出变量，且 <code>Y</code> 是连续变量，给定的数据集表示为：$D &#x3D; { (x_1, y_1), (x_2, y_2), …, (x_N, y_N)}$，那么回归树如何生成？</p><p>一个回归树对应着输入空间（特征空间）的一个划分以及在划分单元上的输出值。假设已将输入空间划分为 <code>M</code> 个单元 $R_1, R_2, …, R_M$ 并且在每个单元 $R_m$ 上有一个固定的输出值 $C_m$，于是回归树的模型可以表示为</p><p>$$ f(x) &#x3D; \sum_{m&#x3D;1}^M C_m I(x \in R_m) $$</p><p>当输入空间的划分确定时就可以使用平方误差 $\sum_{x_i \in R_m} (y_i - f(x_i))^2$ 表示回归树对训练数据集的预测误差，即平方误差最小时表示该单元上的最优输出值。</p><p>那输入空间（特征空间）的切分点如何划分呢？这里采用启发式的方法，选择第 <code>j</code> 个变量 $X_j$ 和对应的取值 $C_j$ 作为<strong>切分变量（<code>splitting variable</code>）</strong>和<strong>切分点（<code>splitting point</code>）</strong> ，定义两个区域<br>$$R_1(j, C_j) &#x3D; {X | X_j &lt;&#x3D; C_m}, R_2(j, C_j) &#x3D; {x | X_j &gt; C_m}$$<br>然后寻找<strong>最优变量 <code>j</code></strong> 和<strong>最优切分点 $C_m$</strong> 。即求解</p><p>$$ \min_{j,C_m}[\min_{C_1}\sum_{x_i \in R_1(j, C_m)}(y_i - C_1)^2 + \min_{C_2}\sum_{x_i \in R_1(j, C_m)}(y_i - C_2)^2] $$</p><p>而对于固定输入变量 <code>j</code> 就可以找到最优切分点 $C_m$</p><p>$$ \bar{c_1} &#x3D; ave(y_i | x_i \in R_1(j, C_m)) , \bar{c_2} &#x3D; ave(y_i | x_i \in R_2(j, C_m)) $$</p><p>接着遍历输入所有变量，找到最优的切分变量 <code>j</code>，构成一个 $(j, C_m)$。依次将输入空间划分为两个区域，接着对上述区域进行重复划分，直至满足条件为止。即可以生成被称为<strong>最小二乘回归树（<code>least squares regression tree</code>）</strong>。</p><p>最小二乘回归树生成算法示例：</p><blockquote><p>输入：训练数据集 <code>D</code><br>输出：回归树 $f(x)$ <br><br>过程：</p><ol><li>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域的输出值。</li><li>计算划分数据集的最优切分变量 <code>j</code> 和最优切分点 $C_m$，求解<br> $$ \min_{j,C_m}[\min_{C_1}\sum_{x_i \in R_1(j, C_m)}(y_i - C_1)^2 + \min_{C_2}\sum_{x_i \in R_1(j, C_m)}(y_i - C_2)^2] $$<br>遍历变量 <code>j</code>，对固定的切分变量 <code>j</code> 和切分点 $C_m$ 能得到最小值的对 $(j, C_m)$。</li><li>用选定的最小值的对 $(j, C_m)$ 划分区域并计算对应的输出值<br> $$ R_1(j, C_m) &#x3D; {x|x_j &lt;&#x3D; C_m}, R_2(j, C_m) &#x3D; {x|x_j &gt; C_m} $$<br> $$ \bar{C_m} &#x3D; {\frac {1} {N_m}} \sum_{x_i \in R_m(j, C_m)} y_i, x \in R_m, m &#x3D; 1, 2 $$</li><li>继续对两个子区域调用步骤 <code>2, 3</code>，直至满足条件。</li><li>至此就可以得到将输入空间划分为 <code>M</code> 个区域 $R_1, R_2, …, R_M$ 即可生成回归决策树。<br> $$ f(x) &#x3D; \sum_{m&#x3D;1}^M \bar{C_m} I(x \in R_m) $$</li></ol></blockquote><h5 id="生成分类树"><a href="#生成分类树" class="headerlink" title="生成分类树"></a>生成分类树</h5><p>分类树用<strong>基尼指数</strong>选择最优特征，同时决定该特征的最优二值切分点。<br>在分类问题中，假设有 <code>K</code> 个类，样本点属于第 <code>k</code> 类的概率为 <code>Pk</code>，则概率分布的基尼指数定义为：</p><p>$$ Gini(p) &#x3D; \sum_{k&#x3D;1}^K P_k (1-P_k) &#x3D; 1-\sum_{k&#x3D;1}^K p_k^2 $$</p><p>对于二分类问题，若样本点属于第一个类的概率为 <code>p</code>，则概率分布的基尼指数为：</p><p>$$ Gini(p) &#x3D; 2p(1-p) $$</p><p>对于给定的样本集合 <code>D</code>，其基尼指数为：</p><p>$$ Gini(D) &#x3D; 1 - (\sum_{k&#x3D;1}^K)({\frac {\vert{C_k}\vert} {\vert{D}\vert} })^2 $$</p><p><small>其中 $C_k$ 是 <code>D</code> 中属于第 <code>k</code> 类的样本子集，<code>K</code> 是类的个数。</small> </p><p>如果样本集合 <code>D</code> 根据特征 <code>A</code> 是否取某一个可能值 <code>a</code> 被分割为 <code>D1</code> 和 <code>D2</code>，即 $D1 &#x3D; {(x, y) \in D | A(x) \in a}, D2 &#x3D; D - D1$<br>则在特征 <code>A</code> 的条件下，集合 <code>D</code> 的基尼指数定义为：</p><p>$$ Gini(D, A) &#x3D; {\frac {\vert {D1} \vert} {\vert {D} \vert} } Gini(D1) + {\frac {\vert {D2} \vert} {\vert {D} \vert} } Gini(D2) $$</p><p><small>基尼指数 <code>Gini(D, A)</code> 表示 <code>A = a</code> 分割后集合 <code>D</code> 的不确定性。</small></p><p><small>基尼指数越大，样本集合的不稳定性则越大。</small></p><p><code>CART</code> 决策树基尼系数生成算法示例：</p><blockquote><p>输入：训练数据集 <code>D</code>，停止计算的条件<br>输出：<code>CART</code> 决策树<br>过程：</p><ol><li>根据训练数据集，根结点的数据集为 <code>D</code>，计算现有特征对该数据集的基尼指数，对于特征 <code>A</code> 有一个可能的取值 <code>a</code>，根据样本对 <code>A = a</code> 的测试为“是”或“否”，从而将数据集 <code>D</code> 拆分为 $D_1$ 和 $D_2$ 两部分，计算对应的基尼指数，</li><li>在计算特征 <code>A</code> 所有取值 <code>a</code> 对应的基尼指数后，选择其中基尼指数最小的特征作为对应的最优特征和最优切分点，至此就可以将数据集切分为两部分，生成两个子结点，将数据集分配到对应的子结点中。</li><li>对两个子结点递归地调用 <code>1, 2</code>，直至结点中的数据集个数小于预定阈值，或者基尼指数低于预定阈值，又或者没有足够的特征。</li><li>至此在满足所有条件后即可生成 <code>CART</code> 决策树。</li></ol></blockquote><hr><h3 id="决策树修剪"><a href="#决策树修剪" class="headerlink" title="决策树修剪"></a>决策树修剪</h3><p>决策树通过生成算法递归地产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确，但是却对未知的数据分类没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多的考虑如何提高针对训练数据的正确分类，从而构建出复杂的决策树，那么解决这个问题就是将复杂的决策树进行简单化。<br>在决策树的学习中将已生成的树进行简化的过程称为<strong>剪枝（<code>pruning</code>）</strong>，具体就是剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化树模型。</p><p>决策树的剪枝往往通过极小化决策树整体的 <em>损失函数（<code>lose function</code>）</em> 或 <em>代价函数（<code>cost function</code>）</em> 来实现。<br>设树 <code>T</code> 的叶结点个树为 ${\vert {T} \vert}$，<code>t</code> 是树 <code>T</code> 的叶结点，该叶结点有 $N_t$ 个样本点，其中 <code>k</code> 类的样本点有 $N_{tk}$ 个 <code>k = 1, 2, ..., K</code>，$H_t(T)$ 为叶结点 <code>t</code> 上的经验熵，<code>a &gt;= 0</code> 为参数，则决策树学习的损失函数可以定义为：</p><p>$$ C_a(T) &#x3D; \sum_{t&#x3D;1}^{\vert {T} \vert} N_t H_t (T) + a {\vert {T} \vert} $$</p><p>其中经验熵为：</p><p>$$ H_t(T) &#x3D; -\sum_k {\frac {N_{tk} } {N_t} } log {\frac {N_{tk} } {N_t} } $$</p><p>那么：</p><p>$$ C(T) &#x3D;  \sum_{t&#x3D;1}^{\vert {T} \vert} N_t H_t (T) &#x3D; -\sum_{t&#x3D;1}^{\vert {T} \vert} N_t \sum_{k&#x3D;1}^K  {\frac {N_{tk} } {N_t} } log {\frac {N_{tk} } {N_t} } &#x3D;  -\sum_{t&#x3D;1}^{\vert {T} \vert} \sum_{k&#x3D;1}^K {N_{tk} } log {\frac {N_{tk} } {N_t} } $$</p><p>即可得到：</p><p>$$ C_a(T) &#x3D; C(T) + a(T) $$</p><p>上述中 $C(T)$ 表示模型对训练数据的预测误差，即模型和训练数据的拟合程度，${\vert {T} \vert}$ 表示模型复杂度，参数 <code>a &gt;= 0</code> 控制两者之间的影响。那么较大的 <code>a</code> 就意味着选择较简单的决策树，较小的 <code>a</code> 选择较简单的决策树，<code>a = 0</code> 也就意味着只考虑决策树与训练数据的拟合程度，不考虑决策树的复杂度。<br>决策树剪枝也就是当 <code>a</code> 确定时，选择损失函数最小的决策树，即损失函数最小的子树，刚好通过使得损失函数最小化来平衡复杂的模型和训练数据的拟合程度。</p><p>下面就是具体的剪枝算法示例：</p><blockquote><p>输入；算法生成的决策树 <code>T</code>，参数 <code>a</code><br>输出：修剪后的子树 $T_a$ <br><br>过程：</p><ol><li>计算每个结点的经验熵。</li><li>递归地从树的叶结点向上回缩。<br> <img src="/ML-%E5%86%B3%E7%AD%96%E6%A0%91/decision_tree_2.jpg" alt="decision_tree-2"><br> 假设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_B$ 与 $T_A$，其对应的损失函数为 $C_a(T_B)$ 与 $C_a(T_A)$，如果 $ C_a(T_B) &lt;&#x3D; C_a(T_A) $ 则进行剪枝，即将父结点变为新的叶结点。</li><li>返回 <code>2</code>，直至不能继续为止，即可得到损失函数最小的子树 $T_a$。</li></ol></blockquote><h4 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a><code>CART</code> 剪枝</h4><p><code>CART</code> 剪枝算法从决策树底端剪去一些子树，使决策树变小，模型变简单，从而能够对未知数据更准确的预测。<br><code>CART</code> 剪枝算法由两步组成：</p><ul><li><p>首先从算法生成的决策树 $T_0$ 底端开始不断剪枝，直到 $T_0$ 的根结点，形成一个子树序列（$T_0$, $T_1$, …, $T_n$）。<br>  剪枝过程中，计算子树的损失函数：<br>  $$ C_a(T) &#x3D; C(T) + a{\vert {T} \vert} $$<br>  <small><code>T</code> 为任意子树，$C(T)$ 为对训练数据的预测误差，${\vert {T} \vert}$ 为子树的叶结点个树，<code>a &gt;= 0</code> 为参数，$C_a(T)$ 为参数是 <code>a</code> 时的子树 <code>T</code> 的整体损失。</small><br>  对于某个的 <code>a</code>，一定存在使损失函数 $C_a(T)$ 最小的子树 $T_a$，即 $T_a$ 在损失函数 $C_a(T)$ 最小时最优。当 <code>a</code> 大的时候，最优子树 $T_a$ 偏小；当 <code>a</code> 小的时候，最优子树 $T_a$ 偏大；而极端情况下 <code>a = 0</code>，整体树最优；当 <code>a -&gt; </code> $\infty$ 时，根结点组成的单结点树是最优的。</p><p>  具体从整树 $T_0$ 开始剪枝，对 $T_0$ 的任意内部结点 <code>t</code>，以 <code>t</code> 为单结点树的损失函数为：<br>  $$ C_a(T) &#x3D; C(t) + a $$</p><p>  以 <code>t</code> 为根结点的子树 $T_t$ 的损失函数为：<br>  $$ C_a(T_t) &#x3D; C(T_t) + a {\vert {T_t} \vert} $$ </p><ul><li>当 <code>a = 0</code> 及 <code>a</code> 充分小的时候，有不等式：$ C_a(T_t) &lt; C_a(t) $</li><li>当 <code>a</code> 增大时，在某一个 <code>a</code> 有：$ C_a(T_t) &#x3D; C_a(t) $</li><li>当 <code>a</code> 继续增大时，只要 $a &#x3D; {\frac {C(t) - C(T_t)} { {\vert {T_t}\vert} } - 1}$ 即 $T_t$ 与 <code>t</code> 具有相同的损失函数，而 <code>t</code> 的结点较少，因此 <code>t</code> 比 $T_t$ 更可取，对 $T_t$ 进行剪枝。</li></ul><p> 为此计算 $T_0$ 中每个内部结点 <code>t</code><br> $$ g(t) &#x3D; {\frac {C(t) - C(T_t)} { {\vert {T_t}\vert} - 1} }$$<br> 表示剪枝后整体损失函数减小的程度。</p><p> 在 $T_0$ 中剪去 <code>g(t)</code> 最小的子树 $T_t$，将得到子树作为 $T_1$，同时将最小的 <code>g(t)</code> 设为 $a_1$，$T_1$ 为区间 $[a_1, a_2)$ 的最优子树。如此剪枝下去直至根结点，而在这一过程中，不断递增 <code>a</code>，不断产生新的区间。</p></li><li><p>接着通过交叉验证法在独立的验证数据集上对子树序列进行测试，从而选出最优子树。<br>  利用验证数据集测试子树序列 $T_0, T_1, …, T_n$ 中各个子树的平均方差或基尼系数，平方误差或基尼系数最小的子树也就是最优的决策树，而当最优子树被确定时，子树对应的 <code>a</code> 也会被确定，即可得到最优子树 $T_a$。</p></li></ul><p>下面就是具体的 <code>CART</code> 剪枝算法示例：</p><blockquote><p>输入：<code>CART</code> 算法生成的决策树 $T_0$<br><br>输出：最优决策树 $T_a$<br><br>过程：</p><ol><li>设 $k &#x3D; 0, T &#x3D; T_0$。</li><li>设 <code>a </code> $ &#x3D; + \infty$。</li><li>自上而下地对各内部结点 <code>t</code> 计算 $C(T_t)$，${\vert {T_t} \vert}$ 以及<br>  $$ g(t) &#x3D; {\frac {C(t) - C(T_t)} { {\vert {T_t} \vert} - 1} } $$<br>  $$ a &#x3D; min(a, g(t)) $$<br> <small> $T_t$ 表示以 <code>t</code> 为根结点的子树，$C(T_t)$ 是对训练数据的预测误差，${\vert {T_t} \vert}$ 是 $T_t$ 的叶结点个树。</small></li><li>自上而下地访问内部结点 <code>t</code>，如果有 <code>g(t) = a</code>，则进行剪枝并对叶结点 <code>t</code> 以多数表决法决定其类，得到树 <code>T</code>。</li><li>设 $k &#x3D; k + 1, a_k &#x3D;$ <code> a</code> $, T_k &#x3D; T$。</li><li>如果 <code>T</code> 不是由根结点单独组成的树，则返回 <code>4</code>。</li><li>采用交叉验证法在子树序列 $T_0, T_1, …, T_n$ 中选取最优子树 $T_a$。</li></ol></blockquote><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>分类决策树模型是用于表示基于特征对实例进行分类的树形结构体，决策树可以转换为 <code>if-then</code> 规则集合，也可以看作是定义在特征空间上划分的条件概率分布。</li><li>决策树学习宗旨是构建一个可以对训练数据很好的分类，但同时复杂度较小的决策树。<br> 决策树学习算法包含三部分：特征选择、树的生成、树的剪枝。<br> 常用的算法：<code>ID3</code>、 <code>C4.5</code>、 <code>CART</code>。</li><li>特征选择的目的在于能够选取对实例分类有用的特征，常用的准则如下：<ul><li>样本集合 <code>D</code> 对特征 <code>A</code> 的信息增益：<br> $$ g(D, A) &#x3D; H(D) - H(D|A) $$<br> $$ H(D) &#x3D; - \sum_{k&#x3D;1}^K {\frac {\vert {C_k}\vert} {\vert {D} \vert} } log_2 {\frac {\vert {C_k}\vert} {\vert {D} \vert} } $$<br> $$ H(D|A) &#x3D; \sum_{i&#x3D;1}^n {\frac {\vert {D_i} \vert} {\vert {D} \vert} H(D_i) } $$</li><li>样本集合 <code>D</code> 对特征 <code>A</code> 的信息增益比：<br> $$ g_r(D, A) &#x3D; {\frac {g(D, A)} {H(D)} } $$</li><li>样本集合 <code>D</code> 的基尼指数：<br> $$ Gini(D) &#x3D; 1 - \sum_{k&#x3D;1}^K ({\frac {\vert {C_k} \vert} {\vert {D} \vert} })^2 $$<br> 特征 <code>A</code> 条件下集合 <code>D</code> 的基尼指数：<br> $$  Gini(D, A) &#x3D; {\frac {\vert {D1} \vert} {\vert {D} \vert} } Gini(D1) + {\frac {\vert {D2} \vert} {\vert {D} \vert} } Gini(D2) $$</li></ul></li><li>决策树的生成同时是采用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则，然后从根结点开始，通过特征选择准则划分正确的子集，递归地生成决策树。</li><li>生成决策树的过程中会产生过拟合问题，因此需要剪枝，从已生成的决策树上剪掉一些叶结点或者子结点，并将其父结点作为新的叶结点，从而简化决策树。</li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习《统计学习方法》所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概要&quot;&gt;&lt;a href=&quot;#概要&quot; class=&quot;headerlink&quot; title=&quot;概要&quot;&gt;&lt;/a&gt;概要&lt;/h3&gt;&lt;p&gt;决策树（&lt;code&gt;decision tree&lt;/code&gt;）是一种基本的&lt;strong&gt;分类和回归方法&lt;/strong&gt;。其主要呈现为&lt;strong&gt;树状结构&lt;/strong&gt;，在分类问题中，表示基于特征对实例进行分类的过程，可以被认为是 &lt;strong&gt;&lt;code&gt;if-then&lt;/code&gt; 的规则集合&lt;/strong&gt;，也可以被认为是定义在&lt;strong&gt;特征空间与类空间上的条件概率分布&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其优点主要有分类速度快、模型具有可读性，在学习时利用训练数据根据&lt;strong&gt;损失函数最小化&lt;/strong&gt;的原则建立决策树模型；而在预测时对新的数据利用&lt;strong&gt;决策树模型&lt;/strong&gt;进行分类。&lt;/p&gt;
&lt;p&gt;决策树模型主要包含以下步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征选择&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;决策树的生成&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;决策树的修剪&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实践-Pima 数据集</title>
    <link href="https://blog.vgbhfive.cn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://blog.vgbhfive.cn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/</id>
    <published>2023-07-17T15:30:07.000Z</published>
    <updated>2023-07-19T14:20:40.521Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h3><p>该数据集最初来自糖尿病&#x2F;消化&#x2F;肾脏疾病研究所，此<a href="https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database">数据集</a>的目标是基于数据集中包含的某些身体指标来诊断性的预测患者是否患有糖尿病。<br>数据集由多个医学指标和一个目标变量 <code>Outcome</code> 组成，医学指标包含患者的怀孕次数、<code>BMI</code> 指数、胰岛素水平、年龄、血压等。</p><span id="more"></span><hr><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h4 id="导入基础依赖"><a href="#导入基础依赖" class="headerlink" title="导入基础依赖"></a>导入基础依赖</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure><h4 id="导入数据查看基础信息"><a href="#导入数据查看基础信息" class="headerlink" title="导入数据查看基础信息"></a>导入数据查看基础信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pima = pd.read_csv(<span class="string">&quot;diabetes.csv&quot;</span>)</span><br><span class="line"><span class="comment"># pima.head()</span></span><br><span class="line"><span class="comment"># pima.info()</span></span><br><span class="line"><span class="comment"># pima.shape</span></span><br><span class="line">pima.describe()</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/1.png" alt="1"></p><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 柱状图</span></span><br><span class="line">pima.hist(figsize=(<span class="number">16</span>, <span class="number">14</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 散点图</span></span><br><span class="line">sns.pairplot(pima, hue=<span class="string">&quot;Outcome&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 箱图</span></span><br><span class="line">pima.plot(kind=<span class="string">&quot;box&quot;</span>, subplots=<span class="literal">True</span>, layout=(<span class="number">3</span>,<span class="number">3</span>), sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, figsize=(<span class="number">16</span>,<span class="number">14</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 热点图</span></span><br><span class="line">column_x = pima.columns[<span class="number">0</span>: <span class="built_in">len</span>(pima.columns)-<span class="number">1</span>]</span><br><span class="line">column_x</span><br><span class="line">corr = pima[pima.columns].corr()</span><br><span class="line">plt.subplots(figsize=(<span class="number">16</span>,<span class="number">14</span>))</span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest </span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"> </span><br><span class="line">X = pima.iloc[:, <span class="number">0</span>:<span class="number">8</span>] <span class="comment"># 特征列 0-7列，不含第8列。</span></span><br><span class="line">Y = pima.iloc[:, <span class="number">8</span>] <span class="comment"># 目标列为第8列</span></span><br><span class="line"><span class="comment"># iloc([rows], [cols]) 第一个参数为要截取的行，第二个参数为要截取的列。loc([rows], [cols_name]) 第一个参数为要截取的行，第二个参数为要截取的列名称</span></span><br><span class="line"></span><br><span class="line">select_top_4 = SelectKBest(score_func=chi2, k =<span class="number">4</span>) <span class="comment"># 通过卡方检验选择4个得分最高的特征</span></span><br><span class="line">fits = select_top_4.fit(X, Y) <span class="comment">#将特征输入到评分函数，获取特征信息和目标值信息</span></span><br><span class="line">features = fits.transform(X) <span class="comment">#展现特征转换后的结果</span></span><br><span class="line">features[<span class="number">0</span>:<span class="number">5</span>] <span class="comment">#新特征列</span></span><br><span class="line"></span><br><span class="line">pima.head() <span class="comment"># 表现最佳的特征为 Glucose， Insulin， BMI， Age</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造新的 DataFrame</span></span><br><span class="line">x_features = pd.DataFrame(data=features, columns=[<span class="string">&#x27;Glucose&#x27;</span>, <span class="string">&#x27;Insulin&#x27;</span>, <span class="string">&#x27;BMI&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>])</span><br><span class="line">x_features</span><br><span class="line">y_features = pd.DataFrame(data=Y, columns=[<span class="string">&#x27;Outcome&#x27;</span>])</span><br><span class="line">y_features</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/2.png" alt="2"><br><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/3.png" alt="3"></p><h4 id="特征标准化"><a href="#特征标准化" class="headerlink" title="特征标准化"></a>特征标准化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将属性值更改为 均值为0，标准差为1 的 高斯分布. 当算法期望输入特征处于高斯分布时，它非常有用</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">rescaled_x = StandardScaler().fit_transform(x_features)</span><br><span class="line">x = pd.DataFrame(data=rescaled_x, columns=x_features.columns)</span><br><span class="line">x.head()</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/4.png" alt="4"></p><h4 id="数据切分"><a href="#数据切分" class="headerlink" title="数据切分"></a>数据切分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y_features, test_size=test_size, random_state=seed)</span><br><span class="line">x_train.head()</span><br><span class="line">y_train.head()</span><br></pre></td></tr></table></figure><h4 id="构建二分类算法模型"><a href="#构建二分类算法模型" class="headerlink" title="构建二分类算法模型"></a>构建二分类算法模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((<span class="string">&quot;LB&quot;</span>, LogisticRegression()))</span><br><span class="line">models.append((<span class="string">&quot;NB&quot;</span>, GaussianNB()))</span><br><span class="line">models.append((<span class="string">&quot;KNN&quot;</span>, KNeighborsClassifier()))</span><br><span class="line">models.append((<span class="string">&quot;DT&quot;</span>, DecisionTreeClassifier()))</span><br><span class="line">models.append((<span class="string">&quot;SVM&quot;</span>, SVC()))</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">names = []</span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">22</span>)</span><br><span class="line">    cv_result = cross_val_score(model, x_train, y_train, cv=kfold, scoring=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">    names.append(name)</span><br><span class="line">    results.append(cv_result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(names)):</span><br><span class="line">    <span class="built_in">print</span>(names[i], results[i].mean())</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/5.png" alt="5"></p><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">classifier = SVC(kernel=<span class="string">&quot;rbf&quot;</span>)</span><br><span class="line">classifier.fit(x_train, y_train)</span><br><span class="line">y_pred = classifier.predict(x_test)</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred) <span class="comment"># 混淆矩阵</span></span><br><span class="line">cm</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred)) <span class="comment"># 显示准确率</span></span><br><span class="line"></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f%%&quot;</span> % (accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/6.png" alt="6"><br><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/7.png" alt="7"><br><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/8.png" alt="8"></p><h4 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用”网格搜索“来提高模型 - 模型优化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>]&#125;</span><br><span class="line">grid = GridSearchCV(SVC(), param_grid, refit=<span class="literal">True</span>, verbose=<span class="number">2</span>)</span><br><span class="line">grid.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">grid_prediction = grid.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, grid_prediction))</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/9.png" alt="9"></p><hr><h3 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h3><h4 id="评分函数"><a href="#评分函数" class="headerlink" title="评分函数"></a>评分函数</h4><ol><li><p><code>SelectKBest()</code><br>只保留 <code>K</code> 个最高分的特征，能够返回特征评价的得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SelectKBest(score_func=&lt;function f_classif&gt;, k=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>SelectPercentile()</code><br>只保留用户指定百分比的最高得分的特征，能够返回特征评价的得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SelectPercentile(score_func=&lt;function f_classif&gt;, percentile=<span class="number">10</span>) </span><br></pre></td></tr></table></figure><p>使用常见的单变量统计检验：假正率 <code>SelectFpr</code>，错误发现率 <code>SelectFdr</code>，或者总体错误率 <code>SelectFwe</code>。</p></li><li><p><code>GenericUnivariateSelect()</code><br>通过结构化策略进行特征选择，通过超参数搜索估计器进行特征选择。</p></li></ol><h4 id="cross-val-score-函数"><a href="#cross-val-score-函数" class="headerlink" title="cross_val_score() 函数"></a><code>cross_val_score()</code> 函数</h4><p><code>cross_val_score()</code> 函数，交叉验证评分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cross_validation.cross_val_score(estimator, X, y=<span class="literal">None</span>, scoring=<span class="literal">None</span>, cv=<span class="literal">None</span>, n_jobs=<span class="number">1</span>, verbose=<span class="number">0</span>, fit_params=<span class="literal">None</span>, pre_dispatch=<span class="string">&#x27;2*n_jobs&#x27;</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>estimator</code>：数据对象 </li><li><code>X</code>：数据 </li><li><code>y</code>：预测数据 </li><li><code>soring</code>：调用的方法</li><li><code>cv</code>：交叉验证生成器或可迭代的次数 </li><li><code>n_jobs</code>：同时工作的 <code>cpu</code> 个数（<code>-1</code> 代表全部）</li><li><code>verbose</code>：详细程度</li><li><code>fit_params</code>：传递给估计器的拟合方法的参数</li><li><code>pre_dispatch</code>：控制并行执行期间调度的作业数量</li></ul><h4 id="KFold-交叉验证"><a href="#KFold-交叉验证" class="headerlink" title="KFold 交叉验证"></a><code>KFold</code> 交叉验证</h4><p><code>K</code> 折交叉验证，将数据集分成 <code>K</code> 份的官方给定方案，所谓 <code>K</code> 折就是将数据集通过 <code>K</code> 次分割，使得所有数据既在训练集出现过，又在测试集出现过，当然每次分割中不会有重叠，相当于无放回抽样。<br><code>StratifiedKFold</code> 用法类似 <code>Kfold</code>，但是他是分层采样，确保训练集，测试集中各类别样本的比例与原始数据集中相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.KFold(n_splits=<span class="number">3</span>, shuffle=<span class="literal">False</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>将训练&#x2F;测试数据集划分 <code>n_splits</code> 个互斥子集，每次用其中一个子集当作验证集，剩下的 <code>n_splits-1</code> 个作为训练集，进行 <code>n_splits</code> 次训练和测试，得到 <code>n_splits</code> 个结果。</p><p>参数说明：</p><ul><li><code>n_splits</code>：表示划分几等份</li><li><code>shuffle</code>：在每次划分时，是否进行洗牌<ul><li>若为 <code>False</code> 时，其效果等同于 <code>random_state</code> 等于整数，每次划分的结果相同。</li><li>若为 <code>True</code> 时，每次划分的结果都不一样，表示经过洗牌，随机取样的。</li></ul></li><li><code>random_state</code>：随机种子数</li></ul><p><small>注意点：对于不能均等份的数据集，其前 <code>n_samples % n_splits</code> 子集拥有 <code>n_samples // n_splits + 1</code> 个样本，其余子集都只有 <code>n_samples // n_splits</code> 样本。</small></p><h4 id="LeaveOneOut-留一法"><a href="#LeaveOneOut-留一法" class="headerlink" title="LeaveOneOut 留一法"></a><code>LeaveOneOut</code> 留一法</h4><p><code>LeaveOneOut</code> 留一法，每一回合中，几乎所有的样本都用于训练模型，因此最接近原始样本的分布，这样的评估所得的结果比较可靠。实验过程中，没有随机因素会影响实验数据，确保实验过程是可以被复制的。<br>但是 <code>LeaveOneOut</code> 也有明显的缺点，就是计算成本高，当原始样本数很多时，需要花费大量的时间去完成算法的运算与评估。</p><h4 id="SVC-函数"><a href="#SVC-函数" class="headerlink" title="SVC() 函数"></a><code>SVC()</code> 函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;auto&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>,tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, max_iter=-<span class="number">1</span>, decision_function_shape=<span class="literal">None</span>,random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>C</code>：<code>C-SVC</code> 的惩罚参数 <code>C</code>，默认值是 <code>1.0</code>。<br>  <code>C</code> 越大，相当于惩罚松弛变量，希望松弛变量接近 <code>0</code>，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。<br>  <code>C</code> 值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。</li><li><code>kernel</code>：核函数，默认是 <code>rbf</code>，可以是 <code>linear</code>、 <code>poly</code>、 <code>rbf</code>、 <code>sigmoid</code>、 <code>precomputed</code><ul><li>线性：<code>u&#39;v</code></li><li>多项式：<code>(gamma*u&#39;*v + coef0)^degree</code></li><li><code>RBF</code> 函数：<code>exp(-gamma|u-v|^2)</code></li><li><code>sigmoid</code>：<code>tanh(gamma*u&#39;*v + coef0)</code></li></ul></li><li><code>degree</code>：多项式 <code>poly</code> 函数的维度，默认是 <code>3</code>，选择其他核函数时会被忽略</li><li><code>gamma</code>： <code>rbf</code>、 <code>poly</code> 和 <code>sigmoid</code> 的核函数参数，默认是 <code>auto</code>，则会选择 <code>1/n_features</code></li><li><code>coef0</code>：核函数的常数项，对于 <code>poly </code>和 <code>sigmoid</code> 有用</li><li><code>probability</code>：是否采用概率估计，默认为 <code>False</code></li><li><code>shrinking</code>：是否采用 <code>shrinking heuristic</code> 方法，默认为 <code>True</code></li><li><code>tol</code>：停止训练的误差值大小，默认为 <code>1e-3</code></li><li><code>cache_size</code>：核函数 <code>cache</code> 缓存大小，默认为 <code>200</code></li><li><code>class_weight</code>：类别的权重，字典形式传递。设置第几类的参数 <code>C</code> 为 <code>weight*C</code></li><li><code>verbose</code>：是否允许冗余输出</li><li><code>max_iter</code>：最大迭代次数，<code>-1</code> 为无限制</li><li><code>decision_function_shape</code>：<code>ovo</code>、 <code>ovr</code> 或者 <code>None</code>，默认值为 <code>None</code></li><li><code>random_state</code>：数据洗牌时的种子值</li></ul><p>主要调节的参数有：<code>C</code>、 <code>kernel</code>、 <code>degree</code>、 <code>gamma</code>、 <code>coef0</code>。</p><h4 id="confusion-matrix-函数"><a href="#confusion-matrix-函数" class="headerlink" title="confusion_matrix() 函数"></a><code>confusion_matrix()</code> 函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.confusion_matrix(y_true, y_pred, labels=<span class="literal">None</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>y_true</code>：是样本真实分类结果</li><li><code>y_pred</code>：是样本预测分类结果</li><li><code>labels</code>：是所给出的类别，通过这个可对类别进行选择 </li><li><code>sample_weight</code> : 样本权重</li></ul><h4 id="classification-report-预测准确率"><a href="#classification-report-预测准确率" class="headerlink" title="classification_report() 预测准确率"></a><code>classification_report()</code> 预测准确率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report(y_true, y_pred, *, labels=<span class="literal">None</span>, target_names=<span class="literal">None</span>, sample_weight=<span class="literal">None</span>, digits=<span class="number">2</span>, output_dict=<span class="literal">False</span>, zero_division=<span class="string">&#x27;warn&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-Pima-%E6%95%B0%E6%8D%AE%E9%9B%86/7.png" alt="7"><br>输出说明：</p><ul><li><code>precision</code>: 准确率，<code>TP/ (TP+FP) </code></li><li><code>recall</code>: 召回率，<code>TP(TP + FN)</code></li><li><code>f1-score</code>: 是准确率与召回率的综合，可以认为是平均结果，<code>2*TP/(2*TP + FP + FN)</code><ul><li><code>TP</code>: 预测为正，实现为正</li><li><code>FP</code>: 预测为正，实现为负</li><li><code>FN</code>: 预测为负，实现为正</li><li><code>TN</code>: 预测为负，实现为负</li></ul></li></ul><h4 id="GridSearchCV"><a href="#GridSearchCV" class="headerlink" title="GridSearchCV()"></a><code>GridSearchCV()</code></h4><p><code>GridSearchCV()</code> 可以拆分为两部分 <code>GridSearch</code> 和 <code>CV</code>，即网格搜索和交叉验证。网格搜索，指的是参数，即在指定的参数范围内，按步长依次调整参数，利用调整的参数训练学习器，从所有的参数中找到在验证集上精度最高的参数，这其实是一个训练和比较的过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, refit=<span class="literal">True</span>, cv=<span class="literal">None</span>, verbose=<span class="number">0</span>, pre_dispatch=<span class="string">&#x27;2*n_jobs&#x27;</span>, error_score=nan, return_train_score=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>estimator</code>：选择使用的分类器，并且传入除需要确定最佳的参数之外的其他参数。并且每一个分类器都需要一个 <code>scoring</code> 参数或者 <code>score</code> 方法。</li><li><code>param_grid</code>：需要最优化的参数的取值，值为字典或者列表。</li><li><code>scoring=None</code>：模型评价标准，默认 <code>None</code>。这时需要使用 <code>score</code> 函数，根据所选模型不同，评价准则不同。</li><li><code>n_jobs</code>：并行数，<code>-1</code> 跟 <code>CPU</code> 核数一致。</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>机器学习总体来说不像编程需要很强的计算机基础知识，但额外需要了解业务方面的知识以及对所使用包的熟悉程度，因此还是那句老话“<strong>实践出真知</strong>”。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;数据简介&quot;&gt;&lt;a href=&quot;#数据简介&quot; class=&quot;headerlink&quot; title=&quot;数据简介&quot;&gt;&lt;/a&gt;数据简介&lt;/h3&gt;&lt;p&gt;该数据集最初来自糖尿病&amp;#x2F;消化&amp;#x2F;肾脏疾病研究所，此&lt;a href=&quot;https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database&quot;&gt;数据集&lt;/a&gt;的目标是基于数据集中包含的某些身体指标来诊断性的预测患者是否患有糖尿病。&lt;br&gt;数据集由多个医学指标和一个目标变量 &lt;code&gt;Outcome&lt;/code&gt; 组成，医学指标包含患者的怀孕次数、&lt;code&gt;BMI&lt;/code&gt; 指数、胰岛素水平、年龄、血压等。&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.vgbhfive.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Docker install jupyter-notebook</title>
    <link href="https://blog.vgbhfive.cn/Docker-install-jupyter-notebook/"/>
    <id>https://blog.vgbhfive.cn/Docker-install-jupyter-notebook/</id>
    <published>2023-06-23T03:31:59.000Z</published>
    <updated>2023-06-23T04:13:23.268Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><code>Jupyter Notebook</code> 是一个开源的 <code>Web</code> 应用程序，允许用户创建和共享包含代码、方程式、可视化和文本的文档。主要用于 <strong>数据清理和转换</strong>、<strong>数值模拟</strong>、<strong>统计建模</strong>、<strong>数据可视化</strong>、<strong>机器学习</strong> 等等。<br>具有以下优势：</p><ul><li>可选择语言：支持超过 <code>40</code> 种编程语言，包括 <code>Python</code>、<code>R</code>、<code>Julia</code>、<code>Scala</code> 等。</li><li>分享笔记本：可以使用电子邮件、<code>Dropbox</code>、<code>GitHub</code> 和 <code>Jupyter Notebook Viewer</code> 与他人共享。</li><li>交互式输出：代码可以生成丰富的交互式输出，包括 <code>HTML</code>、图像、视频、<code>LaTeX</code> 等等。</li><li>大数据整合：通过 <code>Python</code>、<code>R</code>、<code>Scala</code> 编程语言使用 <code>Apache Spark</code> 等大数据框架工具。支持使用 <code>pandas</code>、<code>scikit-learn</code>、<code>ggplot2</code>、<code>TensorFlow</code> 来探索同一份数据。</li></ul><span id="more"></span><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker search jupyter</span></span><br><span class="line">NAME                                 DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">jupyter/scipy-notebook               Scientific Jupyter Notebook Python Stack fro…   404</span><br><span class="line">jupyter/tensorflow-notebook          Scientific Jupyter Notebook Python Stack w/ …   344</span><br><span class="line">jupyter/all-spark-notebook           Python, Scala, R and Spark Jupyter Notebook …   417</span><br><span class="line">jupyter/pyspark-notebook             Python and Spark Jupyter Notebook Stack from…   277</span><br><span class="line">jupyter/datascience-notebook         Data Science Jupyter Notebook Python Stack f…   1027</span><br><span class="line">jupyterhub/singleuser                single-user docker images for use with Jupyt…   45                   [OK]</span><br><span class="line">jupyterhub/jupyterhub                JupyterHub: multi-user Jupyter notebook serv…   326                  [OK]</span><br><span class="line">jupyter/minimal-notebook             Minimal Jupyter Notebook Python Stack from h…   183</span><br><span class="line">jupyter/base-notebook                Base image for Jupyter Notebook stacks from …   203</span><br><span class="line">jupyterhub/k8s-hub                                                                   22</span><br><span class="line">jupyterhub/k8s-network-tools                                                         2</span><br><span class="line">jupyterhub/configurable-http-proxy   node-http-proxy + REST API                      6                    [OK]</span><br><span class="line">jupyter/nbviewer                     Jupyter Notebook Viewer                         32                   [OK]</span><br><span class="line">jupyterhub/k8s-singleuser-sample                                                     10</span><br><span class="line">jupyter/r-notebook                   R Jupyter Notebook Stack from https://github…   54</span><br><span class="line">jupyterhub/k8s-image-awaiter                                                         2</span><br><span class="line">jupyter/repo2docker                  Turn git repositories into Jupyter enabled D…   21</span><br><span class="line">jupyterhub/k8s-secret-sync                                                           1</span><br><span class="line">jupyterhub/jupyterhub-onbuild        onbuild version of JupyterHub images            6</span><br><span class="line">jupyter/demo                         (DEPRECATED) Demo of the IPython/Jupyter Not…   16</span><br><span class="line">bitnami/jupyter-base-notebook                                                        39</span><br><span class="line">jupyterhub/k8s-image-cleaner                                                         1</span><br><span class="line">jupyterhub/k8s-binderhub                                                             3</span><br><span class="line">jupyterhub/k8s-pre-puller                                                            1</span><br><span class="line">bitnami/jupyterhub                                                                   18</span><br></pre></td></tr></table></figure><h4 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">dockcer pull jupyter/datascience-notebook</span></span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from jupyter/datascience-notebook</span><br><span class="line">d5fd17ec1767: Pull complete </span><br><span class="line">9288915018bc: Pull complete </span><br><span class="line">ad895732ee5c: Pull complete </span><br><span class="line">4f4fb700ef54: Pull complete </span><br><span class="line">7b053b0d567d: Pull complete </span><br><span class="line">577d6f3bb6f4: Pull complete </span><br><span class="line">1d18b5a5f242: Pull complete </span><br><span class="line">ffc9ad0a0b36: Pull complete </span><br><span class="line">75f3e04b1547: Pull complete </span><br><span class="line">e9036ae1aec3: Pull complete </span><br><span class="line">8016e50184c6: Pull complete </span><br><span class="line">55f4c93ee7b8: Pull complete </span><br><span class="line">17c3e54db24b: Pull complete </span><br><span class="line">e8c81a9b6c9a: Pull complete</span><br><span class="line">530f1db1e9d7: Pull complete </span><br><span class="line">44fa9360bdc5: Pull complete </span><br><span class="line">6f59df66069f: Pull complete </span><br><span class="line">a8c1c1bcf1d4: Pull complete </span><br><span class="line">5784e3ca1d66: Pull complete </span><br><span class="line">60e2c9b0e0a4: Pull complete </span><br><span class="line">4866b0f6598a: Pull complete </span><br><span class="line">613fc67c0714: Pull complete </span><br><span class="line">2a41639ceb55: Pull complete </span><br><span class="line">fa391f2a4b79: Pull complete </span><br><span class="line">Digest: sha256:acd52864dd364e2e5c494ccefa661e6f58f551c9006ec7263d7b5afd5d1852e9</span><br><span class="line">Status: Downloaded newer image for jupyter/datascience-notebook:latest</span><br><span class="line">docker.io/jupyter/datascience-notebook:latest</span><br></pre></td></tr></table></figure><h4 id="启动镜像"><a href="#启动镜像" class="headerlink" title="启动镜像"></a>启动镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run -d -p 8000:8888 –name jupyter-notebook jupyter/datascience-notebook</span></span><br><span class="line">xxxxxxxxx</span><br></pre></td></tr></table></figure><h4 id="部署代理"><a href="#部署代理" class="headerlink" title="部署代理"></a>部署代理</h4><p>服务部署完毕后，通过 <code>Nginx</code> 反向代理到公网访问，下面是示例配置。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen          80;</span><br><span class="line">    server_name     jupyter.vgbhfive.com;</span><br><span class="line">    index           index.html;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass      http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header Host http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ /api/kernels/ &#123;</span><br><span class="line">        proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">        proxy_http_version    1.1;  # websocket support</span><br><span class="line">        proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">        proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">        proxy_read_timeout    86400;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ /terminals/ &#123;</span><br><span class="line">        proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">        proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">        proxy_http_version    1.1;  # websocket support</span><br><span class="line">        proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">        proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">        proxy_read_timeout    86400;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="页面访问"><a href="#页面访问" class="headerlink" title="页面访问"></a>页面访问</h4><img src="/Docker-install-jupyter-notebook/docker-1111.png" class="" title="docker-1111"><p>第一次访问建议通过 <code>Token</code> 访问并设置，之后就只需要密码即可进入。</p><hr><h3 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h3><h4 id="403-GET-api-kernels"><a href="#403-GET-api-kernels" class="headerlink" title="403 GET /api/kernels"></a><code>403 GET /api/kernels</code></h4><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[W 2023-06-21 08:28:50.567 ServerApp] 403 GET /api/contents/demo.ipynb/checkpoints?1687335545679 (@172.17.0.1) 1.28ms referer=http://jupyter.vgbhfive.com/lab/tree/demo.ipynb</span><br><span class="line">[I 2023-06-21 08:32:19.956 ServerApp] Connecting to kernel 66e74bf7-4e19-40cc-8437-1eb43b35e208.</span><br><span class="line">[I 2023-06-21 08:32:19.956 ServerApp] Restoring connection for 66e74bf7-4e19-40cc-8437-1eb43b35e208:35bb8ace-627e-4574-b476-08fae23ccaa5</span><br><span class="line">[W 2023-06-21 08:33:49.957 ServerApp] WebSocket ping timeout after 90000 ms.</span><br><span class="line">[I 2023-06-21 08:33:54.961 ServerApp] Starting buffering for 66e74bf7-4e19-40cc-8437-1eb43b35e208:35bb8ace-627e-4574-b476-08fae23ccaa5</span><br><span class="line">[W 2023-06-21 08:34:04.639 ServerApp] wrote error: &#x27;Forbidden&#x27;</span><br><span class="line">    Traceback (most recent call last):</span><br><span class="line">      File &quot;/opt/conda/lib/python3.11/site-packages/tornado/web.py&quot;, line 1784, in _execute</span><br><span class="line">        result = method(*self.path_args, **self.path_kwargs)</span><br><span class="line">                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span><br><span class="line">      File &quot;/opt/conda/lib/python3.11/site-packages/tornado/web.py&quot;, line 3278, in wrapper</span><br><span class="line">        url = self.get_login_url()</span><br><span class="line">              ^^^^^^^^^^^^^^^^^^^^</span><br><span class="line">      File &quot;/opt/conda/lib/python3.11/site-packages/jupyter_server/base/handlers.py&quot;, line 753, in get_login_url</span><br><span class="line">        raise web.HTTPError(403)</span><br><span class="line">    tornado.web.HTTPError: HTTP 403: Forbidden</span><br></pre></td></tr></table></figure><p>根据 <code>403</code> 状态码的解释，服务器已经解析请求但没有权限访问资源，那么问题就来了，在查了无数资料之后，看到了这个东西：</p><img src="/Docker-install-jupyter-notebook/docker-2222.png" class="" title="docker-2222"><p>该问题是由于在 <code>Nginx</code> 代理请求时，将所有的请求代理为 <code>HTTP</code> 请求，而 <code>Jupyter Notebook</code> 的部分请求为 <code>WebSocket</code>，从而导致请求异常，修改 <code>Nginx</code> 代理之后问题就迎刃而解。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">location ~ /api/kernels/ &#123;</span><br><span class="line">    proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">    proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">    proxy_http_version    1.1;  # websocket support</span><br><span class="line">    proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">    proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">    proxy_read_timeout    86400;</span><br><span class="line">&#125;</span><br><span class="line">location ~ /terminals/ &#123;</span><br><span class="line">    proxy_pass            http://127.0.0.1:8000;</span><br><span class="line">    proxy_set_header      Host $host;</span><br><span class="line"></span><br><span class="line">    proxy_http_version    1.1;  # websocket support</span><br><span class="line">    proxy_set_header      Upgrade &quot;websocket&quot;;</span><br><span class="line">    proxy_set_header      Connection &quot;Upgrade&quot;;</span><br><span class="line">    proxy_read_timeout    86400;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="https://stackoverflow.com/questions/76094881/jupyter-docker-image-kernel-disconnected-400-apache-proxy">Jupyter Docker Image Kernel Disconnected 400 Apache Proxy</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Jupyter Notebook&lt;/code&gt; 是一个开源的 &lt;code&gt;Web&lt;/code&gt; 应用程序，允许用户创建和共享包含代码、方程式、可视化和文本的文档。主要用于 &lt;strong&gt;数据清理和转换&lt;/strong&gt;、&lt;strong&gt;数值模拟&lt;/strong&gt;、&lt;strong&gt;统计建模&lt;/strong&gt;、&lt;strong&gt;数据可视化&lt;/strong&gt;、&lt;strong&gt;机器学习&lt;/strong&gt; 等等。&lt;br&gt;具有以下优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可选择语言：支持超过 &lt;code&gt;40&lt;/code&gt; 种编程语言，包括 &lt;code&gt;Python&lt;/code&gt;、&lt;code&gt;R&lt;/code&gt;、&lt;code&gt;Julia&lt;/code&gt;、&lt;code&gt;Scala&lt;/code&gt; 等。&lt;/li&gt;
&lt;li&gt;分享笔记本：可以使用电子邮件、&lt;code&gt;Dropbox&lt;/code&gt;、&lt;code&gt;GitHub&lt;/code&gt; 和 &lt;code&gt;Jupyter Notebook Viewer&lt;/code&gt; 与他人共享。&lt;/li&gt;
&lt;li&gt;交互式输出：代码可以生成丰富的交互式输出，包括 &lt;code&gt;HTML&lt;/code&gt;、图像、视频、&lt;code&gt;LaTeX&lt;/code&gt; 等等。&lt;/li&gt;
&lt;li&gt;大数据整合：通过 &lt;code&gt;Python&lt;/code&gt;、&lt;code&gt;R&lt;/code&gt;、&lt;code&gt;Scala&lt;/code&gt; 编程语言使用 &lt;code&gt;Apache Spark&lt;/code&gt; 等大数据框架工具。支持使用 &lt;code&gt;pandas&lt;/code&gt;、&lt;code&gt;scikit-learn&lt;/code&gt;、&lt;code&gt;ggplot2&lt;/code&gt;、&lt;code&gt;TensorFlow&lt;/code&gt; 来探索同一份数据。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Docker" scheme="https://blog.vgbhfive.cn/tags/Docker/"/>
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>北京同仁医院验光攻略</title>
    <link href="https://blog.vgbhfive.cn/%E5%8C%97%E4%BA%AC%E5%90%8C%E4%BB%81%E5%8C%BB%E9%99%A2%E9%AA%8C%E5%85%89%E6%94%BB%E7%95%A5/"/>
    <id>https://blog.vgbhfive.cn/%E5%8C%97%E4%BA%AC%E5%90%8C%E4%BB%81%E5%8C%BB%E9%99%A2%E9%AA%8C%E5%85%89%E6%94%BB%E7%95%A5/</id>
    <published>2023-06-22T04:33:02.000Z</published>
    <updated>2023-06-23T03:31:28.743Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前要简介"><a href="#前要简介" class="headerlink" title="前要简介"></a>前要简介</h3><ol><li>首都医科大学附属北京同仁医院始建于 <code>1886</code> 年，是一所以眼科学、耳鼻咽喉科学为国家重点学科的大型综合三甲医院，对于眼科相关绝对是权威专业。</li><li>一般的眼镜店对于只验光不配镜的顾客有多多少少的抵触。</li><li>镜片和镜框需要单独在网上购买，接着找眼镜店帮忙组装，可以最大化保证钱花在刀刃上。</li></ol><span id="more"></span><hr><h3 id="预约挂号"><a href="#预约挂号" class="headerlink" title="预约挂号"></a>预约挂号</h3><ol><li><p>微信公众号搜索 <strong>北京同仁验光配镜中心服务号</strong> 并关注，该机构是同仁医院下属的营业性机构，其专业性和规范性由同仁医院保证，可以放心。</p></li><li><p>点击公众号的右下角 <strong>验光服务</strong>，首次预约会先填一个预约人的信息，填写完毕后选择对应的预约人，接下来就是选择店铺，推荐 <strong>中心店</strong> 即可。</p></li><li><p>紧跟着根据自己的日期规划选择对应的 <strong>视光服务费W</strong>，在完成缴费后即预约成功。</p></li></ol><hr><h3 id="行程"><a href="#行程" class="headerlink" title="行程"></a>行程</h3><p>目的地位于 <code>2</code> 号线和 <code>5</code> 号线的换乘地铁站 <strong>崇文门地铁站</strong>，到站后从 <strong><code>E</code></strong> 口出战，出站后 <strong>站口方向过马路（不过天桥）直行 <code>100</code> 米</strong> 就是目的地。</p><img src="/%E5%8C%97%E4%BA%AC%E5%90%8C%E4%BB%81%E5%8C%BB%E9%99%A2%E9%AA%8C%E5%85%89%E6%94%BB%E7%95%A5/111.png" class="" title="111.png"><hr><h3 id="验光流程"><a href="#验光流程" class="headerlink" title="验光流程"></a>验光流程</h3><ol><li><p>取号<br>进门之后像是一个眼镜店，此时不要慌，在门口左侧和正面均有一台 <strong>取号机</strong>，使用公众号发给你的二维码取号即可。</p></li><li><p>初步测试<br>在测试完成后，在进门的左侧有一个 <strong>服务台</strong>，到达服务台将号码给工作人员，对方会让你去验一下 <strong>眼镜度数</strong> 和 <strong>初步机器验光测试</strong>，这里根据流程走就可以。</p></li><li><p>人工验光<br>在完成上述测试后，会有工作人员给你一个号码，拿着号码到验光处等到叫号即可。<br><strong>人工验光</strong> 会持续大概 <code>20~30</code> 分钟左右，会简单询问下个人情况、用眼习惯、用眼场景等情况，按照个人情况如实说明即可，如果眼睛有相关就诊经历也可以说明。</p></li><li><p>完成<br>在验光完成后，对方会给你一个单子，然后告知你配镜的注意事项，在拿到单子后就可以走人了。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;前要简介&quot;&gt;&lt;a href=&quot;#前要简介&quot; class=&quot;headerlink&quot; title=&quot;前要简介&quot;&gt;&lt;/a&gt;前要简介&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;首都医科大学附属北京同仁医院始建于 &lt;code&gt;1886&lt;/code&gt; 年，是一所以眼科学、耳鼻咽喉科学为国家重点学科的大型综合三甲医院，对于眼科相关绝对是权威专业。&lt;/li&gt;
&lt;li&gt;一般的眼镜店对于只验光不配镜的顾客有多多少少的抵触。&lt;/li&gt;
&lt;li&gt;镜片和镜框需要单独在网上购买，接着找眼镜店帮忙组装，可以最大化保证钱花在刀刃上。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-HBase</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-HBase/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-HBase/</id>
    <published>2023-05-28T04:31:42.000Z</published>
    <updated>2023-06-21T14:56:16.038Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><code>HBase</code> 是一个在 <code>HDFS</code> 上开发的<strong>面向列</strong>的<strong>分布式数据库</strong>，如果你需要实时访问超大规模的数据集，那么使用 <code>HBase</code> 就对了。</p><p><code>HBase</code> <strong>自底而上</strong>地进行构建，可以简单的通过<strong>增加节点来线性扩展</strong>。其并不是关系型数据库，并且也不支持 <code>SQL</code>，在特定的空间里，能够做 <code>RDBMS</code> 不能做的事，即在廉价的硬件构成的集群上管理超大规模的稀疏表。</p><span id="more"></span><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>应用将数据放到带有标签的表中。而表有行和列组成，表的 <em>单元格</em> 由行和列的交叉决定，其是有版本的。默认情况下，版本号自动分配，为 <code>HBase</code> 插入单元格时的时间戳，单元格的内容是未解释的字节数组。</p><p>表中的行也是字节数组，因此理论上任何东西都可以通过表示成字节数组或者将二进制形式转换为长整型或直接对数据结构进行序列化来作为键值。表中的行根据行的键值进行排序，排序根据字节序进行，所有对表的访问都需要通过表的主键。</p><img src="/Hadoop-HBase/hadoop-HBase-2.jpg" class="" title="Hadoop-HBase"><p>行中的列被分为<strong>列族（<code>column family</code>）</strong>，同一个列族的所有成员具有相同的前缀。列族的前缀必须由 <em>可打印的</em> 字符组成，而修饰性的结尾字符，即列族修饰符，可以为任意字节。列族和修饰符之间始终以冒号（<code>:</code>）分隔。<br>一个表的列族必须作为表模式定义的一部分预先给出，但是新的列族成员可以随后按需加入。</p><p>在物理上所有的列族成员都一起存放在文件系统中，因此结合前面的将 <code>HBase</code> 描述为一个面向列的存储器，其本质上更准确的是面向列族的存储器。<br>但由于调优和存储都是在列族这个维度上进行，所以所有的列族成员具有相同的访问模式和大小特征是最优解。对于存储较大字节的数据和较小字节的数据最好分别存储在不同的列族中。</p><p>简而言之，<code>HBase</code> 的表和 <code>RDBMS</code> 的表类似，只不过其单元格有版本；行是排序的；只要列族是预先存在的，客户端随时可以将列添加进去。</p><h5 id="区域"><a href="#区域" class="headerlink" title="区域"></a>区域</h5><p><code>HBase</code> 自动将表水平划分为<strong>区域（<code>region</code>）</strong>，每个区域由表中行的子集构成，每个区域由所属表、所包含的第一行及最后一行来表示。<br>从表建立之初，一个表只有一个区域，但随着数据增多，其区域也会增大，知道区域超过阈值的界限，就会在某行的边界上将表区分为两个大小相同的新分区。</p><p>区域是 <code>HBase</code> 集群上分布数据的最小单位，使用这种方式不会因为数据太大而无法放置在单个机器上的表会被放到集群中，其中集群中的每个机器负责管理表所有区域的一个子集。而表的加载也是使用这种方法将数据分布到各个节点，集群中所有的机器上的节点按次序排列也就构成了表的所有内容。</p><h5 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h5><p>无论对行进行访问的事务牵涉到多少行，对行的更新都是 <em>原子级别</em>。</p><h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><ul><li><strong>没有真正的索引</strong><br>  行是顺序存储的，每行中的列也是，所以不存在索引膨胀的问题，而且插入性能和表的大小无关。</li><li><strong>自动分区</strong><br>  在表增长的时候，表会自动分裂成区域，并分布到可用的节点上。</li><li><strong>线性扩展和对于新节点的自动处理</strong><br>  增加一个节点将它指向现有集群并运行 <code>regionserver</code>。区域会自动重新进行平衡，负载均匀分布。</li><li>普通商用硬件支持</li><li><strong>容错</strong><br>  大量节点意味着每个节点的重要性并不突出，不用担心单个节点失效。</li><li><strong>批处理</strong><br>  <code>MapReduce</code> 集成功能使用全并行的分布式作业根据数据位置来处理它们。</li></ul><h4 id="与传统-RDBMS-比较"><a href="#与传统-RDBMS-比较" class="headerlink" title="与传统 RDBMS 比较"></a>与传统 <code>RDBMS</code> 比较</h4><p><code>HBase</code> 是一个分布式的、面向列的数据存储系统，通过在 <code>HDFS</code> 上提供随机读&#x2F;写来解决 <code>Hadoop</code> 不能处理的问题。<code>HBase</code> 自底层设计开始即聚集于各种可伸缩性问题：表可以很<em>高</em>（数十亿个数据行）；表可以很<em>宽</em>（数百万个列）；水平分区并在上千个普通机器上自动复制。表的模式是物理存储的直接反映，使系统有可能提供高效的数据结构的序列化、存储和检索，因此程序的开发者就必须选择以正确的方式使用这种存储和检索方式。</p><p>严格来说 <code>RDBMS</code> 是一个遵循 <strong><code>Codd</code> 的 <code>12</code> 条准则</strong>的数据库。标准的 <code>RDBMS</code> 是模式固定、面向行的数据库且具有 <code>ACID</code> 性质和复杂的 <code>SQL</code> 查询处理引擎。<code>RDBMS</code> 强调事务的<em>强一致性</em>、参照完整性、数据抽象和物理存储层相对独立，以及基于 <code>SQL</code> 语言的复杂查询支持。在 <code>RDBMS</code> 中，可以非常容易地建立 <em>二级索引</em>，执行复杂的内连接和外连接，执行计数、求和、排序、分组等操作，或对表、行和列中的数据进行分页存放。</p><hr><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>正如 <code>HDFS</code> 和 <code>YARN</code> 是由客户端、<code>slave</code> 和 <code>master</code> 组成，<code>HBase</code> 也采用了相同的架构，使用一个 <code>master</code> 节点协调管理一个或多个 <code>regionserver</code>。<code>HBase</code> 的 <code>master</code> 节点负责启动一个新的集群，将区域分配个注册的 <code>regionserver</code>，恢复 <code>regionserver</code> 的故障。<br><code>regionserver</code> 负责零个或多个区域的管理以及相应客户端的读写请求，还负责区域的划分并通知 <code>master</code> 有了新的子区域，因此 <code>master</code> 就可以将父区域设置为离线，并用子区域替代父区域。</p><img src="/Hadoop-HBase/hadoop-hbase.jpeg" class="" title="Hadoop-HBase"><p><code>HBase</code> 依赖于 <code>ZooKeeper</code>，默认情况下管理着一个 <code>ZooKeeper</code> 实例，作为集群的 <em>权威机构</em>，其还负责管理 <code>hbase:meta</code> 目录表的位置以及当前集群主控地址等重要信息。<br>如果在区域的分配过程中有服务器崩溃，就可以通过 <code>ZooKeeper</code> 来进行分配的协调。<br>在启动一个客户端到 <code>HBase</code> 集群的连接时，客户端必须至少拿到集群所传递的 <code>ZooKeeper</code> 集合体的位置，这样客户端才能访问 <code>ZooKeeper</code> 的层次结构，从而了解集群的属性。</p><p><code>regionserver</code> 从节点列表可以在 <code>HBase</code> 的 <code>conf/regionservers</code> 文件中看到，与 <code>Hadoop</code> 的一致。<br>集群的站点配置在 <code>HBase</code> 的 <code>conf/hbase-site.xml</code> 和 <code>conf/hbase-env.sh</code> 中，他们的格式和 <code>Hadoop</code> 父项目中对应的格式相同。</p><p>大部分人都是使用 <code>HDFS</code> 来运行 <code>HBase</code>，而 <code>HBase</code> 通过 <code>Hadoop</code> 文件系统 <code>API</code> 来持久化存储数据。但在默认情况下，<code>HBase</code> 会将存储写入本地文件系统。</p><h4 id="使用中的-HBase"><a href="#使用中的-HBase" class="headerlink" title="使用中的 HBase"></a>使用中的 <code>HBase</code></h4><p><code>HBase</code> 内部保留着名为 <strong><code>hbase:meta</code> 的特殊目录表（<code>catalog table</code>）</strong>，他们维护着当前集群上所有区域的列表、状态和位置。<code>hbase:meta</code> 表中的项使用区域名作为键，区域名由 <em>所属的表名</em>、<em>区域的起始行</em>、<em>区域的创建时间</em> 以及对其整体进行的 <em><code>MD5</code> 哈希值（即对表名、起始行、创建的时间戳进行哈希后的结果）</em> 组成。示例如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 表名、起始行、时间戳使用逗号隔开，而 MD5 哈希值则使用两个句号包围</span><br><span class="line">TestTable,xyz,1279753346289.8hjffu7fhsis6f638hf6sjf67jd9sj3g.</span><br></pre></td></tr></table></figure><p>如前所属，行的键都是排序的，因此想要查找一个特定行所在的区域只要在目录表中找到第一个键大于或等于给定行键的项即可。而当区域变化时，目录表也会进行对应的更新，集群上所有区域的状态信息都能保持更新。<br>每个行操作可能要访问三次远程节点，而为了节省代价，通常都会缓存 <code>hbase:meta</code> 信息，其中缓存的内容包含位置信息、用户空间区域的开始行和结束行。当使用缓存中的数据发生异常时，即区域被移动了，客户端会去查看 <code>hbase:meta</code> 获取区域的新位置，如果 <code>hbase:meta</code> 也被移动了，那客户端也会重新寻找。<br>当新的客户端连接到 <code>ZooKeeper</code> 时会首先查找 <code>hbase:meta</code> 的位置，然后通过寻找合适的区域来获取用户空间区域所在的节点和位置，接下来就是客户端直接与管理对应区域的 <code>regionserver</code> 进行交互。</p><p>到达 <code>Regionserver</code> 的写操作首先追加到 <em>提交日志（<code>commit log</code>）</em> 中，然后加入内存中的 <code>metastore</code>，若是 <code>metastore</code> 已满，则会将内容 <em>刷入（<code>flash</code>）</em> 文件系统。<br>提交日志存放在 <code>HDFS</code> 中，当发现某个 <code>Regionserver</code> 崩溃时，<code>master</code> 节点会根据区域对崩溃的 <code>Regionserver</code> 的提交日志进行分割，找到还没有被持久化存储的更新，然后这部分被 <em>重做（<code>replay</code>）</em> 以使区域恢复到崩溃之前的状态。<br>而在读的时候会检查区域的 <code>metastore</code>，如果在其中找到了需要的版本则查询至此结束。否则就需要按照次序从新道旧检查 <em>刷新文件（<code>flash</code>）</em>，直到找到们组查询的版本，或者所有刷新文件都处理完为止。</p><p>之前说到的 <em>刷新文件（<code>flash</code>）</em> 会有一个后台进程在其个数达到阈值时压缩他们，将多个文件重新写入一个文件，在执行压缩操作时，进程会清理掉超出模式所设最大值的版本以及删除单元格或标识单元格为过期。<br>而在 <code>Regionserver</code> 上会有另外一个进程监控着刷新文件的大小，一旦大小超过预先设定的最大值，便会对区域进行分割。</p><hr><h3 id="日常问题"><a href="#日常问题" class="headerlink" title="日常问题"></a>日常问题</h3><h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a><code>HDFS</code></h4><p><code>HBase</code> 使用 <code>HDFS</code> 的方式与 <code>MapReduce</code> 的使用方式截然不同。在 <code>HBase</code> 中数据文件在启动时就被打卡，并在处理过程中始终保持打开状态，这是为了节省每次访问操作打开文件所需的代价。</p><ol><li><p>文件描述符用完<br>由于在连接的集群上始终保持文件的打开状态，因此可能会达到系统和 <code>HDFS</code> 设定的限制。<br>其中一个进程默认的文件描述符限制是 <code>1024</code>，当使用的描述符个数超过文件系统的 <code>ulimit</code> 值，就会在日志中看到异常信息 <code>Too many open files</code>，不过在出现异常信息之前，往往 <code>HBase</code> 就已经出现问题了。</p></li><li><p><code>datanode</code> 中的线程用完<br>与上述的情况类似，<code>datanode</code> 上限制运行的线程数不能超过 <code>256</code>，可以通过在 <code>hdfs-site.xml</code> 中配置 <code>dfs.datanode.max.transfer.threads</code> 来更改设置。</p></li></ol><h4 id="用户界面"><a href="#用户界面" class="headerlink" title="用户界面"></a>用户界面</h4><p><code>HBase</code> 在 <code>master</code> 节点机器上运行了 <code>Web</code> 服务，提供了运行中集群的状态视图。默认情况下，其监听 <code>60010</code> 端口，主界面显示基本的属性（包含软件版本、集群负载、请求频率、集群表的列表）和加入的 <code>Regionserver</code> 等。<br>在主界面上点击选中的 <code>Regionserver</code> 就会展示对应的 <code>Web</code> 服务器，展示该服务器上所有区域的列表及其他基本的属性值。</p><h4 id="度量-Metric"><a href="#度量-Metric" class="headerlink" title="度量 Metric"></a>度量 <code>Metric</code></h4><p><code>Hadoop</code> 有一个<strong>度量 <code>Metric</code></strong> 系统，会在每隔一段时间获取系统重要组件的信息，并将其输出到上下文中。启用该系统可以将信息导出到 <code>JMX</code>，从而展示集群上正在和过去的请求视图。</p><h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p><code>Htable</code> 提供的 <strong><code>incrementColumnValue()</code></strong> 方法可以实现计数器每秒数千次的更新，解决之前计数器存储在 <code>MySQL</code> 中的更新频繁问题。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h3&gt;&lt;p&gt;&lt;code&gt;HBase&lt;/code&gt; 是一个在 &lt;code&gt;HDFS&lt;/code&gt; 上开发的&lt;strong&gt;面向列&lt;/strong&gt;的&lt;strong&gt;分布式数据库&lt;/strong&gt;，如果你需要实时访问超大规模的数据集，那么使用 &lt;code&gt;HBase&lt;/code&gt; 就对了。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;HBase&lt;/code&gt; &lt;strong&gt;自底而上&lt;/strong&gt;地进行构建，可以简单的通过&lt;strong&gt;增加节点来线性扩展&lt;/strong&gt;。其并不是关系型数据库，并且也不支持 &lt;code&gt;SQL&lt;/code&gt;，在特定的空间里，能够做 &lt;code&gt;RDBMS&lt;/code&gt; 不能做的事，即在廉价的硬件构成的集群上管理超大规模的稀疏表。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>to_2023-05-15</title>
    <link href="https://blog.vgbhfive.cn/to-2023-05-15/"/>
    <id>https://blog.vgbhfive.cn/to-2023-05-15/</id>
    <published>2023-05-15T13:02:52.000Z</published>
    <updated>2023-06-21T14:57:29.330Z</updated>
    
    <content type="html"><![CDATA[<p>今天是 <code>2023</code> 年 <code>5</code> 月 <code>15</code> 日，距离我第一次来北京工作的时间刚好跨过了整整三年，关于这个时间我也是在跟朋友的闲聊中才发现原来我已经来北京北漂三年了。</p><span id="more"></span><p>回想起三年前的自己，刚才学校毕业不到一年，工作在西安的一家表面上还看得过去的公司，挣扎在与前女友的回忆中，身体与记忆相互拉扯，不知道明天的路该怎么走，既不敢辞职，也不敢和朋友说，幸亏有大学的舍友在同一家公司上班，不高不低的续命到毕业的第二年。<br>领导在特别突然的一天询问我的工作规划和以后的安排，谈话中我感觉到了不妙的气氛，果不其然我要被安排长期驻场出差了，去原本就定好的北京，领导给了两天的时间让我收拾家里的事情，其实我明白这是领导给我的考虑时间，不同意那就离职，同意就去北京出差。<br>我回到自己组的房子里，思考了一晚，我说服了自己，我要跟之前的生活告别，那最好的方式就是去一个陌生的地方，开启一段陌生的生活。</p><p>两天后，在大学舍友的送别之下，我踏上了开往北京的动车，从西安开往北京，时间花费了 <code>5</code> 小时 <code>35</code> 分钟，全程我没有睡觉，没有玩手机，我就静静地看着窗外，我的内心告诉我新的生活在向我招手，而我也即将开始新的生活。<br>正如预测的那样，刚到北京的一个月住在公司租的房子里，开始跟同事了解驻场开发的所有相关内容，一个月后我开始自己独立租房子，买了锅碗瓢盆自己做饭。幸而遇到一个还算不错的组长，在工作完成之后，他还允许我自己看书学习；半夜熬夜切换演练后，第二天就给我一天假期，让我好好休息；其他项目组的朋友，我们会一起买水果，一起在水房边聊天边吃。合租的室友遇到特别好的两个饭搭子、扑克牌搭子、游玩搭子，那是一段特别开心的日子，有时候竟然可以聊到凌晨的两点钟，哈哈哈哈哈哈哈哈。<br>这些快乐的时光让我逐渐找回自己，每天都在学习新的技术、尝试做新的菜、交往新的朋友、幻想骑着自己买的摩托车，到现在不知不觉来到北京已经三年了，基本上都是快乐的日子，希望在未来的日子里可以多多挣钱，哈哈哈哈哈哈哈哈。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天是 &lt;code&gt;2023&lt;/code&gt; 年 &lt;code&gt;5&lt;/code&gt; 月 &lt;code&gt;15&lt;/code&gt; 日，距离我第一次来北京工作的时间刚好跨过了整整三年，关于这个时间我也是在跟朋友的闲聊中才发现原来我已经来北京北漂三年了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Myx&amp;Lyl" scheme="https://blog.vgbhfive.cn/tags/Myx-Lyl/"/>
    
  </entry>
  
  <entry>
    <title>2023 低生产力 PC 装机报告</title>
    <link href="https://blog.vgbhfive.cn/2023-%E4%BD%8E%E7%94%9F%E4%BA%A7%E5%8A%9B-PC-%E8%A3%85%E6%9C%BA%E6%8A%A5%E5%91%8A/"/>
    <id>https://blog.vgbhfive.cn/2023-%E4%BD%8E%E7%94%9F%E4%BA%A7%E5%8A%9B-PC-%E8%A3%85%E6%9C%BA%E6%8A%A5%E5%91%8A/</id>
    <published>2023-05-11T13:17:18.000Z</published>
    <updated>2023-06-14T14:03:40.400Z</updated>
    
    <content type="html"><![CDATA[<h3 id="配置列表"><a href="#配置列表" class="headerlink" title="配置列表"></a>配置列表</h3><p><code>CPU</code>：<code>Intel i5-12400</code> 散片<br>主板：微星 <code>MAG B660 MORTAR WIFI DDR4</code><br>内存：光威 天策系列 <code>16G * 2</code> 套条<br>固态：宏基掠夺者 <code>GM7000 PCIe4.0 NVMe</code><br>电源：长城 <code>650w</code> 金牌全模<br>散热：九州风神 玄冰 <code>400V5</code>（四热管）<br>机箱：先马 平头哥 <code>M2</code>（五风扇位，侧头玻璃）<br>系统：<code>Windows 10</code> 专业版</p><span id="more"></span><p>上面这台机器从购买组装完再到现在，已经有了半个月的时间，可以基本上满足类似 <code>DNF</code>、 原神、 <code>IDEA</code>、 <code>VS Code</code> 这类低生产、低游戏的日常 PC 机需求。</p><hr><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>没有显卡，使用核显，对于非游戏玩家来说完美适配。后续若想加显卡，电源、主板均可以完美适配。</li><li>整体配置中规中矩，也就意味着不出错、不完美。</li><li>主板和机箱支持多磁盘扩展，散热、颜值在线。</li><li>支持国产品牌，发挥自己的一份力。</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li>没有独显，对于中轻度游戏玩家不是很友好，游戏体验折扣较大。</li><li>没有光污染，对于常年不关机玩家有较好的体验。</li><li>如果没有后续扩展的需求，可以使用 <code>MATX-mini</code> 机箱进一步压缩占用面积。</li></ol><hr><h3 id="优化配置"><a href="#优化配置" class="headerlink" title="优化配置"></a>优化配置</h3><p>机箱：小喆优品 <code>C2P</code><br>散热：乔思伯<code> CR-1400</code><br>机箱风扇：利民 <code>C12C * 3</code></p><p><small>这里的优化配置倾向于压缩机箱占用面积，而不是减少花费的金额，毕竟一分钱一分货。</small></p><hr><h3 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h3><p>上面详细客观的描述了这台机器的情况和需要优化的地方，那下面就是吐槽的内容。<br>因为一直是 <code>JD</code> 的深度用户，基本上大大小小的电子产品都是在官方直营买的，但是我却在这次买主板的时候遇到了<strong>“大坑”</strong> 。店铺客服告诉我他们是官方客服，第一时间我还没有理解这个意思，然后我就问了一嘴，那是京东官方吗？（我想很多人在京东买电子产品都是对京东官方售后有保障才去的吧）<br>反转来了，客服说：他们不是！！！他们是官方、官方、官方客服，这是打着京东自营的招牌，而是主板官方旗舰店？？？</p><p>至于我为什么会跟客服聊这么多呢？是因为装机之后，板子的蓝牙时断时续，用一会就自己断开连接了，我重新连接再过一会就又会断开；自己尝试过各种各样的解决方案：刷 <code>BIOS</code>、抠 <code>BIOS</code> 电池、重装蓝牙驱动，自己无法解决，就去找了店铺客服。<br>此时我看到一行小字<strong>“180天只换不修”</strong> ，我就问客服啊，是真的只换不修吗？我这情况可以只换不修吗？客服说：行，那你寄回来吧。<br>我又是一通折腾，主板拆了下来，我还专门录像了，特别是重要的地方：<code>CPU</code> 接口、零配件、盖板都搞的好好的，自我感觉都可以二次销售了（录像是担心有扯皮的事情出现）。第二天官方收到板子，速度很快另一块板子就上路了，好家伙！这速度杠杠的。第三天早上快递就打电话喊我去取快递，不过当时在上班就喊舍友帮忙取了一下，晚上我回家又是一通折腾，装机、整线、重装系统，一直搞到凌晨一点多。<br>这其中更有意思的来了，因为我录了开箱视频，就发现了几个问题：新的主板外包装没有塑封，感觉像是维修后的；我在找说明书的时候，发现他的保修卡被人撕了一角，这让我更加觉得是维修后的主板。当时由于当时着急装机，我就没管这些，板子能用就好啊，其他的都是小事。那么更大的反转又来了！<br>就在我写这篇博客的今天，感觉之前买的内存条不是很好，想看看板子支不支持 <code>DDR5</code>，我记得之前问过客服，就去翻了之前的聊天记录，此时我发现了<strong>大 <code>BUG</code></strong> ，店铺官方客服说<strong>“180天只换不修”</strong> ，更换的是<strong>良品</strong>；而我去咨询京东官方客服，她们说<strong>“180天只换不修”</strong> ，更换的是<strong>新品</strong>。<br>你品品这其中的差距，一字之差，就是天翻地覆的差距！而我现在呢，被客服留了联系方式，说是 <code>24</code>小时内给我回电，<code>emmmm....</code> 怎么说呢？<br>我好像无话可说，反正我是基本告别 <code>JD</code> 了。</p><hr><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>今天是 <code>2023-06-15</code>，上述的吐槽经过 <code>12315</code> 没有任何解决方案，就是一直拖着。<br>无奈，我接受了解决方案：售后维修再加两百块的补偿。说实话真的很无奈，得不到任何的解释，也没有任何说明，反正就是拿钱办事，对此我只能说<strong>这事办的漂亮</strong>。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;配置列表&quot;&gt;&lt;a href=&quot;#配置列表&quot; class=&quot;headerlink&quot; title=&quot;配置列表&quot;&gt;&lt;/a&gt;配置列表&lt;/h3&gt;&lt;p&gt;&lt;code&gt;CPU&lt;/code&gt;：&lt;code&gt;Intel i5-12400&lt;/code&gt; 散片&lt;br&gt;主板：微星 &lt;code&gt;MAG B660 MORTAR WIFI DDR4&lt;/code&gt;&lt;br&gt;内存：光威 天策系列 &lt;code&gt;16G * 2&lt;/code&gt; 套条&lt;br&gt;固态：宏基掠夺者 &lt;code&gt;GM7000 PCIe4.0 NVMe&lt;/code&gt;&lt;br&gt;电源：长城 &lt;code&gt;650w&lt;/code&gt; 金牌全模&lt;br&gt;散热：九州风神 玄冰 &lt;code&gt;400V5&lt;/code&gt;（四热管）&lt;br&gt;机箱：先马 平头哥 &lt;code&gt;M2&lt;/code&gt;（五风扇位，侧头玻璃）&lt;br&gt;系统：&lt;code&gt;Windows 10&lt;/code&gt; 专业版&lt;/p&gt;</summary>
    
    
    
    
    <category term="Electron" scheme="https://blog.vgbhfive.cn/tags/Electron/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-Hive</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-Hive/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-Hive/</id>
    <published>2023-04-12T14:31:45.000Z</published>
    <updated>2023-05-28T04:36:06.235Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><strong><code>Hive</code></strong> 是一个构建在 <code>Hadoop</code> 之上的<strong>数据仓库框架</strong>，其设计目的在于让精通 <code>SQL</code> 但编程技能较弱的运营人员能够对存放在 <code>HDFS</code> 中的大规模数据集执行查询。<br>但是由于其底层依赖的 <code>Hadoop</code> 和 <code>HDFS</code> 设计本身约束和局限性，限制 <code>Hive</code> 不支持记录级别的更新、插入或者删除操作，不过可以通过查询生成新表或将查询结果导入文件中来实现。同时由于 <code>MapReduce</code> 任务的启动过程需要消耗较长的时间，所以查询延时比较严重。</p><span id="more"></span><img src="/Hadoop-Hive/mmexport1684155627591.jpg" class="" title="mmexport1684155627591"><p><code>Hive</code> 发行版本中包含 <strong><code>CLI</code></strong> 、 <strong><code>HWI</code></strong> （一个简单的网页界面）以及可通过 <strong><code>JDBC</code></strong> 、 <strong><code>ODBC</code></strong> 和一个 <strong><code>Thrift</code> 服务器</strong>进行编程访问的几个模块。<br>所有的命令和查询都会进入 <strong><code>Driver</code>（驱动模块）</strong>，通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行（通常是启动多个 <code>MapReduce</code> 任务 <code>job</code> 来执行）。当需要启动启动 <code>MapReduce</code> 任务（<code>job</code>）时，<code>Hive</code> 本身是不会生成 <code>MapReduce</code> 算法程序，相反 <code>Hive</code> 会通过一个 <code>XML</code> 文件的 <strong><code>job</code> 执行计划</strong>驱动执行内置的、原生的 <code>Mapper</code> 和 <code>Reduce</code> 模块，即这些通用的模块函数类似于微型的语言翻译程序，而驱动此翻译程序的就是 <code>XML</code> 文件。<br><code>Hive</code> 通过和 <strong><code>JobTracker</code> 通信来初始化 <code>MapReduce</code> 任务</strong>，而不必部署在 <code>JobTracker</code> 所在的管理节点上执行。<br><strong><code>Metastore</code>（元数据存储）</strong>是一个独立的关系型数据库，<code>Hive</code> 会在其中保存表模式和其他系统元数据。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p><code>Hive</code> 一般运行在工作站上，将 <code>SQL</code> 查询转换为一系列在 <code>Hadoop</code> 集群上运行的作业。<code>Hive</code> 把数据组织为表，通过这种方式为存储在 <code>HDFS</code> 上的数据结构赋予结构，元数据（表模式等）存储在 <code>metastore</code> 数据库中。</p><p><code>Hive</code> 的安装非常简单，首先必须安装相同版本的 <code>Hadoop</code>。 接下来下载相同版本的 <code>Hive</code>，然后把压缩包解压缩到合适的目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">tar xzf apache-hive-x.y.z-bin.tar.gz</span></span><br></pre></td></tr></table></figure><p>接下来就是将 <code>Hive</code> 加入到全局文件中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash"><span class="built_in">export</span> HIVE_HOME=~/apache-hive-x.y.z-bin</span></span><br><span class="line"><span class="meta prompt_">% </span><span class="language-bash"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span></span><br></pre></td></tr></table></figure><p>最后就是启动 <code>Hive</code> 的 <code>shell</code> 环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">hive</span></span><br><span class="line"><span class="meta prompt_">hive&gt;</span></span><br></pre></td></tr></table></figure><h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><p>与 <code>Hadoop</code> 类似，<code>Hive</code> 用 <code>XML</code> 配置进行设置，配置文件为 <code>hive-site.xml</code>，位于在 <code>Hive</code> 的 <code>conf</code> 目录下。通过该文件可以设置每次运行 <code>Hive</code> 时希望使用的配置项，该目录下还包含 <code>hive-default.xml</code> （其中记录着 <code>Hive</code> 的选项及默认值）。</p><p><code>hive-site.xml</code> 文件最适合存放详细的集群连接信息，可以使用 <code>Hadoop</code> 属性 <code>fa.defaultFS</code> 和 <code>yarn.resourcemanager.address</code> 来指定文件系统和资源管理器。默认值为本地文件系统和本地作业运行器（<code>job runner</code>）。</p><p>传递 <code>--config</code> 选项参数给 <code>hive</code> 命令，可以通过这种方式重新定义 <code>Hive</code> 查找 <code>hive-site.xml</code> 文件的目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">hive --config /Users/home/hive-conf</span></span><br></pre></td></tr></table></figure><p>传递 <code>-hiveconf</code> 选项来为单个会话（<code>pre-session</code>）设置属性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">hive -hiveconf fs.defaultFS=hdfs://localhost -hiveconf mapper.framework.name=yarn</span></span><br></pre></td></tr></table></figure><p>还可以在一个会话中使用 <code>SET</code> 命令更改设置，这对于某个特定的查询修改 <code>Hive</code> 设置非常有用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">SET mapper.framework.name=yarn</span></span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">SET mapper.framework.name</span></span><br><span class="line">mapper.framework.name=yarn</span><br></pre></td></tr></table></figure><p>设置属性有一个优先级层次，越小的值表示优先级越高：</p><ul><li><strong><code>Hive SET</code> 命令</strong> 。</li><li><strong>命令行 <code>-hiveconf</code> 选项</strong> 。</li><li><strong><code>hive-site.xml</code> 和 <code>Hadoop</code> 站点文件</strong>（<code>core-site.xml</code>、<code>hdfs-site.xml</code>、<code>mapper-site.xml</code>、<code>yarn-site.xml</code>）。</li><li><strong><code>Hive</code> 默认值和 <code>Hadoop</code> 默认文件</strong>（<code>core-default.xml</code>、<code>hadfs-default.xml</code>、<code>mapper-default.xml</code>、<code>yarn-default.xml</code>）。</li></ul><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><h5 id="用户接口"><a href="#用户接口" class="headerlink" title="用户接口"></a>用户接口</h5><ol><li><p><strong><code>CLI</code></strong><br><code>Shell</code> 终端命令行（<code>Command Line Interface</code>），交互形式使用 <code>Hive</code> 命令行与 <code>Hive</code> 进行交互。</p></li><li><p><strong><code>JDBC/ODBC</code></strong><br><code>Hive</code> 基于 <code>JDBC</code> 操作提供的客户端，用户可以通过连接来访问 <code>Hive Server</code> 服务。</p></li><li><p><strong><code>Web UI</code></strong><br>通过浏览器访问 <code>Hive</code>。</p></li></ol><h5 id="Thrift-Server"><a href="#Thrift-Server" class="headerlink" title="Thrift Server"></a><code>Thrift Server</code></h5><p>轻量级、跨语言的远程服务调用框架，<code>Hive</code> 集成该服务便于不同的编程语言调用 <code>Hive</code> 的接口。</p><h5 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a><code>Metastore</code></h5><p><code>Metastore</code> 是 <code>Hive</code> 元数据的集中存放地，通常包含两部分：服务和元数据的存储。元数据包含：<strong>表的名字</strong>、<strong>表的列和分区及属性</strong>、<strong>表的属性（内部表和外部表）</strong>、<strong>表数据所在目录</strong>。默认情况下，<code>metastore</code> 服务和 <code>Hive</code> 服务运行在同一个 <code>JVM</code> 中，包含一个内嵌的以本地磁盘作为存储的 <code>Derby</code> 数据库实例，被称为内嵌 <code>metastore</code> 配置（<code>embedded metastore configuration</code>）。</p><p>如果要支持多会话（以及多用户），需要使用一个独立的数据库。这是因为 <code>MetaStore</code> 通常存储在其自带的 <code>Derby</code> 数据库中，缺点是跟随 <code>Hive</code> 部署，数据目录不固定，且不支持多用户操作。另外现在支持外部 <code>MySQL</code> 与 <code>Hive</code> 交互用于存储元数据信息。<br>可以通过把 <code>hive.metastore.uris</code> 设为 <code>metastore</code> 服务器 <code>URI</code>（如果有多个服务器，各个 <code>URI</code> 之间使用逗号分隔），把 <code>Hive</code> 服务设为使用远程 <code>metastore</code>。<code>metastore</code> 服务器 <code>URI</code> 的形式为 <code>thrift://host:port</code>。</p><p><img src="/Hadoop-Hive/hadoop_metastore_1.jpg" alt="hadoop_metastore_1"></p><p>重要的 <code>metastore</code> 配置属性：</p><table><thead><tr><th>属性名称</th><th>类型 <div style="width: 70px"></th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td><code>hive.metastore.warehouse.dir</code></td><td><code>URI</code></td><td><code>/user/hive/warehouse</code></td><td>相当于 <code>fs.default.name</code> 的目录，托管表就存储在这里</td></tr><tr><td><code>hive.metastore.uris</code></td><td>逗号分隔的 <code>URI</code></td><td>未设定</td><td>如果未设置则使用当前的 <code>metastore</code>，否则连接到由 <code>URI</code> 列表指定要连接的远程 <code>metastore</code> 服务器。</td></tr><tr><td><code>javax.jddo.option.ConnectionURL</code></td><td><code>URI</code></td><td><code>jdbc:derby:;databaseName=metastoredb;create=true</code></td><td><code>metastore</code> 数据库的 <code>JDBC URL</code></td></tr><tr><td><code>javax.jddo.option.ConnectionDriveName</code></td><td>字符串</td><td><code>org.apache.derby.jdbc.EmbeddedDriver</code></td><td><code>JDBC</code> 驱动器的类名</td></tr><tr><td><code>javax.jddo.option.ConnectionUserName</code></td><td>字符串</td><td><code>APP</code></td><td><code>JDBC</code> 用户名</td></tr><tr><td><code>javax.jddo.option.ConnectionPassword</code></td><td>字符串</td><td><code>mine</code></td><td><code>JDBC</code> 密码</td></tr></tbody></table><hr><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><h4 id="计算引擎"><a href="#计算引擎" class="headerlink" title="计算引擎"></a>计算引擎</h4><p>目前 <code>Hive</code> 支持 <strong><code>MapReduce</code></strong> 、 <strong><code>Tez</code></strong> 和 <strong><code>Spark</code></strong> 三种计算引擎。</p><ol><li><p><strong><code>MapReduce</code></strong> 计算引擎<br>请参考之前的博客内容。</p></li><li><p><strong><code>Spark</code></strong> 计算引擎<br>请参考之前的博客内容。</p></li><li><p><strong><code>Tez</code></strong> 计算引擎<br><code>Apache Tez</code> 是进行大规模数据处理且支持 <code>DAG</code> 作业的计算框架，它直接源于 <code>MapReduce</code> 框架，除了能够支持 <code>MapReduce</code> 特性之外，还支持新的作业形式，并允许不同类型的作业能够在一个集群中运行。</p><p> <code>Tez</code> 将原有的 <code>Map</code> 和 <code>Reduce</code> 两个操作简化为一个新的概念 **<code>Vertex</code>**，并将原有的计算处理节点拆分成多个组成部分： <strong><code>Vertex Input</code></strong> 、 <strong><code>Vertex Output</code></strong> 、 <strong><code>Sorting</code></strong>  <strong><code>Shuffling</code></strong> 和 <strong><code>Merging</code></strong> 。计算节点之间的数据通信被统称为 <strong><code>Edge</code></strong> ，这些分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的 <strong><code>DAG</code></strong> 作业。<br> 通过允许 <code>Apache Hive</code> 运行更加复杂的 <code>DAG</code> 任务，之前需要多个 <code>MR jobs</code>，但现在运行一个 <code>Tez</code> 任务中。</p> <img src="/Hadoop-Hive/hadoop_mapreduce-tez.jpg" class="" title="hadoop_mapreduce-tez"><p> <code>Tez</code> 和 <code>MapReduce</code> 作业的比较：</p><ul><li><code>Tez</code> 绕过 <code>MapReduce</code> 很多不必要的中间的数据存储和读取的过程，直接在一个作业中表达了 <code>MapReduce</code> 需要多个作业共同协作才能完成的事情。</li><li><code>Tez</code> 和 <code>MapReduce</code> 一样都使用 <code>YARN</code> 作为资源调度和管理。但与 <code>MapReduce on YARN</code> 不同，<code>Tez on YARN</code> 并不是将作业提交到 <code>ResourceManager</code>，而是提交到 <code>AMPoolServer</code> 的服务上，<code>AMPoolServer</code> 存放着若干个已经预先启动的 <code>ApplicationMaster</code> 服务。</li><li>当用户提交一个 <code>Tez</code> 作业上来后，<code>AMPoolServer</code> 从中选择一个 <code>ApplicationMaster</code> 用于管理用户提交上来的作业，这样既可以节省 <code>ResourceManager</code> 创建 <code>ApplicationMaster</code> 的时间，而又能够重用每个 <code>ApplicationMaster</code> 的资源，节省了资源释放和创建时间。</li></ul><p> <code>Tez</code> 相比于 <code>MapReduce</code> 有以下几点重大改进：</p><ul><li>当查询需要有多个 <code>Reduce</code> 逻辑时，<code>Hive</code> 的 <code>MapReduce</code> 引擎会将计划分解，每个 <code>Redcue</code> 提交一个 <code>MapReduce</code> 作业。这个链中的所有 <code>MR</code> 作业都需要逐个调度，每个作业都必须从 <code>HDFS</code> 中重新读取上一个作业的输出并重新洗牌。而在 <code>Tez</code> 任务中，几个 <code>Reduce</code> 接收器可以直接连接，数据可以流水线传输，而不需要临时 <code>HDFS</code> 文件，这种模式称为 <strong><code>MRR（Map-reduce-reduce）</code></strong> 。</li><li><code>Tez</code> 还允许一次发送整个查询计划，实现应用程序动态规划，从而使框架能够更智能地分配资源，并通过各个阶段流水线传输数据。对于更复杂的查询来说，这是一个巨大的改进，因为它消除了 <code>IO/sync</code> 障碍和各个阶段之间的调度开销。</li><li>在 <code>MapReduce</code> 计算引擎中，无论数据大小，在洗牌阶段都以相同的方式执行，将数据序列化到磁盘，再由下游的程序去拉取，并反序列化。<code>Tez</code> 可以允许小数据集完全在内存中处理，而 <code>MapReduce</code> 中没有这样的优化。仓库查询经常需要在处理完大量的数据后对小型数据集进行排序或聚合，因此 <code>Tez</code> 的优化也能极大地提升效率。</li></ul></li></ol><h4 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a>存储格式</h4><p><code>Hive</code> 支持的存储数的格式主要有： <strong><code>TEXTFILE</code>（行式存储）</strong>、 <strong><code>SEQUENCEFILE</code>（行式存储）</strong>、 <strong><code>ORC</code>（列式存储）</strong>、 <strong><code>PARQUET</code>（列式存储）</strong>。</p><p>行存储的特点： 查询满足条件的<strong>一整行数据</strong>的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><p>列存储的特点： 因为每个字段的数据聚集存储，在查询只需要<strong>少数几个字段</strong>的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><h5 id="TEXTFILE"><a href="#TEXTFILE" class="headerlink" title="TEXTFILE"></a><code>TEXTFILE</code></h5><p>默认格式，数据不做压缩，磁盘开销大，数据解析也开销大。可结合 <code>Gzip</code>、<code>Bzip2</code> 使用（系统自动检查，执行查询时自动解压），但使用这种方式，<code>Hive</code> 就不会对数据进行切分，从而无法对数据进行并行操作。</p><h5 id="ORC-格式"><a href="#ORC-格式" class="headerlink" title="ORC 格式"></a><code>ORC</code> 格式</h5><p><code>ORC (Optimized Row Columnar)</code> 是 <code>Hive 0.11</code> 引入的新的存储格式。其可以看到每个 <code>ORC</code> 文件由 <code>1</code> 个或多个 <code>Stripe</code> 组成，每个 <code>stripe</code> 为 <code>250MB</code> 大小。<br><small><code>Stripe</code> 实际相当于 <code>RowGroup</code> 概念，不过大小由 <code>4MB-&gt;250MB</code>，这样能提升顺序读的吞吐率。</small><br>每个 <code>Stripe</code> 里有三部分组成，分别是：</p><ul><li><code>Index Data</code><br> 一个轻量级的 <code>index</code>，默认是每隔 <code>1W</code> 行做一个索引。这里做的索引只是记录某行的各字段在 <code>Row Data</code> 中的 <code>offset</code> 偏移量。</li><li><code>RowData</code><br> 存储的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 <code>Stream</code> 来存储。</li><li><code>Stripe Footer</code><br> 存的是各个stripe的元数据信息。每个文件有一个 <code>File Footer</code>，这里面存的是每个 <code>Stripe</code> 的行数，每个<code>Column</code> 的数据类型信息等；每个文件的尾部是一个 <code>PostScript</code>，记录了整个文件的压缩类型以及 <code>FileFooter</code> 的长度信息等。<br> 在读取文件时，会 <code>seek</code> 到文件尾部读 <code>PostScript</code>，从里面解析到 <code>File Footer</code> 长度，再读 <code>FileFooter</code>，从里面解析到各个 <code>Stripe</code> 信息，再读各个 <code>Stripe</code>，即从后往前读。</li></ul><h5 id="PARQUET-格式"><a href="#PARQUET-格式" class="headerlink" title="PARQUET 格式"></a><code>PARQUET</code> 格式</h5><p><code>Parquet</code> 是面向分析型业务的列式存储格式，以二进制方式存储的，因此是不可以直接读取的，文件中包括该文件的数据和元数据，所以 <code>Parquet</code> 格式文件是自解析的。</p><p>通常情况下，在存储 <code>Parquet</code> 数据的时候会按照 <code>Block</code> 大小设置行组的大小，由于一般情况下每一个 <code>Mapper</code> 任务处理数据的最小单位是一个 <code>Block</code>，这样可以把每一个行组由一个 <code>Mapper</code> 任务处理，增大任务执行并行度。</p><p>该 <code>Parquet</code> 文件的内容中：一个文件中可以存储多个行组，文件的首位都是该文件的 <code>Magic Code</code>，用于校验它是否是一个 <code>Parquet</code>文件；<code>Footer length</code> 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量；文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 <code>Schema</code> 信息；除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据。</p><p>在Parquet中，有三种类型的页：</p><ul><li><strong>数据页</strong><br>  数据页用于存储当前行组中该列的值。</li><li><strong>字典页</strong><br>  字典页存储该列值的编码字典，每一个列块中最多包含一个字典页。</li><li><strong>索引页</strong><br>  索引页用来存储当前行组下该列的索引，目前 <code>Parquet</code> 中还不支持索引页。</li></ul><h4 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h4><p>在 <code>Hive</code> 中处理数据，一般都需要经过压缩，通过压缩来节省网络带宽。</p><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>对应的编解码器</th></tr></thead><tbody><tr><td><code>DEFAULT</code></td><td>无</td><td><code>DEFAULT</code></td><td><code>.deflate</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.DefaultCodec</code></td></tr><tr><td><code>Gzip</code></td><td><code>gzip</code></td><td><code>DEFAULT</code></td><td><code>.gz</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.GzipCodec</code></td></tr><tr><td><code>bzip2</code></td><td><code>bzip2</code></td><td><code>bzip2</code></td><td><code>.bz2</code></td><td>是</td><td><code>org.apache.hadoop.io.compress.BZip2Codec</code></td></tr><tr><td><code>LZO</code></td><td><code>lzop</code></td><td><code>LZO</code></td><td><code>.lzo</code></td><td>否</td><td><code>com.hadoop.compression.lzo.LzopCodec</code></td></tr><tr><td><code>LZ4</code></td><td>无</td><td><code>LZ4</code></td><td><code>.lz4</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.Lz4Codec</code></td></tr><tr><td><code>Snappy</code></td><td>无</td><td><code>Snappy</code></td><td><code>.snappy</code></td><td>否</td><td><code>org.apache.hadoop.io.compress.SnappyCodec</code></td></tr></tbody></table><h4 id="底层执行原理"><a href="#底层执行原理" class="headerlink" title="底层执行原理"></a>底层执行原理</h4><img src="/Hadoop-Hive/mmexport1684155646915.jpg" class="" title="mmexport1684155646915"><h5 id="Driver-运行器"><a href="#Driver-运行器" class="headerlink" title="Driver 运行器"></a><code>Driver</code> 运行器</h5><p><code>Driver</code> 组件完成 <code>HQL</code> 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行计划的生成。生成的逻辑执行计划存储在 <code>HDFS</code> 中，并随后由 <code>MapReduce</code> 调用执行。<br><code>Hive</code> 的核心是驱动引擎， 驱动引擎由四部分组成：</p><ul><li><strong>解释器</strong>：解释器的作用是将 <code>HiveSQL</code> 语句转换为抽象语法树（<code>AST</code>）。</li><li><strong>编译器</strong>：编译器是将语法树编译为逻辑执行计划。</li><li><strong>优化器</strong>：优化器是对逻辑执行计划进行优化。</li><li><strong>执行器</strong>：执行器是调用底层的运行框架执行逻辑执行计划。</li></ul><h5 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h5><p><code>HiveQL</code> 通过命令行或者客户端提交，经过 <code>Compiler</code> 编译器，运用 <code>MetaStore</code> 中的元数据进行类型检测和语法分析，生成一个逻辑方案(<code>Logical Plan</code>)，然后通过的优化处理，产生一个 <code>MapReduce</code> 任务。</p><img src="/Hadoop-Hive/hadoop_hive_driver.jpg" class="" title="hadoop_hive_driver"><h4 id="与传统数据库比较"><a href="#与传统数据库比较" class="headerlink" title="与传统数据库比较"></a>与传统数据库比较</h4><p><code>Hive</code> 在很多方面与传统的数据库类似，但由于需要支持 <code>MapReduce</code> 和 <code>HDFS</code> 就意味着其体系结构有别于传统数据库，而这些区别又影响着 <code>Hive</code> 所支持的特性。</p><h5 id="读时模式和写时模式"><a href="#读时模式和写时模式" class="headerlink" title="读时模式和写时模式"></a>读时模式和写时模式</h5><p>在加载时发现数据不符合模式，则拒绝加载数据，因为数据是在写入数据库时对照模式进行检查，因此这一设计又被称为<strong>“写时模式”</strong> 。而 <code>Hive</code> 对数据的验证并不在加载数据时进行，而是在查询时进行，被称为<strong>“读时模式”</strong> 。</p><p><strong>读时模式</strong>不需要读取数据来进行“解析”，再进行序列化并以数据库内部格式存入磁盘，因此可以使数据加载非常迅速。<strong>写时模式</strong>可以对列进行索引，并对数据进行压缩，但这也会导致加载数据会额外耗时，由此有利于提升查询性能。</p><h5 id="更新、事务和索引"><a href="#更新、事务和索引" class="headerlink" title="更新、事务和索引"></a>更新、事务和索引</h5><p>更新、索引和事务这些是传统数据库最要的特性，但 <code>Hive</code> 并不支持这些，因为需要支持 <code>MapReduce</code> 操作 <code>HDFS</code> 数据，因此 <strong>“全表扫描”</strong> 是常态化操作，而表更新则是将数据变换后放入新表实现。</p><ol><li><p>更新<br><code>HDFS</code> 不提供就地文件更新，因此插入、更新、删除等一系列引起数据变化的操作都会被保存在一个较小的增量文件中，由 <code>metastore</code> 在后台运行的 <code>MapReduce</code> 任务定期将这些增量文件合并到基表文件中。<br><small>上述功能的支持必须启用事务，以保证对表进行读取操作时可以看到表的一致性快照。</small></p></li><li><p>锁<br><code>Hive</code> 引入了表级（<code>table-level</code>）和分区级（<code>partition-level</code>）的锁，因此可以防止一个进程删除正在被另一个进程读取的表。该锁由 <code>ZooKeeper</code> 透明管理，因此用户不必执行获取和释放锁的操作，但可以通过 <code>SHOW LOCKS</code> 语句获取已经获得了哪些锁的信息。默认情况下，未启用锁的功能。</p></li><li><p>索引<br><code>Hive</code> 的索引目前被分为两类：<strong>紧凑索引（<code>compact index</code>）</strong>和<strong>位图索引（<code>bitmap index</code>）</strong>。<br>紧凑索引存储每个值的 <code>HDFS</code> 块号，而不是存储文件内偏移量，因此存储不会占用过多的磁盘空间，并且对于值被聚簇（<code>clustered</code>）存储于相近行的情况，索引仍然能够有效。<br>位图索引使用压缩的位集合（<code>bitset</code>）来高效存储某个特殊值的行，而这种索引一般适用于较少取值的列（例如性别和国家）。</p></li></ol><h5 id="其他-SQL-on-hadoop"><a href="#其他-SQL-on-hadoop" class="headerlink" title="其他 SQL-on-hadoop"></a>其他 <code>SQL-on-hadoop</code></h5><p>针对 <code>Hive</code> 的局限性，也有其他的 <code>SQL-on-Hadoop</code> 技术出现，那么 <strong><code>Cloudera Impala</code></strong> 就是其中的佼佼者，他是开源交互式 <code>SQL</code> 引擎，<code>Impala</code> 在性能上要强于 <code>MapReduce</code> 的 <code>Hive</code> 高一个数量级。<br><code>Impala</code> 使用专门的守护进程，这些守护进程运行在集群中的每个数据节点上，当客户端发起查询时，会首先联系任意一个运行了 <code>Impala</code> 守护进程的节点，该节点会被当作该查询的协调（<code>coordination</code>）节点。协调节点向集群中的其他 <code>Impala</code> 守护进程分发工作，并收集结果以形成该查询的完整结果集。<code>Impala</code> 使用 <code>Hive</code> 的 <code>Metastore</code> 并支持 <code>Hive</code> 格式和绝大多数的 <code>HiveQL</code> 结构，因此在实际中这两者可以直观地相互移植，或者运行在同一个集群上。</p><p>当然也有 <code>Hortonworks</code> 的 <strong><code>Stinger</code></strong> 计划支持 <code>Tez</code> 作为执行引擎，再加上矢量化查询引擎等其他改进技术，使 <code>Hive</code> 在性能上得到很大的提升。</p><p><strong><code>Apache phoenix</code></strong> 则采取了另一种完全不同的方式，提供基于 <code>HBase</code> 的 <code>SQL</code>，通过 <code>JDBC</code> 驱动实现 <code>SQL</code> 访问，<code>JDBC</code> 驱动将查询转换为 <code>HBase</code> 扫描，并利用 <code>HBase</code> 协同处理器来执行服务器端的聚合，当然数据也存储在 <code>HBase</code> 中。 </p><hr> <h3 id="HiveQL"><a href="#HiveQL" class="headerlink" title="HiveQL"></a><code>HiveQL</code></h3><p><code>Hive</code> 的 <code>SQL</code> 被称为 <code>HiveQL</code>，是 <code>SQL-92</code>、 <code>MySQL</code>、 <code>Oracle SQL</code> 的混合体。其概要比较如下：</p><table><thead><tr><th>特性</th><th><code>SQL</code></th><th><code>HiveQL</code></th></tr></thead><tbody><tr><td>更新</td><td><code>UPDATE</code>、 <code>INSERT</code>、 <code>DELETE</code></td><td><code>UPDATE</code>、 <code>INSERT</code>、 <code>DELETE</code></td></tr><tr><td>事务</td><td>支持</td><td>有限支持</td></tr><tr><td>索引</td><td>支持</td><td>支持</td></tr><tr><td>延迟</td><td>亚秒级</td><td>分钟级</td></tr><tr><td>数据类型</td><td>整数、浮点数、定点数、文本和二进制串、时间</td><td>布尔型、整数、浮点数、文本和二进制串、时间戳、数组、映射、结构</td></tr><tr><td>函数</td><td>数百个内置函数</td><td>数百个内置函数</td></tr><tr><td>多表插入</td><td>不支持</td><td>支持</td></tr><tr><td><code>CREATE TABLE AS SELECT</code></td><td><code>SQL-92</code> 中不支持，但有些数据库支持</td><td>支持</td></tr><tr><td><code>SELECT</code></td><td><code>SQL-92</code></td><td><code>SQL-92</code>。支持偏序的 <code>SORT BY</code>，可限制返回数量的 <code>LIMIT</code></td></tr><tr><td>连接</td><td><code>SQL-92</code> 支持或变相支持</td><td>内连接、外连接、半连接、映射连接、交叉连接</td></tr><tr><td>子查询</td><td>在任何子句中支持的或不相关的</td><td>只能在 <code>FROM</code>、 <code>WHERE</code> 或 <code>HAVING</code> 子句中</td></tr><tr><td>视图</td><td>可更新</td><td>只读</td></tr><tr><td>扩展点</td><td>用户定义函数、存储过程</td><td>用户定义函数、<code>MapReduce</code> 脚本</td></tr></tbody></table><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p><code>Hive</code> 支持原子和复杂数据类型。原子数据类型包括<strong>数值型</strong>、<strong>布尔型</strong>、<strong>字符串类型</strong>和<strong>时间戳类型</strong>。复杂数据类型包括<strong>数组</strong>、<strong>映射</strong>和<strong>结构</strong>。</p><p><code>Hive</code> 提供了普通 <code>SQL</code> 操作符，包括：<strong>关系操作符</strong>、<strong>算术操作符</strong>和<strong>逻辑操作符</strong>。</p><h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><p>用户自定义函数（<code>UDF</code>）是一个允许用户通过扩展 <code>HiveQL</code> 的强大功能，用户通过 <code>Java</code> 编写自己的 <code>UDF</code>，在将自定义函数加入用户会话后，就可以跟内置函数一样使用。<br><code>Hive</code> 提供了多种类型的用户自定义函数，每一种都会针对输入数据执行特定的转换过程，具体类型包含以下三种：</p><ul><li><strong><code>UDF (User Defined Function)</code></strong> ：一进一出。传入一个值，逻辑运算后返回一个值，如内置函数的 <code>floor</code>、<code>round</code> 等。</li><li><strong><code>UDAF (User Defined Aggregation Funtion)</code></strong> ：多进一出。传入多行数据，根据选定的值 <code>group by</code> 后返回一行结果，类似 <code>sum</code>、<code>count</code>。</li><li><strong><code>UDTF (User Defined Table Generating Functions)</code></strong> ：一进多出。基于特定的一行值输入，返回展开多行输出，类似内置函数 <code>explode</code>。</li></ul><p>创建自定义函数步骤如下：</p><ul><li><strong>编写自定义函数</strong><br>  引入依赖  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><strong>编译部署</strong><br>  自定义函数代码编写完成后，编译打包为 <code>jar</code> 文件。<br>  部署 <code>jar</code> 文件要根据部署模式进行调整，本地模式则是将 <code>jar</code> 文件采用本地模式部署，而非本地模式则是将 <code>jar</code> 文件放置到共享存储（<code>HHDFS</code>）上。  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地模式</span></span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">ADD jar hive-jar-test-1.0.0.jar</span></span><br><span class="line">Added hive-jar-test-1.0.0.jar to class path</span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">list jars;</span></span><br><span class="line">hive-jar-test-1.0.0.jar</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">非本地模式</span></span><br><span class="line">hadoop fs -put hive-jar-test-1.0.0.jar /usr/home/hive-jar-test-1.0.0.jar</span><br></pre></td></tr></table></figure></li><li><strong><code>Hive</code> 中注册函数</strong><br>  注册函数也被分为两种：<strong>临时函数</strong>和<strong>永久函数</strong>，临时注册函数用于解决一些临时特殊的业务需求开发的函数，<code>Hive</code> 注册的临时函数只在当前会话中可用，注册函数时需要使用 <code>temporary</code> 关键字声明。注册函数时未使用临时关键字 <code>temporary</code> 的都为永久函数，在所有会话中都可用。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE [temporary] FUNCTION [dbname.]function_name AS class_name [USING jar | file | archive &#x27;file_url&#x27;]</span><br></pre></td></tr></table></figure></li><li><strong>使用自定义函数</strong><br>  函数全名使用 <code>dbname.function_name</code> 表示，使用的时候可以直接用函数全名，但查询如果在当前库下操作，则使用函数名即可。  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> id, test_udf_lower(content) <span class="keyword">FROM</span> test_table_1 LIMIT <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><strong>销毁自定义函数</strong>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP [temporary] FUNCTION [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure></li></ul><h5 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a><code>UDF</code></h5><p>编写 <code>UDF</code>，需要继承 <code>org.apache.hadoop.hive.ql.exec.UDF</code> 并重写 <code>evaluate</code> 函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestUDF</span> <span class="keyword">extends</span> <span class="title class_">UDF</span> &#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里接受参数的类型必须是 Hadoop 支持的输入输出类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Test <span class="title function_">evaluate</span><span class="params">(<span class="keyword">final</span> Test x)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Text</span>(x.toString()).toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><small><code>UDF</code> 必须有返回类型，可以返回 <code>null</code>，但返回类型不能为 <code>void</code>。</small></p><h5 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a><code>UDAF</code></h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestUDAF</span> <span class="keyword">extends</span> <span class="title class_">AbstractGenericUDAFResolver</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> GenericUDAFEvaluator <span class="title function_">getEvaluator</span><span class="params">(TypeInfo[] parameters)</span></span><br><span class="line">            <span class="keyword">throws</span> SemanticException &#123;</span><br><span class="line">        <span class="keyword">if</span> (parameters.length != <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(parameters.length - <span class="number">1</span>, <span class="string">&quot;Exactly one argument is expected.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">ObjectInspector</span> <span class="variable">oi</span> <span class="operator">=</span> TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">if</span> (oi.getCategory() != ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(<span class="number">0</span>, <span class="string">&quot;Argument must be PRIMITIVE, but &quot;</span> + oi.getCategory().name() + <span class="string">&quot; was passed.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">PrimitiveObjectInspector</span> <span class="variable">inputOI</span> <span class="operator">=</span> (PrimitiveObjectInspector) oi;</span><br><span class="line">        <span class="keyword">if</span> (inputOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">UDFArgumentTypeException</span>(<span class="number">0</span>, <span class="string">&quot;Argument must be String, but &quot;</span> + inputOI.getPrimitiveCategory().name() + <span class="string">&quot; was passed.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TotalNumOfLettersEvaluator</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TotalNumOfLettersEvaluator</span> <span class="keyword">extends</span> <span class="title class_">GenericUDAFEvaluator</span> &#123;</span><br><span class="line">        PrimitiveObjectInspector inputOI;</span><br><span class="line">        ObjectInspector outputOI;</span><br><span class="line">        PrimitiveObjectInspector integerOI;</span><br><span class="line">        <span class="type">int</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> ObjectInspector <span class="title function_">init</span><span class="params">(Mode m, ObjectInspector[] parameters)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="keyword">assert</span>(parameters.length == <span class="number">1</span>);</span><br><span class="line">            <span class="built_in">super</span>.init(m, parameters);</span><br><span class="line">           </span><br><span class="line">            <span class="comment">//map阶段读取sql列，输入为String基础数据格式</span></span><br><span class="line">            <span class="keyword">if</span> (m == Mode.PARTIAL1 || m == Mode.COMPLETE) &#123;</span><br><span class="line">                inputOI = (PrimitiveObjectInspector) parameters[<span class="number">0</span>];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//其余阶段，输入为Integer基础数据格式</span></span><br><span class="line">            integerOI = (PrimitiveObjectInspector) parameters[<span class="number">0</span>];</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">            <span class="comment">// 指定各个阶段输出数据格式都为Integer类型</span></span><br><span class="line">            outputOI = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class, ObjectInspectorOptions.JAVA);</span><br><span class="line">            <span class="keyword">return</span> outputOI;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 存储当前字符总数的类</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">LetterSumAgg</span> <span class="keyword">implements</span> <span class="title class_">AggregationBuffer</span> &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> num)</span>&#123;</span><br><span class="line">            sum += num;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> AggregationBuffer <span class="title function_">getNewAggregationBuffer</span><span class="params">()</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="type">LetterSumAgg</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LetterSumAgg</span>();</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reset</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">        <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LetterSumAgg</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">warned</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">iterate</span><span class="params">(AggregationBuffer agg, Object[] parameters)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="keyword">assert</span> (parameters.length == <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (parameters[<span class="number">0</span>] != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">                <span class="type">Object</span> <span class="variable">p1</span> <span class="operator">=</span> ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[<span class="number">0</span>]);</span><br><span class="line">                myagg.add(String.valueOf(p1).length());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Object <span class="title function_">terminatePartial</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">            total += myagg.sum;</span><br><span class="line">            <span class="keyword">return</span> total;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">merge</span><span class="params">(AggregationBuffer agg, Object partial)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="keyword">if</span> (partial != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="type">LetterSumAgg</span> <span class="variable">myagg1</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">                <span class="type">Integer</span> <span class="variable">partialSum</span> <span class="operator">=</span> (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class="line">                <span class="type">LetterSumAgg</span> <span class="variable">myagg2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LetterSumAgg</span>();</span><br><span class="line">                </span><br><span class="line">                myagg2.add(partialSum);</span><br><span class="line">                myagg1.add(myagg2.sum);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Object <span class="title function_">terminate</span><span class="params">(AggregationBuffer agg)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">            <span class="type">LetterSumAgg</span> <span class="variable">myagg</span> <span class="operator">=</span> (LetterSumAgg) agg;</span><br><span class="line">            total = myagg.sum;</span><br><span class="line">            <span class="keyword">return</span> myagg.sum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数执行过程：</p><ul><li><code>PARTIAL1</code>：从原始数据到部分聚合数据的过程，会调用 <code>iterate()</code> 和 <code>terminatePartial()</code> 方法。<code>iterate()</code> 函数负责解析输入数据，而 <code>terminatePartial()</code> 负责输出当前临时聚合结果。该阶段可以理解为对应 <code>MapReduce</code> 过程中的 <code>Map</code> 阶段。</li><li><code>PARTIAL2</code>：从部分聚合数据到部分聚合数据的过程（多次聚合），会调用 <code>merge()</code> 和 <code>terminatePartial()</code> 方法。<code>merge()</code> 函数负责聚合 <code>Map</code> 阶段 <code>terminatePartial()</code> 函数输出的部分聚合结果，<code>terminatePartial()</code> 负责输出当前临时聚合结果。阶段可以理解为对应 <code>MapReduce</code> 过程中的 <code>Combine</code> 阶段。</li><li><code>FINAL</code>: 从部分聚合数据到全部聚合数据的过程，会调用 <code>merge()</code> 和 <code>terminate()</code> 方法。<code>merge()</code> 函数负责聚合 <code>Map</code> 阶段或者 <code>Combine</code> 阶段 <code>terminatePartial()</code> 函数输出的部分聚合结果。<code>terminate()</code> 方法负责输出 <code>Reduce</code> 阶段最终的聚合结果。该阶段可以理解为对应 <code>MapReduce</code> 过程中的 <code>Reduce</code> 阶段。</li><li><code>COMPLETE</code>: 从原始数据直接到全部聚合数据的过程，会调用 <code>iterate()</code> 和 <code>terminate()</code> 方法。可以理解为 <code>MapReduce</code> 过程中的直接 <code>Map</code> 输出阶段，没有 <code>Reduce</code> 阶段。</li></ul><p><small>还有另外一种实现方式是继承 <code>org.apache.hadoop.hive.ql.exec.UDAF</code>，并且包含一个或多个嵌套的、实现了 <code>org.apache.hadoop.hive.ql.UDAFEvaluator</code> 的静态类。</small></p><hr> <h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><h4 id="表"><a href="#表" class="headerlink" title="表"></a>表</h4><p><code>Hive</code> 的表在逻辑上由存储的数据和描述表中数据形式的相关元数据组成。数据一般存放在 <code>HDFS</code> 中，但它也可以放在其他任何 <code>Hadoop</code> 文件系统中，包括本地文件系统或 <code>S3</code>。<code>Hive</code> 把元数据存放在关系型数据库中，而不是放在 <code>HDFS</code> 中。</p><h5 id="托管表和外部表"><a href="#托管表和外部表" class="headerlink" title="托管表和外部表"></a>托管表和外部表</h5><p>在 <code>Hive</code> 中创建表时，默认情况下由 <code>Hive</code> 负责管理数据，这也就意味着要将数据移入仓库目录，被称为<strong>内部表</strong>。而另外一种则是<strong>外部表</strong>，数据存放在仓库目录以外的地方。</p><p>这两种表的区别在于 <code>LOAD</code> 和 <code>DROP</code> 命令的语义上：</p><ul><li><code>LOAD</code><br>  托管表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_1(content STRING);</span><br><span class="line">LOAD DATA INPATH <span class="string">&#x27;/usr/home/data.txt&#x27;</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> test_table_1;</span><br></pre></td></tr></table></figure>  加载数据到托管表时，会将数据文件移入到仓库目录下。<br>  外部表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> test_table_2 (content STRING) LOCATION <span class="string">&#x27;/usr/home/data.txt&#x27;</span>;</span><br><span class="line">LOAD DATA INPATH <span class="string">&#x27;/usr/home/data.txt&#x27;</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> test_table_2;</span><br></pre></td></tr></table></figure></li><li><code>DROP</code><br>  托管表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> test_table_1;</span><br></pre></td></tr></table></figure>  执行上述操作，会将其元数据和数据一起被删除，这也就是 <strong>“托管数据”</strong> 的含义。<br>  外部表：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> test_table_2;</span><br></pre></td></tr></table></figure>  删除外部表并不会删除数据，仅会删除元数据。</li></ul><p>那么如何选择并使用这两种类型的表呢？<br>有一个经验法则就是，如果需要所有的处理都由 <code>Hive</code> 完成，那么应该使用托管表。如果要使用其他的工具来处理数据集则使用外部表。</p><h5 id="分区和桶"><a href="#分区和桶" class="headerlink" title="分区和桶"></a>分区和桶</h5><p><code>Hive</code> 将表组织成<strong>分区（<code>partition</code>）</strong>，这是一种根据<strong>分区列（<code>partition column</code>）</strong>的值对表进行粗略划分的机制。使用分区可以加快数据分片（<code>slice</code>）的查询速度。</p><p>表或分区可以进一步分为<strong>桶（<code>bucket</code>）</strong>，会为数据提供额外的结构以获得更高效的查询处理。</p><ol><li><p>分区<br>一个表可以通过多个维度来进行分区。分区是在创建表的时候用 <code>PARTITIONED BY</code> 子句定义的，该子句需要定义列的列表。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_3 (ts <span class="type">INT</span>, line STRING)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (dt STRING, country STRING);</span><br></pre></td></tr></table></figure><p>而在文件系统级别，分区只是表目录下嵌套的子目录，将更多的文件加载到表目录之后，目录结构如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/usr/home/warehouse/test_table_3</span><br><span class="line">dt=2023-05-16</span><br><span class="line">country=CN</span><br><span class="line">file1</span><br><span class="line">file2</span><br><span class="line">country=EU</span><br><span class="line">file5</span><br><span class="line">file6</span><br><span class="line">dt=2023-05-17</span><br><span class="line">country=CN</span><br><span class="line">file3</span><br><span class="line">file4</span><br></pre></td></tr></table></figure><p>之后使用 <code>SHOW PARTITIONS</code> 命令显示表中的分区列表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">SHOW</span> PARTITIONS test_table_3</span><br><span class="line">dt<span class="operator">=</span><span class="number">2023</span><span class="number">-05</span><span class="number">-16</span><span class="operator">/</span>country<span class="operator">=</span>CN</span><br><span class="line">dt<span class="operator">=</span><span class="number">2023</span><span class="number">-05</span><span class="number">-16</span><span class="operator">/</span>country<span class="operator">=</span>EU</span><br><span class="line">dt<span class="operator">=</span><span class="number">2023</span><span class="number">-05</span><span class="number">-17</span><span class="operator">/</span>country<span class="operator">=</span>CN</span><br></pre></td></tr></table></figure><p><small><code>PARTITIONED BY</code> 子句中的列定义是表中正式的列，称为<strong>分区列（<code>partition column</code>）</strong>，但数据中并不包含这些列的值，而是源于目录名。</small><br>在日常的查询中以通常的方式使用分区列，<code>Hive</code> 会对输入进行修剪，从而只扫描相关分区。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ts <span class="keyword">FROM</span> test_table_3 <span class="keyword">where</span> country <span class="operator">=</span> <span class="string">&#x27;CN&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>桶<br>将表（或分区）组织成<strong>桶（<code>bucket</code>）</strong>有以下优点：</p><ul><li>获得更高的查询处理效率。桶为表增加了额外的结构，在处理查询时能够利用这个结构，具体为连接两个在相同列上划分了桶的表，可以使用 <code>map</code> 端连接高效地实现。</li><li>使取样更加高效。在处理大规模数据集时，能使用数据的一部分进行查询，会带来很多方便。</li></ul><p> <code>Hive</code> 使用 <code>CLUSTERED BY</code> 子句来指定划分桶所用的列和要划分的桶的个数：<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_4 (ts <span class="type">INT</span>, line STRING)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (id) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS;</span><br></pre></td></tr></table></figure></p><p> 在实际物理存储上，每个桶就是表（或分区）目录里的一个文件，其文件名并不重要，但桶 <code>n</code> 是按照字典序排列的第 <code>n</code> 个文件。事实上桶对应于 <code>MapReduce</code> 的输出文件分区：一个作业产生的桶和 <code>reduce</code> 任务个数相同。<br> 可以通过查看刚才创建的 <code>bucketed_users</code> 表的布局来了解这一情况：<br> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">dfs -<span class="built_in">ls</span> /usr/home/warehouse/bucketed_users;</span></span><br><span class="line">000000_0</span><br><span class="line">000001_0</span><br><span class="line">000002_0</span><br><span class="line">000003_0</span><br></pre></td></tr></table></figure></p><p> 使用 <code>TABLESAMPLE</code> 子句对表进行取样，可以获得相同的结果，该子句会查询限定在表的一部分桶内，而不是整个表：<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> bucketed_users <span class="keyword">TABLESAMPLE</span>(BUCKET <span class="number">1</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">4</span> <span class="keyword">ON</span> id);</span><br><span class="line"><span class="number">4</span> Ann</span><br><span class="line"><span class="number">0</span> Nat</span><br><span class="line"><span class="number">2</span> Joe</span><br></pre></td></tr></table></figure></p></li></ol><h5 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h5><p>前面已经使用 <code>LOAD DATA</code> 操作，通过把文件复制或移动到表的目录中，从而把数据导入到 <code>Hive</code> 的表（或分区）。也可以使用 <code>INSERT</code> 语句把数据从一个 <code>Hive</code> 表填充到另一个，或在新建表时使用 <strong><code>CTAS</code> （<code>CREATE TABLE ... AS SELECT</code>）</strong>结构。</p><ol><li><p><code>INSERT</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> target [<span class="keyword">PARTITION</span>(dt)] <span class="keyword">SELECT</span> col1, col2 <span class="keyword">FROM</span> source;</span><br></pre></td></tr></table></figure><p><code>OVERWRITE</code> 关键字意味着目标表（分区）中的内容会被替换掉。</p></li><li><p>多表插入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> source</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> table_by_year <span class="keyword">SELECT</span> <span class="keyword">year</span>, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">table</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">year</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> table_by_month <span class="keyword">SELECT</span> <span class="keyword">month</span>, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">table</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span></span><br></pre></td></tr></table></figure><p>这种<strong>多表插入</strong>比使用单独的 <code>INSERT</code> 效率更高，因为只需要扫描一遍源表就可以生成多个不相交的输出。</p></li><li><p><code>CTAS</code> 语句<br>将 <code>Hive</code> 查询的结果输出到一个新的表内是非常方便的，新表的列的定义是从 <code>SELECT</code> 子句所检索的列导出的。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> target</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> col1, col2 <span class="keyword">FROM</span> source;</span><br></pre></td></tr></table></figure></li></ol><h5 id="表的修改和删除"><a href="#表的修改和删除" class="headerlink" title="表的修改和删除"></a>表的修改和删除</h5><p>由于 <code>Hive</code> 使用<em>读时模式（<code>schema on read</code>）</em>，所以表在创建之后可以非常灵活地支持对表定义的修改。使用 <code>ALTER TABLE</code> 语句来重命名表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> source RENAME <span class="keyword">TO</span> target;</span><br></pre></td></tr></table></figure><p><small>此命令除更新元数据外，还会将表目录移动到对应的目录下。</small><br>另外也支持修改列的定义，添加新的列，甚至用一组新的列替换表内已有的列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> target <span class="keyword">ADD</span> COLUMNS (col3 STRING);</span><br></pre></td></tr></table></figure><p><code>DROP TABLE</code> 语句用于删除表的数据和元数据。如果是外部表则只删除元数据，数据不会受到影响。<br>但若是需要仅删除表内的数据，保留表的定义，则需要使用 <code>TRUNCATE TABLE</code> 语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> target;</span><br></pre></td></tr></table></figure><h4 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h4><h5 id="排序和聚集"><a href="#排序和聚集" class="headerlink" title="排序和聚集"></a>排序和聚集</h5><p><code>Hive</code> 中可以使用标准的 <code>ORDER BY</code> 子句对数据进行排序， <strong><code>ORDER BY</code></strong> 将对输入执行并行全排序。</p><p>但是在很多情况下，并不需要对结果全局排序，那么可以使用 <code>Hive</code> 的非标准的扩展 <strong><code>SORT BY</code></strong> ，<code>SORT BY</code> 为每一个 <code>reducer</code> 文件产生一个排序文件。<br><strong><code>DISTRIBUTE BY</code></strong> 子句可以控制某个特定列到 <code>reducer</code>，通常是为了后续的聚集操作。如果 <code>SORT BY</code> 和 <code>DISTRIBUTE BY</code> 中所使用的列相同，可以缩写为 <strong><code>CLUSTER BY</code></strong> 以便同时指定两者所用的列。</p><h5 id="MapReduce-脚本"><a href="#MapReduce-脚本" class="headerlink" title="MapReduce 脚本"></a><code>MapReduce</code> 脚本</h5><p>与 <code>Hadoop Streaming</code> 类似，<code>TRANSFORM</code>、 <code>MAP</code> 和 <code>REDUCE</code> 子句可以在 <code>Hive</code> 中调用外部脚本或程序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">(year, temp, q) = line.strip().split()</span><br><span class="line"><span class="keyword">if</span> (temp != <span class="string">&quot;9999&quot;</span> <span class="keyword">and</span> re.<span class="keyword">match</span>(<span class="string">&quot;[01459]&quot;</span>, q)):</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;%s\t%s&quot;</span> % (year, temp)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">ADD FILE /usr/home/ ./is_year.py</span></span><br><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">FROM record SELECT TRANSFORM(year, temp, name) USING <span class="string">&#x27;is_year.py&#x27;</span> AS year, temp;</span></span><br><span class="line">1950 0</span><br><span class="line">1950 22</span><br><span class="line">1949 111</span><br></pre></td></tr></table></figure><h5 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h5><p>子查询是内嵌在另一个 <code>SQL</code> 语句中的 <code>SELECT</code> 语句。<code>Hive</code> 对子查询的支持很有限，他只允许子查询出现在 <code>SELECT</code> 语句的 <code>FROM</code> 子句中 ，或者某些情况下的 <code>WHERE</code> 子句中。</p><h5 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h5><p>视图是一种用 <code>SELECT</code> 语句定义的<strong>虚表（<code>virtual table</code>）</strong>。视图可以用来以一种不同于磁盘实际存储的形式把数据呈现给用户，视图也可以用来限制用户，使其只能访问被授权的可以看到的子表。</p><p><code>Hive</code> 创建视图时并不把视图物化存储在磁盘上，相反视图的 <code>SELECT</code> 语句只是在执行引用视图的语句时才执行。<code>SHOW TABLES</code> 命令的输出结果里包含视图。还可以使用 <code>DESCRIBE EXTENDED view_name</code> 命令来查看某个视图的详细信息，包括用于定义它的那个查询。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> valid_table</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> col1, col3 <span class="keyword">FROM</span> target <span class="keyword">WHERE</span> <span class="keyword">year</span> <span class="operator">!=</span> <span class="string">&#x27;2022&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="EXPLAIN"><a href="#EXPLAIN" class="headerlink" title="EXPLAIN"></a><code>EXPLAIN</code></h5><p>在查询语句之前加上 <code>EXPLAIN</code> 关键字，就可以了解到查询计划和其他的信息来更加直观的展示 <code>Hive</code> 是如何将查询任务转化为 <code>MapReduce</code> 任务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">EXPLAIN SELECT <span class="built_in">sum</span>(col1) FROM target;</span></span><br></pre></td></tr></table></figure><p>首先会输出抽象语法树。其表明 <code>Hive</code> 是如何将查询解析为 <code>token</code>（符号）和 <code>literal</code>（字面值）的，是将查询转化到最终结果的一部分。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ABSTRACT SYNTAX TREE:</span><br><span class="line">(TOK_QUERY</span><br><span class="line">    (TOK_FROM (TOK_TABREF (TOK_TABNAME target)))</span><br><span class="line">    (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))</span><br><span class="line">    (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION sum (TOK_TABLE_OR_COL number))))))</span><br></pre></td></tr></table></figure><p>接下来可以看到列明 <code>col1</code>、 表明 <code>target</code> 还有 <code>sum</code> 函数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">    Stage-1 is a root stage</span><br><span class="line">    Stage-0 is a root stage</span><br></pre></td></tr></table></figure><p><small>一个 <code>Hive</code> 任务会包含一个或多个 <code>stage</code> 阶段，不同的 <code>stage</code> 阶段间会存在着依赖关系。一个 <code>stage</code> 可以是一个 <code>MapReduce</code> 任务，也可以是一个抽样阶段，或者一个合并阶段，还可以是一个 <code>limit</code> 阶段，以及 <code>Hive</code> 需要的其他任务的一个阶段。</small><br><small>默认情况下，<code>Hive</code> 一次只执行一个 <code>stage</code>（阶段）。</small></p><p>除此之外还可以使用 <code>EXPLAIN EXTENDED</code> 语句产生更多的输出信息。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;Hive&lt;/code&gt;&lt;/strong&gt; 是一个构建在 &lt;code&gt;Hadoop&lt;/code&gt; 之上的&lt;strong&gt;数据仓库框架&lt;/strong&gt;，其设计目的在于让精通 &lt;code&gt;SQL&lt;/code&gt; 但编程技能较弱的运营人员能够对存放在 &lt;code&gt;HDFS&lt;/code&gt; 中的大规模数据集执行查询。&lt;br&gt;但是由于其底层依赖的 &lt;code&gt;Hadoop&lt;/code&gt; 和 &lt;code&gt;HDFS&lt;/code&gt; 设计本身约束和局限性，限制 &lt;code&gt;Hive&lt;/code&gt; 不支持记录级别的更新、插入或者删除操作，不过可以通过查询生成新表或将查询结果导入文件中来实现。同时由于 &lt;code&gt;MapReduce&lt;/code&gt; 任务的启动过程需要消耗较长的时间，所以查询延时比较严重。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-Could not find first log file name in binary log index file</title>
    <link href="https://blog.vgbhfive.cn/MySQL-Could-not-find-first-log-file-name-in-binary-log-index-file/"/>
    <id>https://blog.vgbhfive.cn/MySQL-Could-not-find-first-log-file-name-in-binary-log-index-file/</id>
    <published>2023-04-02T12:32:07.000Z</published>
    <updated>2023-04-02T12:41:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>在之前的博客中说明过，我负责的业务有数据同步的需求，是从 <code>MySQL</code> 实时同步数据到 <code>ClickHouse</code>，为此我们使用了一个工具 <code>clickhouse-mysql-data-reader</code>，该工具的底层是通过监听 <code>MySQL</code> 的 <code>bin log</code> 来实现实时同步数据。</p><p>就在今早，数据同步不知为何停止了，当发现问题重新拉起同步任务时，就发现同步脚本出现了异常：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Could not find first log file name in binary log index file</span><br></pre></td></tr></table></figure><span id="more"></span><hr><h3 id="思考问题"><a href="#思考问题" class="headerlink" title="思考问题"></a>思考问题</h3><p>在看过异常报错信息之后，其大致是因为同步脚本停止了同步任务，之后就没有更新本地的 <code>bin log</code> 索引，此时等待我再拉起同步任务时，同步脚本使用本地未修改的 <code>bin log</code> 索引去 <code>MySQL</code> 拉取数据时，<code>MySQL</code> 的 <code>bin log</code> 索引经过业务数据的写入已经覆盖了之前的索引，同步脚本没有找到对应的索引就抛出异常。</p><hr><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>那么就下来的问题就是如何查找正确的 <code>bin log</code> 索引，然后修改同步脚本的 <code>bin log</code> 索引即可恢复同步数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show binary logs;</span><br></pre></td></tr></table></figure><p>上述命令可以帮助你查看到 <code>MySQL</code> 最新的 <code>bin log</code> 索引，之后同步修改脚本的 <code>bin log</code> 索引即可。</p><p><small>那么关于两次索引之间的数据如何同步呢？可以通过添加 <code>where case</code> 采用 <code>select</code> 的形式同步数据，更多的操作可以参考同步工具的使用文档。</small></p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;复现&quot;&gt;&lt;a href=&quot;#复现&quot; class=&quot;headerlink&quot; title=&quot;复现&quot;&gt;&lt;/a&gt;复现&lt;/h3&gt;&lt;p&gt;在之前的博客中说明过，我负责的业务有数据同步的需求，是从 &lt;code&gt;MySQL&lt;/code&gt; 实时同步数据到 &lt;code&gt;ClickHouse&lt;/code&gt;，为此我们使用了一个工具 &lt;code&gt;clickhouse-mysql-data-reader&lt;/code&gt;，该工具的底层是通过监听 &lt;code&gt;MySQL&lt;/code&gt; 的 &lt;code&gt;bin log&lt;/code&gt; 来实现实时同步数据。&lt;/p&gt;
&lt;p&gt;就在今早，数据同步不知为何停止了，当发现问题重新拉起同步任务时，就发现同步脚本出现了异常：&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Could not find first log file name in binary log index file&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="https://blog.vgbhfive.cn/tags/MySQL/"/>
    
    <category term="ClickHouse" scheme="https://blog.vgbhfive.cn/tags/ClickHouse/"/>
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-Spark</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-Spark/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-Spark/</id>
    <published>2023-03-12T03:06:30.000Z</published>
    <updated>2023-04-18T14:06:05.388Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p><code>Spark</code> 是用于<strong>处理大数据的集群计算框架</strong> ，与其他大多数数据处理框架不同之处在于 <code>Spark</code> 没有以 <code>MapReduce</code> 作为执行引擎，而是使用它自己的<strong>分布式运行环境</strong>在集群上执行工作。另外 <code>Spark</code> 与 <code>Hadoop</code> 又紧密集成，<code>Spark</code> 可以在 <code>YARN</code> 上运行，并支持 <code>Hadoop</code> 文件格式及其存储后端（例如 <code>HDFS</code>）。</p><p><code>Spark</code> 最突出的表现在于其能将 <strong>作业与作业之间的大规模的工作数据集存储在内存中</strong>。这种能力使得在性能上远超 <code>MapReduce</code> 好几个数量级，原因就在于 <code>MapReduce</code> 数据都是从磁盘上加载。根据 <code>Spark</code> 的处理模型有两类应用获益最大，分别是 <strong>迭代算法（即对一个数据集重复应用某个函数，直至满足退出条件）</strong>和 <strong>交互式分析（用户向数据集发出一系列专用的探索性查询）</strong> 。<br>另外 <code>Spark</code> 还因为其具有的 <strong><code>DAG</code> 引擎</strong>更具吸引力，原因在于 <code>DAG</code> 引擎可以处理任意操作流水线，并为用户将其转化为单个任务。</p><span id="more"></span><hr><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>从官方页面下载一个稳定版本的 <code>Spark</code> 二进制发行包（选择与当前使用 <code>Hadoop</code> 匹配的版本），然后在合适的位置解压文件包，并将 <code>Spark</code> 的解压路径添加到 <code>PATH</code> 中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tar -xzvf spark-x.y.z.tgz</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> PATH=~/spark-x.y.z/bin:<span class="variable">$PATH</span></span></span><br></pre></td></tr></table></figure><h4 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h4><p><code>Spark</code> 像 <code>MapReduce</code> 一样也有 <strong>作业（<code>job</code>）</strong>的概念，只不过 <code>Spark</code> 的作业相比 <code>MapReduce</code> 更加通用，因为 <code>Spark</code> 作业可以由任意的<strong>多阶段（<code>stages</code>）有向无环图（<code>DAG</code>）</strong>构成，其中每个阶段大致相当于 <code>MapReduce</code> 中的 <code>map</code> 阶段或 <code>reduce</code> 阶段。<br>这些阶段又被 <code>Spark</code> 运行环境分解为多个任务（<code>tash</code>），任务并行运行在分布式集群中的 <code>RDD</code> 分区上，类似于 <code>MapReduce</code> 中的任务。</p><p><code>Spark</code> 作业始终运行在应用（<code>application</code>）上下文中，它提供了 <code>RDD</code> 分组以及共享变量。一个应用可以串行或并行地运行多个作业，并为这些作业提供访问由同一应用的先前作业所缓存的 <code>RDD</code> 的机制。</p><h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a><code>RDD</code></h4><p><code>RDD</code> 又称<strong>弹性分布式数据集（<code>Resilient Distributed Dataset</code>，简称 <code>RDD</code>）</strong>是 <code>Spark</code> 最核心的概念，他是在<strong>集群中跨多个机器分区存储的一个只读的对象集合</strong> ，在 <code>Spark</code> 应用中首先会加载一个或多个 <code>RDD</code>，他们作为输入通过一系列转换得到一组目标 <code>RDD</code>，然后对这些目标 <code>RDD</code> 执行一个动作。<br><small>弹性分布式数据集中的“弹性”是指 <code>Spark</code> 可以通过重新安排计算来自动重建丢失的分区。</small></p><p><code>RDD</code> 是 <code>Spark</code> 最基本的抽象，<code>RDD</code> 关联着三个至关重要的属性：</p><ul><li>依赖关系。</li><li>分区（包括一些位置信息）</li><li>计算函数：<code>Partition =&gt; Iterator[T]</code></li></ul><p>这三大属性对于简单的 <code>RDD</code> 是不可或缺的，其他更高层的功能也都是基于这套模型构建的。首先，依赖关系列表会告诉 <code>Spark</code> 如何从必要的输入构建 <code>RDD</code>。其次，分区允许 <code>Spark</code> 将工作以分区为单位，分配到多个执行器上并行计算。最后，每个 <code>RDD</code> 都是一个计算函数 <code>compute</code>，可用于生成 <code>RDD</code> 所表示数据的 <code>Iterator[T]</code> 对象。</p><p>创建 <code>RDD</code> 共有三种方式：</p><ul><li>来自一个内存中的对象集合（也称为并行化一个集合）。适用于对少量的输入数据进行并行的 <code>CPU</code> 密集型计算。</li><li>使用外部存储器（例如 <code>HDFS</code>）中的数据集。创建一个外部数据集的引用。</li><li>对现有的 <code>RDD</code> 进行转换。</li></ul><h4 id="窄依赖和宽依赖"><a href="#窄依赖和宽依赖" class="headerlink" title="窄依赖和宽依赖"></a>窄依赖和宽依赖</h4><p>在对每一个 <code>RDD</code> 操作时，都会得到一个新的 <code>RDD</code>，那么前后的两个 <code>RDD</code> 就有了某种联系，即新的 <code>child RDD</code> 会依赖旧的 <code>parent RDD</code>。目前这些依赖关系被分为： <strong>窄依赖（<code>NarrowDependency</code>）</strong>和 <strong>宽依赖（<code>ShuffleDependency</code>）</strong>。</p><ol><li><p><strong>窄依赖</strong><br>官方解释为：<code>child RDD</code> 中的每个分区都依赖 <code>parent RDD</code> 中的一小部分分区。</p><p> <img src="https://s2.loli.net/2023/04/17/OyXb2IrHGMF3z5J.jpg" alt="hadoop_spark_10.jpg"></p><p> 上图包括了有关窄依赖的各种依赖情况：</p><ul><li><strong>一对一依赖</strong>：<code>child RDD</code> 中的每个分区都只依赖 <code>parent RDD</code> 中的一个分区，并且 <code>child RDD</code> 的分区数和 <code>parent RDD</code> 的分区数相同。属于这种依赖关系的转换算子有 <code>map()</code>、<code>flatMap()</code>、<code>filter()</code> 等。</li><li><strong>范围依赖</strong>：<code>child RDD</code> 和 <code>parent RDD</code> 的分区经过划分，每个范围内的父子 <code>RDD</code> 的分区都为一一对应的关系。属于这种依赖关系的转换算子有 <code>union()</code> 等。</li><li><strong>窄依赖</strong>：窄依赖可以理解为一对一依赖和范围依赖的组合使用。属于这种依赖关系的转换算子有 <code>join()</code>、<code>cartesian()</code>、<code>cogroup()</code> 等。</li></ul></li><li><p><strong>宽依赖</strong><br>宽依赖官方解释为需要两个 <code>shuffle</code> 的两个 <code>stage</code> 的依赖。</p><p> <img src="https://s2.loli.net/2023/04/17/BzWSR42FvDLtMql.jpg" alt="hadoop_spark_11.jpg"></p><p> <code>child RDD</code> 的一个分区依赖的是 <code>parent RDD</code> 中各个分区的某一部分，即 <code>child RDD</code> 的两个分区分别只依赖 <code>parent RDD</code> 中的部分，而计算出某个部分的过程，以及 <code>child RDD</code> 分别读取某个部分的过程（<code>shuffle write/shuffle read</code>），此过程正是 <code>shuffle</code> 开销所在。</p></li></ol><h4 id="转换和动作"><a href="#转换和动作" class="headerlink" title="转换和动作"></a>转换和动作</h4><p><code>Spark</code> 对 <code>RDD</code> 提供了两大操作：<strong>转换（<code>transformation</code>）</strong> 和 <strong>动作（<code>action</code>）</strong> 。转换是从现有 <code>RDD</code> 生成新的 <code>RDD</code>，而动作则触发对 <code>RDD</code> 的计算并对计算结果执行某种操作，要么返回给用户，要么保存到外部存储器中。</p><p>加载 <code>RDD</code> 或者执行转换并不会立即触发任何数据处理的操作，只不过是创建了一个计划，只有当对 <code>RDD</code> 执行某个动作时才会触发真正的计算。<br><small>如果想判断一个操作是转换还是动作，可以通过观察其返回类型：如果返回的类型是 <code>RDD</code>，那么他是一个转换否则就是一个动作。</small></p><p>在 <code>Spark</code>库中包含了丰富的操作，包含映射、分组、聚合、重新分区、采样、连接 <code>RDD</code> 以及把 <code>RDD</code> 作为集合来处理的各种转换，同时还包括将 <code>RDD</code> 物化为集合；对 <code>RDD</code> 进行统计数据的计算；从一个 <code>RDD</code> 中采样固定数量的元素；以及将 <code>RDD</code> 保存到外部存储器等各种动作。</p><h4 id="Lineage-机制"><a href="#Lineage-机制" class="headerlink" title="Lineage 机制"></a><code>Lineage</code> 机制</h4><p>相比其他系统的细颗粒度的内存数据更新级别的备份或者 <code>LOG</code> 机制，<code>RDD</code> 的 <strong><code>Lineage</code> 记录的是粗颗粒度的特定数据的 <code>Transformation</code> 操作</strong>（如 <code>filter</code>、<code>map</code>、<code>join</code> 等）行为。<br>当某个 <code>RDD</code> 的部分分区数据丢失时，它可以通过 <code>Lineage</code> 记录获取足够的信息来重新运算和恢复丢失的数据分区，该记录的内容就是前面提到的 <code>RDD</code> 之间的依赖关系。</p><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p><code>Spark</code> 可以在跨集群的内存中<strong>缓存数据</strong> ，也就意味着对数据集所做的任何计算都会非常快。相比较而言，<code>MapReduce</code> 在执行另一个计算时必须从磁盘重新加载输入数据集，即使他们可以使用中间数据集作为输入，但也无法摆脱始终从磁盘加载数据的事实，也必然影响其执行速度。<br><small>被缓存的 <code>RDD</code> 只能由同一应用的作业来读取。同理，应用终止时，作业所缓存的 <code>RDD</code> 都会被销毁，除非这些 <code>RDD</code> 已经被持久化保存，否则无法访问。</small></p><p>默认的持久化级别共分为两类： <strong><code>MEMORY_ONLY</code></strong> 是默认持久化级别，使用对象在内存中的常规表示方式； <strong><code>MEMORY_ONLY_SER</code></strong> 是一种更加紧凑的表示方法，通过把分区中的元素序列化为字节数组来实现的。<br><code>MEMORY_ONLY_SER</code> 相比 <code>MEMORY_ONLY</code> 多了一笔 <code>CPU</code> 的开销，但若是生成的序列化 <code>RDD</code> 分区的大小适合被保存在内存中，而默认的持久化方式无法做到，那就说明额外的开销是值得。另外 <code>MEMORY_ONLY_SER</code> 还可以减少垃圾回收的压力，因为每个 <code>RDD</code> 被存储为一个字节数组而不是大量的对象。</p><p>默认情况下，<code>RDD</code> 分区的序列化使用的是 <code>Kryo</code> 序列化方法，通过压缩序列化分区可以进一步节省空间，而这通常是更好的选择。<br><small>将 <code>spark.rdd.compress</code> 属性设置为 <code>true</code>，并且可选地设置 <code>spark.io.compression.codec</code> 属性。</small></p><h4 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h4><p>在使用 <code>Spark</code> 的序列化时，需要从两个方面来考虑：</p><ul><li><strong>数据序列化</strong></li><li><strong>函数序列化（闭包函数）</strong></li></ul><h5 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h5><p>数据序列化在默认情况下，<code>Spark</code> 在通过网络将数据从一个 <code>executor</code> 发送到另一个 <code>executor</code> 时，或者以序列化的形式缓存（持久化）数据时，所使用的都是 <code>Java</code> 序列化机制。<br>使用 <code>Kryo</code> 序列化机制对于大多数 <code>Spark</code> 应用都是更好的选择，<code>Kryo</code> 是一个高效的通用 <code>Java</code> 序列化库，要想使用 <code>Kryo</code> 序列化机制，需要在应用中的 <code>SparkConf</code> 中设置 <code>spark.serializer</code> 属性，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br></pre></td></tr></table></figure><p><strong><code>Kryo</code></strong> 不要求被序列化的类实现某个特性的的接口，因此如果旧的对象需要使用 <code>Kryo</code> 序列化也是可以的，在配置启用 <code>Kryo</code> 序列化之后就可以使用了，不过话虽如此，若是使用前可以在 <code>Kryo</code> 中对这些类进行注册，那么就可以提高其性能。这是因为 <code>Kryo</code> 需要写被序列化对象的类的引用，如果已经引用已经注册的类，那么引用标识就只是一个整数，否则就是完整的类名。<br>在 <code>Kryo</code> 注册类很简单，创建一个 <code>KryoRegistrator</code> 子类，重写 <code>registerClasses()</code> 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomKryoRegistrator</span> <span class="keyword">extends</span> <span class="title class_">KryoRegistrator</span> &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">registerClasses</span><span class="params">(Kryo kryo)</span> &#123;</span><br><span class="line">      kryo.register(Object.class);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后在 <code>driver</code> 应用中将 <code>spark.kryo.registrator</code> 属性设置为你的 <code>KryoRegistrator</code> 实现的完全限定类名：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;spark.kryo.registrator&quot;, &quot;CustomKryoRegistrator&quot;)</span><br></pre></td></tr></table></figure><h5 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h5><p>通常函数的序列化都会<strong>谨守本分</strong>，对于 <code>Spark</code> 来说，即使在本地模式下，也需要序列化函数，假若引入一个不可序列化的函数，那么应该在开发期间就应该发现。</p><h4 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h4><p><code>Spark</code> 应用可能会使用一些不属于 <code>RDD</code> 的数据，这些数据会被作为闭包函数的一部分被序列化后传递给下一个动作，这可以保证应用正常执行，但使用广播变量可以跟高效的完成相同的工作。</p><h5 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h5><p><strong>广播变量（<code>broadcast variable</code>）</strong>在经过序列化后被发送给各个 <code>executor</code>，然后缓存在那里，以便后期任务可以在需要时访问它。它与常规变量不同，常规变量是作为闭包函数的一部分被序列化的，因此他们在每个任务中都要通过网络被传输一次。<br>广播变量的作用类似于 <code>MapReduce</code> 中的分布式缓存，两者的不同之处在于 <code>Spark</code> 将数据保存到内存中，只有在内存耗尽时才会溢出到磁盘上。</p><h5 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h5><p><strong>累加器（<code>accumulator</code>）</strong>是在任务中只能对它做加法的共享变量，类似于 <code>MapReduce</code> 中的计数器。当作业完成后 <code>driver</code> 程序可以检索累加器的最终值。</p><hr><h3 id="作业运行机制"><a href="#作业运行机制" class="headerlink" title="作业运行机制"></a>作业运行机制</h3><p>在 <code>Spark</code> 作业的最高层，他有两个独立的实体： <strong><code>driver</code></strong> 和 <strong><code>executor</code></strong> 。<code>driver</code> 负责托管应用（<code>SparkContext</code>）并为作业调度任务。<code>executor</code> 专属于应用，他在应用运行期间运行，并执行该应用的任务。通常 <code>driver</code> 作为一个不由集群管理器（<code>cluster manager</code>）管理的客户端来运行，而 <code>executor</code> 则运行在集群的计算机上。 </p><h4 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h4><p>当对 <code>RDD</code> 执行一个动作时，会自动提交一个 <code>Spark</code> 作业。从内部看导致对 <code>SparkContext</code> 调用 <code>runJob()</code> 方法，然后将调用传递给作为 <code>driver</code> 的一部分运行的调度程序。调度程序由两部分组成： <strong><code>DAG</code> 调用程序</strong> 和 <strong>任务调度程序</strong> 。<code>DAG</code> 调度程序把作业分解为若干阶段，并由这些阶段组成一个 <code>DAG</code>。任务调度程序则负责把每个阶段中的任务提交到集群。</p><p><img src="https://s2.loli.net/2023/04/10/z1HoGR53NuTWdbI.jpg" alt="hadoop_spark_1.jpg"></p><h4 id="DAG-构建"><a href="#DAG-构建" class="headerlink" title="DAG 构建"></a><code>DAG</code> 构建</h4><p>要想了解一个作业如何被划分为阶段，首先需要了解在阶段中运行的任务的类型。有两种类型的任务： <strong><code>shuffle map</code> 任务</strong> 和 <strong><code>result</code> 任务</strong> ，从任务类型的名称可以看出 <code>Spark</code> 会怎样处理任务的输出。</p><ul><li><code>shuffle map</code> 任务。<br> 顾名思义 <code>shuffle map</code> 任务类似于 <code>MapReduce</code> 中 <code>shuffle</code> 的 <code>map</code> 端部分。每个 <code>shuffle map</code> 任务在一个 <code>RDD</code> 分区上运行计算，并根据分区函数把输出写入到一组新的分区中，以允许在后面的阶段中取用（后面的阶段可能由 <code>shuffle map</code> 任务组成，也可能由 <code>result</code> 任务组成），<code>shuffle map</code> 任务运行在除最终阶段之外的其他所有阶段中。</li><li><code>result</code> 任务。<br> <code>result</code> 任务运行在最终阶段，并将结果返回给用户程序。每个 <code>result</code> 任务在他自己的 <code>RDD</code> 分区上运行计算，然后把结果发送回 <code>driver</code>，再由 <code>driver</code> 将每个分区的计算结果汇集成最终结果。</li></ul><p>最简单的 <code>Spark</code> 作业不需要使用 <code>shuffle</code>，因此它只有一个由 <code>result</code> 任务构成阶段，就像是 <code>MapReduce</code> 中仅有 <code>map</code> 一样。而比较复杂的作业要涉及到分组操作，并且要求一个或多个 <code>shuffle</code> 阶段。</p><p>如果 <code>RDD</code> 已经被同一应用（<code>SparkContext</code>）中先前的作业持久化保存，那么 <code>DAG</code> 调度程序将会省掉一些任务，不会再创建一些阶段来重新计算（或者它的父 <code>RDD</code>）。</p><p><code>DAG</code> 调度程序负责将一个阶段分解为若干任务以提交给任务调度程序。另外 <code>DAG</code> 调度程序会为每个任务赋予一个位置偏好（<code>placement preference</code>），以允许任务调度程序充分利用数据本地化（<code>data locality</code>）。</p><h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><p>当任务集合被发送到任务调度程序后，任务调度程序用该应用运行的 <code>executor</code> 的列表，在斟酌位置偏好的同时构建任务到 <code>executor</code> 的映射。接着任务调度程序将任务分配给具有内核的 <code>executor</code>，并且在 <code>executor</code> 完成运行任务时继续分配更多的任务，直到任务集合全部完成。默认情况下，每个任务到分配一个内核，不过也可以通过设置 <code>spark.task.cpus</code> 来更改。<br><small>任务调度程序在为某个 <code>executor</code> 分配任务时，首先分配的是进程本地化（<code>process-local</code>）任务，再分配节点本地（<code>node-local</code>）任务，然后分配机架本地（<code>rack-local</code>）任务，最后分配任意（非本地）任务或者推测任务（<code>speculative task</code>）。</small></p><p>这些被分配的任务通过调度程序后端启动。调度程序后端向 <code>executor</code> 后端发送远程启动任务的消息，以告知 <code>executor</code> 开始运行任务。</p><p>当任务成功完成或者失败时，<code>executor</code> 都会向 <code>driver</code> 发送状态更新信息。如果失败任务调度程序将在另一个 <code>executor</code> 上重新提交任务。若是启用推测任务（默认情况下不启用），它还会为运行缓慢的任务启动推测任务。</p><h4 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h4><p><code>executor</code> 首先确保任务的 <code>JAR</code> 包和文件依赖关系都是最新的，<code>executor</code> 在本地高速缓存中保留了先前任务已使用的所有依赖，因此只有在更新的情况下才会重新下载。接下来由于任务代码是以启动任务消息的一部分而发送的序列化字节，因此需要反序列化任务代码（包括用户自己的函数）。最后执行任务代码，不过需要注意的是因为运行任务在于 <code>executor</code> 相同的 <code>JVM</code> 中，因此任务的启动没有进程开销。</p><p>任务可以向 <code>driver</code> 返回执行结果，这些执行结果被序列化并发送到 <code>executor</code> 后端，然后以状态更新消息的形式返回 <code>driver</code>。<code>shuffle map</code> 任务返回的是一些可以让下一个阶段检索其输出分区的消息，而 <code>result</code> 任务则返回其运行的分区的结果值，<code>driver</code> 将这些结果值收集起来，并把最终结果返回给用户的程序。</p><h4 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h4><p><code>Spark</code> 如何依靠 <code>executor</code> 来运行构成 <code>Spark</code> 作业的任务，负责管理 <code>executor</code> 生命周期的是集群管理器（<code>cluster manager</code>），同时 <code>Spark</code> 提供了多种具有不同特性的集群管理器：</p><ul><li>本地模式<br> 在使用本地模式时，有一个 <code>executor</code> 与 <code>driver</code> 运行在同一个 <code>JVM</code> 中。此模式对于测试或运行小规模作业非常有用。</li><li>独立模式<br> 独立模式的集群管理器是一个简单的分布式实现，它运行了一个 <code>master</code> 以及一个或多个 <code>worker</code>。当 <code>Spark</code> 应用启动时，<code>master</code> 要求 <code>worker</code> 代表应用生成多个 <code>executor</code> 进程，这种模式的主 <code>URL</code> 为 <code>spark://host:port</code>。</li><li><strong><code>Mesos</code> 模式</strong><br> <code>Apache Mesos</code> 是一个通用的集群资源管理器，它允许根据组织策略在不同的应用之间细化资源共享。默认情况下（细粒度模式）每个 <code>Spark</code> 任务被当作是一个 <code>Mesos</code> 任务运行，这样做可以更有效地使用集群资源，但是以额外的进程启动开销为代价。在粗粒度模式下 <code>executor</code> 在进程中运行任务，因此在 <code>Spark</code> 应用运行期间的集群资源由 <code>executor</code> 进程来掌管，这种模式的主 <code>URL</code> 为 <code>mesos://host:port</code>。</li><li><strong><code>YARN</code> 模式</strong><br> <code>YARN</code> 是 <code>Hadoop</code> 中使用的资源管理器，每个运行的 <code>Spark</code> 应用对应于一个 <code>YARN</code> 应用实例，每个 <code>executor</code> 在自己的 <code>YARN</code> 容器中运行，这种模式的主 <code>URL</code> 为 <code>yarn-client</code> 或 <code>yarn-cluster</code>。<br> <small><code>YARN</code> 是唯一一个能够与 <code>Hadoop</code> 的 <code>Kerberos</code> 安全机制集成的集群管理器。</small></li></ul><h5 id="运行在-YARN-上的-Spark"><a href="#运行在-YARN-上的-Spark" class="headerlink" title="运行在 YARN 上的 Spark"></a>运行在 <code>YARN</code> 上的 <code>Spark</code></h5><p>在 <code>YARN</code> 上运行 <code>Spark</code> 提供了与其他 <code>Hadoop</code> 组件最紧密的集成，为了在 <code>YARN</code> 上运行，<code>Spark</code> 提供了两种部署模式： <strong><code>YARN</code> 客户端模式</strong>和 <strong><code>YARN</code> 集群模式</strong>。<code>YARN</code> 客户端模式的 <code>driver</code> 在客户端运行，而 <code>YARN</code> 集群模式的 <code>driver</code> 在 <code>YARN</code> 的 <code>application master</code> 集群上运行。</p><p>对于具有任何交互式组件的程序都必须使用 <code>YARN</code> 客户端模式，在交互式组件上的任何调试都是立即可见的。<br>另一方面 <code>YARN</code> 集群模式适用于生成作业（<code>production job</code>），因为整个应用在集群上运行，这样更易于保留日志文件（包括来自 <code>driver</code> 的日志文件）以供稍后检查。如果 <code>application master</code> 出现故障，<code>YARN</code> 还可以尝试重新运行该应用。</p><ol><li><p><code>YARN</code> 客户端模式<br>在 <code>YARN</code> 客户端模式下，当 <code>driver</code> 构建新的 <code>SparkContext</code> 实例时就会启动与 <code>YARN</code> 之间的交互，该 <code>SparkContext</code> 向 <code>YARN</code> 资源管理器提交一个 <code>YARN</code> 应用，而 <code>YARN</code> 资源管理器则启动集群节点管理器上的 <code>YARN</code> 容器，并在其中运行一个名为 <code>SparkExecutorLauncher</code> 的 <code>application master</code>。该 <code>ExecutorLauncher</code> 的工作是启动 <code>YARN</code> 容器中的 <code>executor</code>，为了做到这一点 <code>ExecutorLauncher</code> 要向资源管理器请求资源，然后启动 <code>ExecutorBackend</code> 进程作为分配给它的容器。</p><p><img src="https://s2.loli.net/2023/04/10/A4U8BWLM21ix9a5.jpg" alt="hadoop_spark_2.jpg"></p><p>每个 <code>executor</code> 在启动时都会连接回 <code>SparkContext</code>，并注册自身。因此这就向 <code>SparkContext</code> 提供了关于可用于运行任务的 <code>executor</code> 的数量及其位置的信息，之后这些信息会被用在任务的位置偏好策路中。</p><p><code>YARN</code> 资源管理器的地址并没有在主 <code>URL</code> 中指定(这与使用独立模式或 <code>Mesos</code> 模式的集群管理器不同)，而是从 <code>HADOOP_CONF_DIR</code> 环境变量指定的目录中的 <code>Hadoop</code> 配置中选取。</p></li><li><p><code>YARN</code> 集群模式<br>在 <code>YARN</code> 集群模式下，用户的 <code>driver</code> 程序在 <code>YARN</code> 的 <code>application master</code> 进程中运行，<code>spark-submit</code> 客户端将会启动 <code>YARN</code> 应用，但是它不会运行任何用户代码。剩余过程与客户端模式相同，除了 <code>application master</code> 在为 <code>executor</code> 分配资源之前先启动 <code>driver</code> 程序外。</p><p><img src="https://s2.loli.net/2023/04/10/sMCpmWhGg5qeiV1.jpg" alt="hadoop_spark_3.jpg"></p></li></ol><p>在这两种 <code>YARN</code> 模式下，<code>executor</code> 都是在还没有任何本地数据位置信息之前先启动的，因此最终有可能会导致 <code>executor</code> 与存有作业所希望访同文件的 <code>datanode</code> 不在一起。而这些对于交互式会话是可以接受的，特别是因为会话开始之前可能开不知道需要访问哪些数据集。但是对于生成作业来说情况并非如此，所以 <code>Spark</code> 提供了一种方法，可以在 <code>YARN</code> 群集模式下运行时提供一些有关位置的提示，以提高数据本地性。</p><p><code>sparkContext</code> 构造函数可以使用第二个参数来传递一个优选位置，该优选位置是利用 <code>InputFormatInfo</code> 辅助类根据输人格式和路径计算得到的。因此当向资源管理器请求分配时 <code>application master</code> 需要用到这个优选位置。</p><hr><h3 id="数据结构化-DataFrame"><a href="#数据结构化-DataFrame" class="headerlink" title="数据结构化 DataFrame"></a>数据结构化 <code>DataFrame</code></h3><p><code>Spark</code> 的 <strong><code>DataFrame</code> 是结构化的、有格式的</strong> ，且支持一些特定的操作，就像分布式内存中的表那样，每列都有名字，有表结构定义，每列都有特定的数据类型：整数、字符串型、数组、映射表、实数、日期、时间戳等。另外 <code>DataFrame</code> 中的数据是不可变的，<code>Spark</code> 记录着所有转化操作的血缘关系。可以添加列或者改变已有列的名字和数据类型，这些操作都会创建新的 <code>DataFrame</code> ，原有的 <code>DataFrame</code> 则会保留。在 <code>DataFrame</code> 中，一列与其名字和对应的 <code>Spark</code> 数据类型都在表结构中定义。</p><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><p>基本数据类型：</p><ul><li><code>ByteType</code></li><li><code>ShortType</code></li><li><code>IntegerType</code></li><li><code>LongType</code></li><li><code>FloatType</code></li><li><code>DoubleType</code></li><li><code>StringType</code></li><li><code>BooleanType</code></li><li><code>DecimalType</code></li></ul><p>复杂数据类型：</p><ul><li><code>BinaryType</code></li><li><code>TimestampType</code></li><li><code>DateType</code></li><li><code>ArrayType</code></li><li><code>MapType</code></li><li><code>StructType</code></li><li><code>StructField</code></li></ul><h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><p><code>Spark</code> 中的表结构为 <code>DataFrame</code> 定义了各列的名字和对应的数据类型。从外部数据源读取结构化数据时，表结构就会派上用场。相较于在读取数据时确定数据结构，提前定义表结构有如下优点：</p><ul><li>可以避免 <code>Spark</code> <strong>推断数据类型的额外开销</strong>。</li><li>可以防止 <code>Spark</code> 为决定表结构而单独创建一个作业来从数据文件读取很大一部分内容，对于较大的数据文件而言，其<strong>耗时相当长</strong> 。</li><li>可以<strong>尽早发现数据与表结构不匹配</strong> 。</li></ul><p>定义表结构有两种方式：</p><ul><li>编程的方式。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">schema = StructType([StructField(<span class="string">&quot;author&quot;</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">   StructField(<span class="string">&quot;title&quot;</span>, StringType(), <span class="literal">False</span>),</span><br><span class="line">   StructField(<span class="string">&quot;pages&quot;</span>, IntegerType(), <span class="literal">False</span>)])</span><br></pre></td></tr></table></figure></li><li>使用数据定义语言（<code>data definition language, DDL</code>）。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schema = &quot;author STRING, title STRING, pages INT&quot;</span><br></pre></td></tr></table></figure></li></ul><h4 id="行与列"><a href="#行与列" class="headerlink" title="行与列"></a>行与列</h4><p><code>DataFrame</code> 中的具名列与 <code>Pandas</code> 中的 <code>DataFrame</code> 对象的具名列，以及关系数据库表的列的概念上是类似的：描述的都是一种字段。<br><small><code>Spark</code> 的文档对列有 <code>col</code> 和 <code>Column</code> 两种表示。<code>Column</code> 是列对象的类名，而 <code>col()</code> 是标准的内建函数，返回一个 <code>Column</code> 对象。</small><br><code>DataFrame</code> 中的 <code>Column</code> 对象不能单独存在，在一条记录中，每一列都是行的一部分，所有的行共同组成整个 <code>DataFrame</code>。</p><p><code>Spark</code> 中的行是用 <code>Row</code> 对象来表示的，它包含一列或多列，各列既可以是相同的类型，也可以是不同的类型。由于 <code>Row</code> 是 <code>Spark</code> 中的对象，表示一系列字段的有序集合，因此可以在编程中很容易的实例化 <code>Row</code> 对象，并用自 <code>0</code> 开始的下标访问该对象的各字段。</p><h4 id="表与视图"><a href="#表与视图" class="headerlink" title="表与视图"></a>表与视图</h4><p>表存放数据，<code>Spark</code> 中的每张表都关联有相应的元数据，而这些元数据是表及其数据的一些信息，包括表结构、描述、表名、数据库名、列名、分区、实际数据所在的物理位置等，这些全都存放在中心化的元数据库中。<br><code>Spark</code> 没有专门的元数据库，<strong>默认使用 <code>Apache Hive</code> 的元数据库来保存表的所有数据</strong> ，仓库路径位于 <code>/user/hive/warehouse</code>。如果想要想要修改默认路径，可以修改 <code>Spark</code> 配置变量 <code>spark.sql.warehouse.dir</code> 为别的路径，这个路径既可以是本地路径，也可以是外部的分布式存储。</p><h5 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h5><p><code>Spark</code> 允许创建两种表：</p><ul><li><strong>有管理表</strong><br><code>Spark</code> 既管理元数据，也管理文件存储上的数据。这里的文件存储可以理解为本地文件系统或 <code>HDFS</code>，也可以是外部的对象存储系统。</li><li><strong>无管理表</strong><br><code>Spark</code> 只管理元数据，需要自行管理外部数据源中的数据。</li></ul><h5 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h5><ol><li><p>创建数据库和表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE demo_db;</span><br><span class="line">USE demo_db;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> demo_table_1 (<span class="type">date</span> STRING, delay <span class="type">INT</span>, distance <span class="type">INT</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> demo_table_2 (<span class="type">date</span> STRING, delay <span class="type">INT</span>, distance <span class="type">INT</span>) <span class="keyword">USING</span> csv OPTIONS (PATH <span class="string">&#x27;/data/learing/data.csv&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>新增视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> REPLACE TEMP <span class="keyword">VIEW</span> [global_temp.]test_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="type">date</span>, delay, distance <span class="keyword">FROM</span> demo_table_1 <span class="keyword">WHERE</span> distance <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure></li><li><p>缓存 <code>SQL</code> 表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CACHE [LAZY] <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span></span><br><span class="line">UNCACHE <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h4><h5 id="DataFrameReader"><a href="#DataFrameReader" class="headerlink" title="DataFrameReader"></a><code>DataFrameReader</code></h5><p><code>DataFrameReader</code> 是从数据源读取数据到 <code>DataFrame</code> 所用到的核心结构。用法有固定的格式和推荐的使用模式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrameReader.format(args).option(&quot;key&quot;, &quot;value&quot;).schema(args).load()</span><br></pre></td></tr></table></figure><p>这种将一串方法串联起来使用的模式在 <code>Spark</code> 中很常见，可读性也不错。</p><p>需要注意的是只能通过 <code>SparkSession</code> 实例访问 <code>DataFrameReader</code>，也就是说不能自行创建 <code>DataFrameReader</code> 实例，获取该实例的方法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.read // 返回 DataFrameReader 从静态数据源读取 DataFrame</span><br><span class="line">SparkSession.readStream // 返回的实例用于读取流式数据源</span><br></pre></td></tr></table></figure><p><code>DataFrameReader</code> 的公有方法如下：</p><table><thead><tr><th>方法</th><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>format()</code></td><td><code>&quot;parquet&quot;</code>、 <code>&quot;csv&quot;</code>、 <code>&quot;txt&quot;</code>、 <code>&quot;json&quot;</code>、 <code>&quot;orc&quot;</code>、 <code>&quot;avro&quot;</code></td><td>如果不指定方法的格式，则使用 <code>spark.sql.sources.default</code> 所指定的默认格式</td></tr><tr><td><code>option()</code></td><td><code>(&quot;mode&quot;, [PERMISSIVE FAILFAST DROPMALFORMAD])</code> 、 <code>(&quot;inferSchema&quot;, [true false])</code>、<code>(&quot;path&quot;, &quot;path_file_data_source&quot;)</code></td><td>一系列键值对，<code>Spark</code> 文档中解释了不同模式下的对应行为</td></tr><tr><td><code>schema()</code></td><td><code>DDL</code> 字符串或 <code>StructType</code> 对象</td><td>对于 <code>JSON</code> 或者 <code>CSV</code> 格式，可以使用 <code>option()</code> 方法自行推断表结构</td></tr><tr><td><code>load()</code></td><td><code>/path/source</code></td><td>要读取的数据源路径</td></tr></tbody></table><p><small>从静态的 <code>Parquet</code> 数据源读取数据不需要提供数据结构，因为 <code>Parquet</code> 文件的元数据通常包含表结构信息。不过对于流式数据源，表结构信息是需要提供的。</small></p><h5 id="DataFrameWriter"><a href="#DataFrameWriter" class="headerlink" title="DataFrameWriter"></a><code>DataFrameWriter</code></h5><p><code>DataFrameWriter</code> 是 <code>DataFrameReader</code> 的反面，将数据保存或写入特定的数据源。与 <code>DataFrameReader</code> 不同，<code>DataFrameWriter</code> 需要从保存的 <code>DataFrame</code> 获取，推荐的使用模式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataFrameWriter.format(args).option(args).bucketBy(args).partitionBy(args).save(path)</span><br><span class="line">DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)</span><br></pre></td></tr></table></figure><p><code>DataFrameWriter</code> 的公有方法如下：</p><table><thead><tr><th>方法</th><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>format()</code></td><td><code>&quot;parquet&quot;</code>、 <code>&quot;csv&quot;</code>、 <code>&quot;txt&quot;</code>、 <code>&quot;json&quot;</code>、 <code>&quot;orc&quot;</code>、 <code>&quot;avro&quot;</code></td><td>如果不指定方法的格式，则使用 <code>spark.sql.sources.default</code> 所指定的默认格式</td></tr><tr><td><code>option()</code></td><td><code>(&quot;mode&quot;, append [overwrite ignore error])</code>、 <code>(&quot;path&quot;, &quot;path_to_write_to&quot;)</code></td><td>一系列键值对，<code>Spark</code> 文档中解释了不同模式下的对应行为</td></tr><tr><td><code>bucketBy()</code></td><td><code>(numBuckets, col, ..., coln)</code></td><td>按桶写入时，指定桶数量和分桶所依据字段的名字列表</td></tr><tr><td><code>save()</code></td><td><code>&quot;/path/source&quot;</code></td><td>写入的路径</td></tr><tr><td><code>saveAsTable()</code></td><td><code>&quot;table_name&quot;</code></td><td>写入的表名</td></tr></tbody></table><h5 id="文件类型"><a href="#文件类型" class="headerlink" title="文件类型"></a>文件类型</h5><ol><li><p><strong><code>Parquet</code></strong><br><code>Spark</code> 的默认数据源，很多大数据框架和平台都支持，它是一种开源的列式存储文件格式，提供多种 <code>I/O</code> 优化措施（比如压缩，以节省存储空间，支持快速访问数据列）。</p></li><li><p><code>JSON</code><br><code>JSON</code> 的全称为 <code>JavaScript Object Notation</code>，它 是一种常见的数据格式。<code>JSON</code> 有两种表示格式：单行模式和多行模式。</p></li><li><p><code>CSV</code><br><code>CSV</code> 格式应用非常官方，是一种将所有数据字段用逗号隔开的文本文件格式。在这些用逗号隔开的字段中，每行表示一条记录。（这里的逗号分隔符号是可以被修改的）</p></li><li><p><strong><code>Avro</code></strong><br><code>Avro</code> 格式有很多优点，包括直接映射 <code>JSON</code>、快速且高效、支持多种编程语言。</p></li><li><p><strong><code>ORC</code></strong><br>作为另一种优化后的列式存储文件格式，<code>Spark</code> 支持 <code>ORC</code> 的向量化读。向量化读通常会成块（）读入数据，而不俗一次读一行，同时操作会串起来，降低扫描、过滤、聚合、连接等集中操作时的 <code>CPU</code> 使用率。</p></li></ol><h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><p>复杂数据类型由简单数据类型组合而成，而实际则是经常直接操作复杂数据类型，操作复杂数据类型的方式有以下两种:</p><ul><li>将嵌套的结构打散到多行，调用某个函数，然后重建嵌套结构。</li><li>构建用户自定义函数。</li></ul><p>这两种方式都有助于以表格格式处理问题，一般会涉及到 <code>get_json_object()</code>、<code>from_json()</code>、<code>to_json()</code>、<code>explode()</code> 和 <code>selectExpr()</code> 等工具函数。</p><h5 id="打散再重组"><a href="#打散再重组" class="headerlink" title="打散再重组"></a>打散再重组</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT id, collect_list(value + 1) AS values</span><br><span class="line">FROM (</span><br><span class="line">   SELECT id, EXPLODE(values) AS value</span><br><span class="line">   FROM table</span><br><span class="line">) x</span><br><span class="line">GROUP BY id</span><br></pre></td></tr></table></figure><p>上述的嵌套的 <code>SQL</code> 语句中，先执行 <code>EXPLODE(values)</code>，会为每一个 <code>value</code> 创建新的一行（包括 <code>id</code> 字段）。<code>collect_list()</code> 返回的是未去重的对象列表，由于 <code>GROUP BY</code> 语句会触发数据混洗操作，因此重新组合的数组顺序和原数组不一定相同。 </p><h5 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h5><p>要想自行上述等价的任务，也可以创建 <code>UDF</code>，用 <code>map()</code> 迭代各个元素并执行额外的操作。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val = plusOneInt = (values: Array[Int] =&gt; &#123;</span><br><span class="line">   values.map(value =&gt; value + 1)</span><br><span class="line">&#125;)</span><br><span class="line">spark.udf.register(&quot;plusOneInt&quot;, plusOneInt)</span><br></pre></td></tr></table></figure><p>然后可以在 <code>Spark SQL</code> 中使用这个 <code>UDF</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;SELECT id, plusOneInt(values) AS values FROM table&quot;).show()</span><br></pre></td></tr></table></figure><p>由于没有顺序问题，这种方法比使用 <code>explode()</code> 和 <code>collect_list()</code> 好一些，但序列化和反序列化过程本身开销很大。</p><h5 id="复杂类型的内建函数"><a href="#复杂类型的内建函数" class="headerlink" title="复杂类型的内建函数"></a>复杂类型的内建函数</h5><p><code>Spark</code> 专门为复杂数据类型准备的内建函数，完整列表可以参考官方文档。</p><h5 id="高阶函数-1"><a href="#高阶函数-1" class="headerlink" title="高阶函数"></a>高阶函数</h5><p>除了上述的内建函数外，还有部分高阶函数接受匿名 <code>lambda</code> 函数作为参数，示例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># transform 函数接受一个数组和匿名函数作为输入，通过对数组的每个元素应用匿名函数，该函数将结果赋值到输出数组，透明地创建出一个新数组。</span><br><span class="line">transform(<span class="keyword">values</span>, <span class="keyword">values</span> <span class="operator">-</span><span class="operator">&gt;</span> lambda expression)</span><br></pre></td></tr></table></figure><ol><li><p><code>transform()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform(array&lt;T&gt;, function&lt;T, U&gt;): array&lt;U&gt;</span><br></pre></td></tr></table></figure><p>通过对输入数组的每个元素使用一个函数，<code>transform()</code> 函数会生成新的数组。 </p></li><li><p><code>filter()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter(array&lt;T&gt;, function&lt;T, Boolean&gt;): array&lt;T&gt;</span><br></pre></td></tr></table></figure><p><code>filter()</code> 函数输出的数组仅包含输入数组中让布尔表达式结果为 <code>true</code> 的元素。</p></li><li><p><code>exists()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exists(array&lt;T&gt;, function&lt;T, V, Boolean&gt;): Boolean</span><br></pre></td></tr></table></figure><p>当输入数组中有任意一元素满足布尔函数时，<code>exists()</code> 函数返回 <code>true</code>。</p></li><li><p><code>reduce()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(array&lt;T&gt;, B, function&lt;B, T, B&gt;, function&lt;B, R&gt;)</span><br></pre></td></tr></table></figure><p>通过函数 <code>function&lt;B, T, B&gt;</code>，<code>reduce()</code> 函数可以将数组的元素合并到缓冲区 <code>B</code>，最后对最终缓冲区使用最终函数 <code>function&lt;B, R&gt;</code>，并将数组归约为单个值。</p></li></ol><h5 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h5><ol><li><p>联合<br>将具有相同表结构的 <code>DataFrame</code> 联合起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 联合两种表</span></span><br><span class="line">bar = deplays.union(foo)</span><br><span class="line">bar.createOrReplaceTempView(<span class="string">&quot;bar&quot;</span>)</span><br><span class="line"><span class="comment"># 展示联合结果</span></span><br><span class="line">bar.filtyer(expr(<span class="string">&quot;origin == &#x27;SEA&#x27; AND destination == &#x27;SFO&#x27; AND date LIKE &#x27;0010%&#x27; AND deplay &gt; 0&quot;</span>)).show()</span><br></pre></td></tr></table></figure></li><li><p>连接<br>连接两个 <code>DataFrame</code> 是常用操作之一。<br>默认情况下连接为 <code>inner join</code>，可选的种类包含 <code>inner</code>、 <code>cross</code>、 <code>outer</code>、 <code>full</code>、 <code>full_outer</code>、 <code>left</code>、 <code>left_outer</code>、 <code>right</code>、 <code>right_outer</code>、 <code>left_semi</code> 和 <code>left_anti</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foo.join(ports, ports.IATA == foo.origin).select(<span class="string">&quot;City&quot;</span>, <span class="string">&quot;date&quot;</span>, <span class="string">&quot;deplay&quot;</span>, <span class="string">&quot;distance&quot;</span>).show()</span><br></pre></td></tr></table></figure></li><li><p>窗口<br>窗口函数使用窗口（一个范围内的输入行）中各行的值计算出一组值来返回，返回的一般是新的一行。通过使用窗口函数可以在每次操作一组的同时，返回的行数仍然和输入行数一一对应。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> origin, destiation, TotalDelays, rank</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">SELECT</span> origin, destiation, TotalDelays, <span class="built_in">dense_rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> origin <span class="keyword">ORDER</span> <span class="keyword">BY</span> TotalDelays <span class="keyword">DESC</span>) <span class="keyword">as</span> rank) t</span><br><span class="line"><span class="keyword">WHERE</span> rank <span class="operator">&lt;=</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></li><li><p>修改<br>对 <code>DataFrame</code> 进行修改，<code>DataFrame</code> 本身不允许被修改，不过可以通过新建 <code>DataFrame</code> 的方式来实现修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加新列</span></span><br><span class="line">foo2 = (foo.withColumn(<span class="string">&quot;status&quot;</span>, expr(<span class="string">&quot;CASE WHEN delay &lt;= 10 THEN &#x27;On-time&#x27; ELSE &#x27;Delayed&#x27; END&quot;</span>)))</span><br><span class="line">foo2.show()</span><br><span class="line"><span class="comment"># 删除列</span></span><br><span class="line">foo3 = foo2.drop(<span class="string">&quot;delay&quot;</span>)</span><br><span class="line">foo3.show()</span><br><span class="line"><span class="comment"># 修改列名</span></span><br><span class="line">foo4 = foo3.withColumnRenamed(<span class="string">&quot;status&quot;</span>, <span class="string">&quot;flight_status&quot;</span>)</span><br><span class="line">foo4.show()</span><br><span class="line"><span class="comment"># 转置（将行与列数据互换）</span></span><br><span class="line">SELECT * FROM (</span><br><span class="line">   SELECT destination, CAST(SUBSTRING(date, <span class="number">0</span>, <span class="number">2</span>) AS <span class="built_in">int</span>) AS month, delay FROM departureDelays WHERE origin = <span class="string">&#x27;SEA&#x27;</span></span><br><span class="line">)</span><br><span class="line">PIVOT (</span><br><span class="line">   CAST(AVG(delay) AS DECIMAL(<span class="number">4</span>, <span class="number">2</span>)) AS AvgDelay, MAX(delay) AS MaxDelay FOR month IN (<span class="number">1</span> JAN, <span class="number">2</span> FEB)</span><br><span class="line">)</span><br><span class="line">ORDER BY destination</span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="数据结构化-Dataset"><a href="#数据结构化-Dataset" class="headerlink" title="数据结构化 Dataset"></a>数据结构化 <code>Dataset</code></h3><p>前面看过了 <code>DataFrame</code>，那么你基本上就理解了 <code>Dataset</code>，不过还是有那么一些差别，<code>Dataset</code> 主要区分两种特性： <strong>有类型</strong>和<strong>无类型</strong> 。</p><p><img src="https://s2.loli.net/2023/04/10/897iHPxjcUebNE5.jpg" alt="hadoop_spark_4.jpg"></p><p>从概念上看，可以将 <code>DataFrame</code> 看作是 <code>Dataset[Row]</code> 这种由普通对象组成的集合的一个别称，其中 <code>Row</code> 是普通的无类型对象，可以包含不同数据类型的字段。而 <code>Dataset</code> 则与之相反，是由同一类型的对象所组成的集合。正如官方文档中描述的那样：</p><blockquote><p>一种由领域专用对象组成的强类型集合，可以使用函数式或关系型的操作将其并行转化。</p></blockquote><table><thead><tr><th>语言</th><th>有类型和无类型的主要抽象结构</th><th>有类型或无类型</th></tr></thead><tbody><tr><td><code>Scala</code></td><td><code>Dataset[T]</code> 和 <code>DataFrame</code>（<code>Dataset[Row]</code> 的别命）</td><td>都有</td></tr><tr><td><code>Java</code></td><td><code>Dataset&lt;T&gt;</code></td><td>有类型</td></tr><tr><td><code>Python</code></td><td><code>DataFrame</code></td><td>普通 <code>Row</code> 对象，无类型</td></tr><tr><td><code>R</code></td><td><code>DataFrame</code></td><td>普通 <code>Row</code> 对象，无类型</td></tr></tbody></table><h4 id="转化数据"><a href="#转化数据" class="headerlink" title="转化数据"></a>转化数据</h4><p><code>Dataset</code> 是强类型的对象集合，这些对象可以使用函数式或关系型的算子并行转化。可用的转化操作包括 <code>map()</code>、<code>reduce()</code>、<code>filter()</code>、<code>select()</code> 和 <code>aggregate()</code>，这些方法都属于高阶函数，他们接受 <code>lambda</code> 表达式、闭包或函数作为参数，然后返回结果，因此这些操作非常适合函数式编程。</p><p>不过上述 <code>Dataset</code> 是有不足之处的，在使用高阶函数时，会产生从 <code>Spark</code> 内部的 <code>Tungsten</code> 格式反序列化为 <code>JVM</code> 对象的开销，那么避免多余的序列化和反序列化的策略有以下两种：</p><ul><li>在查询中使用 <code>DSL</code> 表达式，避免过多地使用 <code>lambda</code> 表达式的匿名函数作为高阶函数的参数。</li><li>将查询串联起来，以尽量减少序列化和反序列化。</li></ul><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p><strong>编码器将堆外内存中的数据从 <code>Tungsten</code> 格式转为 <code>JVM</code> 对象，即编码器承担着在 <code>Spark</code> 内部格式和 <code>JVM</code> 对象之间序列化和反序列化 <code>Dataset</code> 对象</strong> 。<br><code>Spark</code> 支持自动生成原生类型、<code>Scala</code> 样例类和 <code>JavaBean</code> 的编码器。比起 <code>Java</code> 和 <code>Kryo</code> 的序列化和反序列化，<code>Spark</code> 的编码器要快很多。</p><p><code>Spark</code> 不为 <code>Dataset</code> 或 <code>DataFrame</code> 创建基于 <code>JVM</code> 的对象，而会分配 <code>Java</code> 堆外内存来存储数据，并使用编码器将内存表示的数据转为 <code>JVM</code> 对象。<br>当数据以紧凑的方式存储并通过指针和偏移量访问时，编码器可以快速序列化和反序列化数据。</p><p><img src="https://s2.loli.net/2023/04/10/S5PKNrgB8GFZHRd.jpg" alt="hadoop_spark_5.jpg"></p><p>相比 <code>JVM</code> 自建的序列化和反序列化，<code>Dataset</code> 编码器的优点如下：</p><ul><li><code>Spark</code> 内部的 <code>Tungsten</code> 二进制格式将对象存储在 <code>Java</code> 的堆内存之外，存储的方式很紧凑，因此对象占用的空间更小。</li><li>通过使用指针和计算出的内存地址与偏移量来访问内存，编码器可以实现快速序列化。</li><li>在接收端，编码器能快速地将二进制格式反序列化为 <code>Spark</code> 内部的表示形式。编码器不受 <code>JVM</code> 垃圾回收暂停的影响。</li></ul><p><img src="https://s2.loli.net/2023/04/10/a6jyR9tQhSOGAEz.jpg" alt="hadoop_spark_6.jpg"></p><hr><h3 id="Spark-引擎"><a href="#Spark-引擎" class="headerlink" title="Spark 引擎"></a><code>Spark</code> 引擎</h3><p>在编程层面上，<code>Spark SQL</code> 允许开发人员对带有表结构的结构化数据发起兼容 <code>ANSI SQL:2003</code> 标准的查询。至此 <code>Spark SQL</code>  已经演变成一个非常重要的引擎，许多高层的结构化功能都是基于它构建出来的，除了可以对数据发起类似 <code>SQL</code> 的查询，<code>Spark SQL</code> 引擎还支持下列功能：</p><ul><li>统一 <code>Spark</code> 的各个组件，允许在 <code>Java</code>、<code>Scala</code>、<code>Python</code>、<code>R</code> 程序中将结构化数据集抽象为 <code>DataFrame</code> 或 <code>Dataset</code>，从而简化编程工作。</li><li>连接 <code>Apache Hive</code> 的元数据库和表。</li><li>从结构化的文件格式（<code>JSON</code>、<code>CSV</code>、<code>Text</code>、<code>Avro</code>、<code>Parquet</code>、<code>ORC</code> 等）使用给定的表结构读取结构化数据，并将数据转换为临时表。</li><li>为快速的数据探索提供交互式 <code>Spark SQL shell</code>。</li><li>通过标准的 <code>JDBS/ODBC</code> 连接器，提供与外部工具互相连接的纽带。</li><li>生成优化后的查询计划和紧凑的 <code>JVM</code> 二进制代码，用于最终执行。</li></ul><p><img src="https://s2.loli.net/2023/04/10/yilF4BnUgvVhQTC.jpg" alt="hadoop_spark_7.jpg"></p><p><code>Spark SQL</code> 引擎的核心是 <strong><code>Catalyst</code> 优化器</strong>和 <strong><code>Tungsten</code> 项目</strong> ，其两者共同支撑高层的 <code>DataFrame API</code> 和 <code>Dataset API</code>，以及 <code>SQL</code> 查询。</p><h4 id="Catalyst-优化器"><a href="#Catalyst-优化器" class="headerlink" title="Catalyst 优化器"></a><code>Catalyst</code> 优化器</h4><p><code>Catalyst</code> 优化器接受计算查询作为参数，并将查询转化为执行计划。共分为四个转换阶段：</p><ul><li><strong>解析</strong><br><code>Spark SQL</code> 引擎首先会为 <code>SQL</code> 或 <code>DataFrame</code> 查询生成相应的抽象语法树（<code>abstract synrax tree, AST</code>）。所有的列名和表名都会通过查询内部元数据而解析出来，全部解析完成后，会进入下一阶段。</li><li><strong>逻辑优化</strong><br>这个阶段在内部共分为两步。通过应用基于规则的优化策略，<code>Catalyst</code> 优化器会首先构建出多个计划，然后使用基于代价的优化器（<code>cost-based optimizer, CBO</code>）为每个计划计算出执行开销。这些计划以算子树的形式呈现，其优化过程包括常量折叠、谓词下推、列裁剪、布尔表达式简化等，最终获得的逻辑计划作为下一阶段的输入，用于生成物理计划。</li><li><strong>生成物理计划</strong>hexo<br><code>Spark SQL</code> 会为选中的逻辑计划生成最佳的物理计划，这个物理计划由 <code>Spark</code> 执行引擎可用的物理算子组成。</li><li><strong>生成二进制代码</strong><br>在查询优化的最终阶段，<code>Spark</code> 会最终生成高效的 <code>Java</code> 字节码，用于在各个机器上执行。而在这个过程中用到了 <code>Tungsten</code> 项目，实现了执行计划的全局代码生成。<br>那什么是全局代码生成呢？他是物理计划的一个优化阶段，将整个查询合并为一个函数，避免虚函数调用，利用 <code>CPU</code> 寄存器存放中间数据，而这种高效策略可以显著提升 <code>CPU</code> 效率和性能。</li></ul><p><img src="https://s2.loli.net/2023/04/10/uevImbGLT7N9rJK.jpg" alt="hadoop_spark_8.jpg"></p><p>通过上面的图示，可以发现只要执行过程相同，最终会生成相似的查询计划和一样的字节码用于执行，也就是说，无论使用什么编程语言，查询都会经过同样的过程，所生成的字节码很有可能都是一样的。</p><p>在经过最初的解析阶段之后，查询计划会被 <code>Catalyst</code> 优化器转化和重排。</p><p><img src="https://s2.loli.net/2023/04/10/cUXY3alpo6qZmrn.jpg" alt="hadoop_spark_9.jpg"></p><h4 id="Tungsten-项目"><a href="#Tungsten-项目" class="headerlink" title="Tungsten 项目"></a><code>Tungsten</code> 项目</h4><p><code>Tungsten</code> 项目致力于提升 <code>Spark</code> 应用对内存和 <code>CPU</code> 的利用率，使性能达到硬件的极限，主要包含以下内容：</p><ul><li><code>Memory Management and Binary Processing</code>: <code>off-heap</code> 管理内存，降低对象的开销和消除 <code>JVM GC</code> 带来的延时。</li><li><code>Cache-aware computation</code>: 优化存储，提升 <code>CPU L1/L2/L3</code> 缓存命中率。</li><li><code>Code generation</code>: 优化 <code>Spark SQL</code> 的代码生成部分，提升 <code>CPU</code> 利用率。</li></ul><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;引入&quot;&gt;&lt;a href=&quot;#引入&quot; class=&quot;headerlink&quot; title=&quot;引入&quot;&gt;&lt;/a&gt;引入&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Spark&lt;/code&gt; 是用于&lt;strong&gt;处理大数据的集群计算框架&lt;/strong&gt; ，与其他大多数数据处理框架不同之处在于 &lt;code&gt;Spark&lt;/code&gt; 没有以 &lt;code&gt;MapReduce&lt;/code&gt; 作为执行引擎，而是使用它自己的&lt;strong&gt;分布式运行环境&lt;/strong&gt;在集群上执行工作。另外 &lt;code&gt;Spark&lt;/code&gt; 与 &lt;code&gt;Hadoop&lt;/code&gt; 又紧密集成，&lt;code&gt;Spark&lt;/code&gt; 可以在 &lt;code&gt;YARN&lt;/code&gt; 上运行，并支持 &lt;code&gt;Hadoop&lt;/code&gt; 文件格式及其存储后端（例如 &lt;code&gt;HDFS&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Spark&lt;/code&gt; 最突出的表现在于其能将 &lt;strong&gt;作业与作业之间的大规模的工作数据集存储在内存中&lt;/strong&gt;。这种能力使得在性能上远超 &lt;code&gt;MapReduce&lt;/code&gt; 好几个数量级，原因就在于 &lt;code&gt;MapReduce&lt;/code&gt; 数据都是从磁盘上加载。根据 &lt;code&gt;Spark&lt;/code&gt; 的处理模型有两类应用获益最大，分别是 &lt;strong&gt;迭代算法（即对一个数据集重复应用某个函数，直至满足退出条件）&lt;/strong&gt;和 &lt;strong&gt;交互式分析（用户向数据集发出一系列专用的探索性查询）&lt;/strong&gt; 。&lt;br&gt;另外 &lt;code&gt;Spark&lt;/code&gt; 还因为其具有的 &lt;strong&gt;&lt;code&gt;DAG&lt;/code&gt; 引擎&lt;/strong&gt;更具吸引力，原因在于 &lt;code&gt;DAG&lt;/code&gt; 引擎可以处理任意操作流水线，并为用户将其转化为单个任务。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-mysqldump warning GTID</title>
    <link href="https://blog.vgbhfive.cn/MySQL-mysqldump-warning-GTID/"/>
    <id>https://blog.vgbhfive.cn/MySQL-mysqldump-warning-GTID/</id>
    <published>2023-02-23T15:44:40.000Z</published>
    <updated>2023-02-23T16:09:54.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>工作时需要拉一下测试环境的数据到开发环境，所以就是 <code>mysqldump</code> 老哥出场了…</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># mysqldump -h localhost -u root -p --tables test_table --where=&quot;str=&#x27;str1&#x27;&quot; &gt; test_table_data.sql</span><br><span class="line">Enter password:</span><br><span class="line">Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don&#x27;t want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.</span><br></pre></td></tr></table></figure><span id="more"></span><p>上面的警告内容翻译如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">默认情况下，来自具有 GTID 的服务器的部分转储将包括所有事务的 GTID，即使是那些更改了数据库被抑制部分的事务。如果不想恢复 gtid，请使用 --set-gtid-purged=OFF 参数。</span><br><span class="line">如果要进行完整的转储，请使用参数 --all-databases --triggers --routines --events</span><br></pre></td></tr></table></figure><p>得知警告原因之后，就会想到 <code>GTID</code> 是用来保证事务全局唯一的配置信息，另外发现数据已经完整转储出来，那么接下来这个 <code>GTID</code> 是什么意思呢？</p><hr><h3 id="GTID"><a href="#GTID" class="headerlink" title="GTID"></a><code>GTID</code></h3><p><code>GTID (Global Transaction IDentifier) </code>是全局事务标识，其具有全局唯一性，一个事务对应一个 <code>GTID</code>。<br>由于其唯一性不仅限于主服务器，<code>GTID</code> 在所有的从服务器上也是唯一的；一个 <code>GTID</code> 在一个服务器上只执行一次，从而避免重复执行导致数据混乱或主从不一致。</p><p>在传统的复制里面，当发生故障需要主从切换时，服务器需要找到 <code>binlog</code> 和 <code>pos</code> 点，然后将其设定为新的主节点开启复制。相对来说比较麻烦也容易出错，因此在 <code>MySQL 5.6</code> 版本里会通过内部机制自动匹配 <code>GTID</code> 断点，不再寻找 <code>binlog</code> 和 <code>pos</code> 点，此时只需要知道主节点的 <code>IP</code>、端口、以及账号密码就可以自动复制。</p><h4 id="产生"><a href="#产生" class="headerlink" title="产生"></a>产生</h4><p><code>GTID</code> 的生成受 <strong><code>GTID_NEXT</code></strong> 控制。</p><p>在主服务器上 <strong><code>GTID_NEXT</code> 默认值是 <code>AUTOMATIC</code></strong> ，即在每次事务提交时自动生成 <code>GTID</code>，并且它会从当前已执行的 <code>GTID</code> 集合（即<code>gtid_executed</code>）中找一个大于 <strong><code>0</code></strong> 的未使用的最小值作为下个事务 <code>GTID</code>。而现实中在实际的更新事务记录之前，会将 <code>GTID</code> 写入到 <code>binlog</code>（<code>set GTID_NEXT</code> 记录）。<br>在从服务器上，首先从 <code>binlog</code> 先读取到主库的 <code>GTID</code>（<code>get GTID_NEXT</code>记录），然后执行事务时使用该 <code>GTID</code>。</p><p>产生步骤：</p><ul><li>主服务器更新数据时，会在事务前产生 <code>GTID</code>，一同记录到 <code>binlog</code> 日志中。</li><li><code>binlog</code> 传送到从服务器后，被写入到本地的 <code>relay log</code> 中，接着从服务器读取 <code>GTID</code>，并将其设定为自己的 <code>GTID</code>。</li><li>从服务器的 <code>SQL</code> 线程从 <code>relay log</code> 中获取 <code>GTID</code>，然后对比从服务器端的 <code>binlog</code> 中是否有记录。</li><li>如果有记录，说明该 <code>GTID</code> 的事务已经执行，从服务器会忽略。</li><li>如果没有记录，从服务器就会从 <code>relay log</code> 中执行该 <code>GTID</code> 的事务，并记录到 <code>binlog</code>。</li></ul><h4 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h4><p><code>GDIT</code> 由两部分组成： <strong><code>source_id</code></strong> 和 <strong><code>transaction_id</code></strong> 。<br><strong><code>source_id</code></strong> 是产生 <code>GTID</code> 的服务器，即 <code>server_uuid</code>，在第一次启动时生成（<code>sql/mysqld.cc: generate_server_uuid()</code>）并保存到 <code>DATADIR/auto.cnf</code> 文件里。<br><strong><code>transaction_id</code></strong> 是序列号（<code>sequence number</code>），在每台 <code>MySQL</code> 服务器上都是从 <code>1</code> 开始自增长的顺序号，是事务的唯一标识。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="http://mysql.taobao.org/monthly/2020/05/09/">http://mysql.taobao.org/monthly/2020/05/09/</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;复现&quot;&gt;&lt;a href=&quot;#复现&quot; class=&quot;headerlink&quot; title=&quot;复现&quot;&gt;&lt;/a&gt;复现&lt;/h3&gt;&lt;p&gt;工作时需要拉一下测试环境的数据到开发环境，所以就是 &lt;code&gt;mysqldump&lt;/code&gt; 老哥出场了…&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# mysqldump -h localhost -u root -p --tables test_table --where=&amp;quot;str=&amp;#x27;str1&amp;#x27;&amp;quot; &amp;gt; test_table_data.sql&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Enter password:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don&amp;#x27;t want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="https://blog.vgbhfive.cn/tags/MySQL/"/>
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-MapReduce</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-MapReduce/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-MapReduce/</id>
    <published>2023-02-14T16:13:22.000Z</published>
    <updated>2023-03-11T15:10:44.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p><code>MapReduce</code> 是一种用于<strong>数据处理的编程模型</strong>，其本质是<strong>并行运行</strong>，因此可以将大规模的数据分析任务分发给任何一个拥有足够多机器的数据中心，当然其优势也是<strong>处理大规模数据集</strong>。</p><p><code>MapReduce</code> 任务过程分为两个处理阶段： <strong><code>map</code> 阶段</strong>和 <strong><code>reduce</code> 阶段</strong>。每个阶段都是以<strong>键值对</strong>作为输入和输出，其类型由开发者决定，当然 <code>map</code> 函数和 <code>reduce</code> 函数也是由开发者实现。</p><span id="more"></span> <h4 id="MapReduce-原理"><a href="#MapReduce-原理" class="headerlink" title="MapReduce 原理"></a><code>MapReduce</code> 原理</h4><p><code>MapReduce</code> <strong>作业（<code>job</code>）</strong>是客户端执行工作的单元，包含：<strong>输入数据</strong>、 <strong><code>MapReduce</code> 程序</strong>和<strong>配置信息</strong>。<code>Hadoop</code> 将作业分成若干个<strong>任务（<code>task</code>）</strong>来执行，其中包含两类任务： <strong><code>map</code> 任务</strong>和 <strong><code>reduce</code> 任务</strong>，这些任务运行在集群的节点上，并通过 <strong><code>YARN</code></strong> 进行调度，其中如果有一个任务失败，则将在另一个不同的节点上重新调度运行。</p><p><code>MapReduce</code> 的输入数据会被划分为等长的小数据块，称为<strong>输入分片（<code>input split</code>）</strong>或者简称<strong>分片</strong>，而每一个分片会构建一个 <code>map</code> 任务，并由该任务来运行用户自定义的 <code>map</code> 函数从而处理分片中的每条记录。<br><small>有许多分片就意味着每个分片处理所需时间少于整个输入数据所需时间；另外如果分片切分得太小，那管理分片得事件和构建 <code>map</code> 任务的事件将组成作业的整体运行事件。</small><br><small>如果 <code>map</code> 任务运行在存储有输入数据（<code>HDFS</code> 中的数据）的节点上，无需使用带宽资源即可获得最佳性能，也就是<strong>数据本地化优化（<code>data locality optimization</code>）</strong> 。</small></p><p><code>map</code> 任务将其输出写入本地磁盘（此输出为<strong>中间结果</strong>），该中间结果由 <code>reduce</code> 任务处理后才会产生最终输出结果，随着作业完成，<code>map</code> 的结果也会被随之删除。</p><p><code>reduce</code> 任务的输入通常来自于所有 <code>map</code> 任务的输出，其并不具备数据本地化的优势，且 <code>reduce</code> 任务的数量是<strong>独立指定</strong>的不由输入数据的大小决定。如果存在多个 <code>reduce</code> 任务，那每个 <code>map</code> 任务就会针对输出进行分区，即为每个 <code>reduce</code> 任务创建一个分区，每个分区都有许多键及其对应的值，但是每个键对应的键值对记录都在同一分区中。<br><small>分区可以由用户定义的分区函数控制，但通常采用默认的 <code>partitioner</code> 通过哈希函数来区分。</small></p><p><code>map</code> 任务和 <code>reduce</code> 任务之间的排序和分组，该部分也被称为 <strong><code>shuffle</code> （混洗）</strong> ，每个 <code>reduce</code> 任务的输入都来自于所有 <code>map</code> 任务的输出，另外 <code>shuffle</code> 一般比图中的更加复杂，而且调整混洗参数对作业总执行时间的影响非常大。<br><small>当数据完全并行时（即无需混洗）可能会出现无 <code>reduce</code> 任务的情况。</small></p><p>总结 <code>map</code> 阶段的输入是 <code>NCDC</code> 原始数据，键是某一行起始位置相对于文件起始位置的偏移量，而值是文本文件的每一行；<code>reduce</code> 阶段的输入则是将 <code>map</code> 阶段的输出结果经由 <code>shuffle</code> 排序和分组之后的数据。</p><h4 id="combiner-函数"><a href="#combiner-函数" class="headerlink" title="combiner 函数"></a><code>combiner</code> 函数</h4><p>集群上可用的带宽限制了 <code>MapReduce</code> 作业的数量，因此尽量避免 <code>map</code> 任务和 <code>reduce</code> 任务之间的数据传输是有利的；为此 <code>Hadoop</code> 允许用户指对 <code>map</code> 任务的输出指定一个 <code>combiner</code>，该 <strong><code>combiner</code> 函数</strong>的输出将作为 <code>reduce</code> 任务的输入。<br><small>由于 <code>combiner</code> 属于优化方案，因此无法确定要对一个指定的 <code>map</code> 任务输出记录调用多少次 <code>combiner</code>。</small></p><hr><h3 id="经典示例"><a href="#经典示例" class="headerlink" title="经典示例"></a>经典示例</h3><p><code>WordCount</code> 单词计数是最简单也是最能体现 <code>MapReduce</code> 思想的示例程序之一，该程序完整的代码可以在 <code>Hadoop</code> 安装包的 <code>src/examples</code> 目录下找到。其主要功能是：统计一系列文本文件中每个单词出现的次数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 继承 Mapper 类，实现 map 功能</span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * key 表示每一行的起始位置（偏移量offset）</span></span><br><span class="line"><span class="comment">   * value 表示每一行的文本内容</span></span><br><span class="line"><span class="comment">   * context.key 表示每一行中的每个单词</span></span><br><span class="line"><span class="comment">   * context.value 表示每一行中的每个单词的出现次数，固定值为1</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// map 功能必须实现的函数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 继承 Reducer 类，实现 reduce 功能</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * key 表示每一行中的每个单词</span></span><br><span class="line"><span class="comment"> * values 表示每一行中的每个单词的出现次数，固定值为1</span></span><br><span class="line"><span class="comment"> * context.key 表示每一行中的每个单词</span></span><br><span class="line"><span class="comment"> * context.value 表示每一行中的每个单词的出现次数之和</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      result.set(sum);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 初始化 Configuration，读取 mapreduce 系统配置信息</span></span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构建 Job 并且加载计算程序 WordCount.class</span></span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定 Mapper、Combiner、Reducer，也就是继承实现的类</span></span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置输入输出数据</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.setInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p><code>Job</code> 初始化<br><code>main</code> 函数构建 <code>Configuration</code> 对象设置系统配置信息，接着 <code>Job</code> 自定义实例并设置启动类。</p></li><li><p>设置 <code>Job</code> 的 <code>map</code>（拆分）、 <code>combiner</code>（中间结果处理）、 <code>reduce</code>（合并）<br>设置 <code>Job</code> 的相关 <code>map</code> 类、<code>reduce</code> 类、<code>combiner</code> 类。</p><ul><li><code>map</code>：<code>Mapper</code> 类共有四个泛型，分别是 <code>KEYIN</code>、<code>VALUEIN</code>、<code>KEYOUT</code>、<code>VALUEOUT</code>，前面两个 <code>KEYIN</code>、<code>VALUEIN</code> 指的是 <code>map</code> 函数输入参数的键值对的类型；后面两个<code>KEYOUT</code>、<code>VALUEOUT</code> 指的是 <code>map</code> 函数输出的键值对的类型。而这里的 <code>map</code> 函数中通过空格符号来分割文本内容，并对其进行记录。</li><li><code>reduce</code>：<code>Reducer</code> 类也有四个泛型，分别指的是 <code>reduce</code> 函数输入的键值对类型（这里输入的键值对类型通常和 <code>map</code> 的输出键值对类型保持一致）和输出的键值对类型。而这里的 <code>reduce</code> 函数主要是将传入的键值对进行最后的合并统计，形成最后的统计结果。</li></ul></li><li><p>设置 <code>Job</code> 的键值对类型<br>设置 <code>Job</code> 输出结果 <code>&lt;key,value&gt;</code> 的中键值对数据类型，因为结果是&lt;单词,个数&gt;，所以 <code>key</code> 设置为 <code>Text</code> 类型相当于 <code>String</code> 类型。<code>Value</code> 设置为 <code>IntWritable</code> 相当于 <code>int</code> 类型。</p></li><li><p>设置 <code>Job</code> 的输入输出<br>通过 <code>setInputPath</code> 和 <code>setOutputPath</code> 设置 <code>Job</code> 的输入输出路径。</p></li></ol><hr><h3 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h3><h4 id="作业运行机制"><a href="#作业运行机制" class="headerlink" title="作业运行机制"></a>作业运行机制</h4><p>在说明作业运行机制之前，可以通过 <code>Job</code> 对象的 <code>submit()</code> 方法来运行 <code>MapReduce</code> 作业，且其内部封装了大量的处理细节；也可以通过调用 <code>waitForComplete()</code> 方法用来提交之前没有提交过的任务，并等待它完成。</p><p>整体机制流程如下：</p><ul><li>客户端提交 <code>MapReduce</code> 作业。</li><li><code>YARN</code> 资源管理器负责协调集群上计算机资源的分配。</li><li><code>YARN</code> 节点管理器负责启动和监视集群中机器上的计算容器（<code>container</code>）。</li><li><code>MapReduce</code> 的 <code>application master</code> 负责协调运行 <code>MapReduce</code> 作业的任务。它和 <code>MapReduce</code> 任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理。</li><li>分布式文件系统用来与其他实体间共享作业文件。</li></ul><p><img src="https://s2.loli.net/2023/03/11/FWK3eMbnlcvLNHG.png" alt="hadoop-2-1.jpg"></p><h5 id="提交作业"><a href="#提交作业" class="headerlink" title="提交作业"></a>提交作业</h5><p><code>Job</code> 的 <code>submit()</code> 方法创建一个内部的 <strong><code>JobSummiter</code></strong> 实例，并且调用其 <code>submitJoobbInternal()</code> 方法。提交作业后，<code>waitForComplete()</code> 每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功就显示作业计数器；如果失败则导致作业失败的错误被记录到控制台。<br><code>JobSummiter</code> 所实现的作业提交过程如下：</p><ul><li>向资源管理器请求一个新应用 <code>ID</code>，用于 <code>MapReduce</code> 作业 <code>ID</code>。</li><li>检查作业的输出说明。</li><li>计算作业的输入分片。</li><li>将运行作业所需要的资源（包括作业 <code>JAR</code> 文件、配置文件和计算所得的输入分片）复制到一个以作业 <code>ID</code> 命名的目录下的共享文件系统中。作业 <code>JAR</code> 的副本较多（由 <code>mapreduce.client.submit.file.replication</code> 属性控制，默认值为 <code>10</code>），因此在运行作业的任务时，集群中有很多副本可供节点管理器访问。</li><li>通过调用资源管理器的 <code>submitApplication()</code> 方法提交作业。</li></ul><h5 id="作业初始化"><a href="#作业初始化" class="headerlink" title="作业初始化"></a>作业初始化</h5><p>资源管理器收到调用它的 <code>submitApplication()</code> 消息后，便将请求传递给 <strong><code>YARN</code> 调度器（<code>scheduler</code>）</strong> 。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动 <code>application master</code> 的进程。</p><p><code>MapReduce</code> 作业的 <code>application master</code> 是一个 <code>Java</code> 应用程序，它的主类是 <strong><code>MRAppMaster</code></strong> 。由于将接受来自任务的进度和完成报告，因此 <code>application master</code> 对作业的初始化是通过创建多个<strong>薄记对象</strong>以保持对作业进度的跟踪来完成的。接下来它接受来自共享文件系统、在客户端计算的输入分片，接着对每一个分片创建一个 <code>map</code> 任务对象以及由 <code>mapreduce.job.reduces</code> 属性（通过作业的 <code>setNumReduceTasks()</code> 方法设置）确定的多个 <code>reduce</code> 任务对象。任务 <code>ID</code> 也会在此时分配。</p><p><code>application master</code> 必须决定如何运行构成 <code>MapReduce</code> 作业的各个任务。如果作业很小，并且 <code>application master</code> 判断在新的容器中分配和运行任务的开销大于并行运行的开销时，就选择和自己在同一个 <code>JVM</code> 上运行任务。这样的作业被称为 <strong><code>uberized</code>，或者作为 <code>uber</code> 任务运行</strong> 。</p><p>如何判断作何很小呢？默认情况下，小作业就是少于 <code>10</code> 个 <code>mapper</code> 且只有 <code>1</code> 个 <code>reduce</code> 且输入大小小于一个 <code>HDFS</code> 块的作业（设置 <code>mapreduce.job.ubertask.maxmaps</code>、<code>mapreduce.job.ubertask.maxreduces</code> 和 <code>mapreduce.job.ubertask.maxbytes</code> 参数）。必须明确启用 <code>Uber</code> 任务（对于单个作业或者整个集群），具体方法是将 <code>mapreduce.job.ubertask.enable</code> 设置为 <code>true</code>。</p><p>最后在运行任务前，<code>application master</code> 调用 <code>setupJob()</code> 方法设置 <code>OutputCommitter</code>。默认值为 <code>FileOutputCommiter</code>，表示将建立作业的最终输出目录及任务输出的临时工作空间。</p><h5 id="分配任务"><a href="#分配任务" class="headerlink" title="分配任务"></a>分配任务</h5><p>如果作业不适合作为 <code>uber</code> 任务运行，那么 <code>application master</code> 就会为该作业中的所有 <code>map</code> 任务和 <code>reduce</code> 任务向资源管理器请求容器。首先为 <code>map</code> 任务发出请求，该请求优先级要高于 <code>reduce</code> 任务的请求，这是因为所有的 <code>map</code> 任务必须在 <code>reduce</code> 的排序阶段能够启动前完成，直到有 <code>5%</code> 的 <code>map</code> 任务已经完成时为 <code>reduce</code> 任务的请求才会发出。</p><p><code>reduce</code> 任务能够在集群中任意位置运行，但是 <code>map</code> 任务的请求有着数据本地化局限，在理想的情况下，任务是数据本地化（<code>data local</code>）的，意味着任务在分片驻留的同一节点上运行。可选的情况下，任务可能时机架本地化（<code>rack local</code>）的，即和分片在同一机架而非同一节点上运行。同时也有一些任务既不是数据本地化，也不是机架本地化，它们会从别的机架上获取自己的数据。</p><p>请求也为任务指定了内存需求和 <code>CPU</code> 数。在默认情况下，每个 <code>map</code> 任务和 <code>reduce</code> 任务都分配到 <code>1024MB</code> 的内存和一个虚拟内核，这些值可以在每个作业的基础上进行配置，分别通过 <code>4</code> 个属性来设置 <code>mapreduce.map.memory.mb</code>、<code>mapreduce.map.cpu.vcores</code>、<code>mapreduce.reduce.memory.mb</code> 和 <code>mapreduce.reduce.cpu.vcoresp.memory.mb</code>。</p><h5 id="执行任务"><a href="#执行任务" class="headerlink" title="执行任务"></a>执行任务</h5><p>一旦资源管理器的调度器为任务分配了一个特定节点上的容器，<code>application master</code> 就通过与节点管理器通信来启动容器。该任务由主类为 <code>YarnChild</code> 的一个 <code>Java</code> 应用程序执行，在它运行任务之前，需要首先将任务需要的资源本地化，包括作业的配置、<code>JAR</code> 文件和所有来自分布式缓存的文件。最后运行 <code>map</code> 任务或 <code>reduce</code> 任务。</p><p><code>YarnChild</code> 在指定的 <code>JVM</code> 中运行，因此用户定义的 <code>map</code> 或 <code>reduce</code> 函数中的任何缺陷不会影响到节点管理器。</p><p>每个任务都能够执行搭建（<code>setup</code>）和提交（<code>commit</code>）动作，它们和任务本身在同一个 <code>JVM</code> 中运行，并由作业的 <code>OutputCommitter</code> 确定。对于基于文件的作业，提交动作将任务输出由临时位置迁移到最终位置。提交协议确保当推测执行（<code>speculative execution</code>）被启用时，只有一个任务副本被提交，其他的都被取消。</p><h5 id="更新作业状态和进度"><a href="#更新作业状态和进度" class="headerlink" title="更新作业状态和进度"></a>更新作业状态和进度</h5><p><code>MapReduce</code> 作业是长时间运行的批量作业，运行时间范围从数秒到数小时。一个作业和它的每个任务都有一个状态（<code>status</code>），包括：作业或任务的状态、<code>map</code> 和 <code>reduce</code> 的进度、作业计数器的值、状态消息或描述（可以由用户来设置）。<br>上述的状态信息在作业期间不断改变，那又是如何与客户端通信呢？任务在运行时，对其进度（<code>progress</code>，即任务完成百分比）保持跟踪。对 <code>map</code> 任务，任务进度是已处理输入所占比例。对 <code>reduce</code> 任务，情况有点复杂，整个过程分为三个步骤，与 <code>shuffle</code> 的三个阶段相对应，同时也会估计已处理 <code>reduce</code> 输入的比例。</p><p><code>MapReduce</code> 中进度的组成如下：</p><ul><li>读入一条输入记录</li><li>写入一条输出记录</li><li>设置状态描述</li><li>增加计数器的值</li><li>调用 <code>Reporter</code> 或 <code>TaskAttemptContext</code> 的 <code>progress()</code> 方法</li></ul><p>任务也有一组计数器，负责对任务运行过程中各个事件进行计数，这些计数器要么置于框架中，要么由用户自己定义。</p><p>当 <code>map</code> 任务或 <code>reduce</code> 任务运行时，子进程和自己的父 <code>application master</code> 通过 <code>umbilical</code> 接口通信。每个三秒钟，任务通过这个 <code>umbilical</code> 接口向自己的 <code>application master</code> 报告进度和状态（包括计数器），<code>application master</code> 会形成一个作业的汇聚视图（<code>aggregate view</code>）。</p><p>在作业期间，客户端每秒钟轮询一次 <code>application master</code> 以接收最新状态（轮询间隔通过 <code>mapreduce.client.progressmonitor.pollinterval</code> 参数设置）。客户端也可以使用 <code>Job</code> 的 <code>getStatus()</code> 方法得到一个 <code>JobStatus</code> 对象的实例，后者会包含作业的所有状态信息。</p><p><img src="https://s2.loli.net/2023/03/11/usIrqE6z2K8biB9.png" alt="hadoop-2-2.jpg"></p><h5 id="作业完成"><a href="#作业完成" class="headerlink" title="作业完成"></a>作业完成</h5><p>当 <code>application master</code> 收到作业最后一个任务已完成的通知后，便将作业的状态设置为“成功”。在 <code>Job</code> 轮询状态时，便知道任务已成功完成，于是 <code>Job</code> 打印一条消息告知用户，然后从 <code>waitForComplete()</code> 方法返回；<code>Job</code> 的统计信息和计数值在这个时候也会输出到控制台。</p><p>如果 <code>application master</code> 有相应的设置，也会发送一条 <code>HTTP</code> 作业通知，可以通过 <code>mapreduce.job.end-notification.url</code> 属性来设置在收到回调指令后通知客户端。<br>最后在作业完成时，<code>application master</code> 和任务容器清理其工作状态（中间状态会被删除），接着 <code>OutputCommitter</code> 的 <code>commitJob()</code> 方法会被调用。作业信息由作业历史服务器存档，以便日后用户需要时可以查询。</p><h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a><code>shuffle</code></h4><p><code>MapReduce</code> 确保每个 <code>reduce</code> 的输入都是按键排序的，系统执行排序将 <code>map</code> 输出作为输入传给 <code>reducer</code> 的过程称为 <code>shuffle</code>。<code>shuffle</code> 属于不断被优化和改进的部分，也是 <code>MapReduce</code> 的“心脏”，是奇迹发生的地方。</p><h5 id="map"><a href="#map" class="headerlink" title="map"></a><code>map</code></h5><p><code>map</code> 函数开始产生输出时，并不是简单地将数据写入磁盘，而是利用缓冲的方式写到内存并处于效率的考虑进行预排序。</p><p><img src="https://s2.loli.net/2023/03/11/gbXM2sNYIwR7aDu.png" alt="hadoop-2-3.jpg"></p><p>每个 <code>map</code> 任务都有一个环形内存缓冲区用于存储任务输出，一旦缓冲内容达到阈值（默认为 <code>80%</code>），此时一个后台线程便开始把内容溢出到磁盘。在溢出写到磁盘的过程中，<code>map</code> 输出继续写到缓冲区，但如果在此期间缓冲区被填满，<code>map</code> 会被阻塞直到写磁盘过程完成。溢出写过程按轮询方式将缓冲区内容写到 <code>mapreduce.cluster.local.dir</code> 属性在作业特定子目录下指定的目录中。<br><small>默认情况下，缓冲区大小为 <code>100MB</code>，此值可以通过 <code>mapreduce.task.io.sort.mb</code> 属性调整</small></p><p>如果 <code>map</code> 输出相当小，则输出会被直接复制到 <code>reduce</code> 任务的 <code>JVM</code> 内存中。指定用于此用途的堆空间的百分比大小可以由 <code>mapreduce.reduce.shuffle.input.buffer.percent</code> 属性控制，。</p><p>在写磁盘之前后台线程会根据数据最终要传递给的 <code>reducer</code> 将数据划分为相应的分区。在每个分区中后台线程会根据键进行排序，如果此时有一个 <code>combiner</code> 函数，在排序后运行此函数，此后会减少写到磁盘上的数据和传递给 <code>reducer</code> 的数据。</p><p>每次内存缓冲区达到溢出阈值，就会创建一个溢出文件（<code>spill file</code>），因此在 <code>map</code> 任务写完其最后一个输出记录之后就会有几个溢出文件。在任务完成之前，溢出文件被合并成一个已分区且已排序的输出文件。<br><small>属性 <code>mapreduce.task.io.sort.factor</code> 可以控制一次最多合并多少溢出文件，默认值为 <code>10</code>。如果最少存在 <code>3</code> 个溢出文件时，则 <code>combiner</code> 函数会在输出文件写到磁盘之前再次运行</small></p><p>在将压缩 <code>map</code> 输出写到磁盘的过程中是否能使用压缩呢？在默认情况下，输出是不压缩的，但可以通过属性 <code>mapreduce.map.output.compress</code> 设置为 <code>true</code>，即可启用此功能。而使用的压缩库则由 <code>mapreduce.map.output.compress.codec</code> 参数指定。</p><p><code>reducer</code> 通过 <code>HTTP</code> 得到输出文件的分区。而用于文件分区的工作线程的数量由任务的 <code>mapreduce.shuffle.max.threads</code> 属性控制，此设置针对的是每一个节点管理器，而不是每个 <code>map</code> 任务。</p><h5 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a><code>reduce</code></h5><p><code>map</code> 输出文件位于运行 <code>map</code> 任务的 <code>tasktracker</code> 的本地磁盘，之后 <code>tasktracker</code> 会为分区文件运行 <code>reduce</code> 任务。那 <code>reducer</code> 如何知道从那台机器上获取 <code>map</code> 输出？<code>map</code> 任务成功后，会通过心跳机制通知他们的 <code>application master</code>，<code>reducer</code> 中的一个线程定期询问 <code>master</code> 以便获取 <code>map</code> 输出文件主机的位置，直到获取所有输出位置。</p><p>每个 <code>map</code> 任务的完成不尽相同，因此每个任务完成时，<code>reduce</code> 任务就开始复制其输出（此为 <code>reduce</code> 任务的复制阶段）。<code>reduce</code> 任务有少量复制线程，可以并行获取 <code>map</code> 输出，默认为 <code>5</code> 个线程，此默认值可以通过 <code>mapreduce.reduce.shuffle.parallelcopies</code> 属性修改。</p><p>复制完成所有的 <code>map</code> 输出之后，<code>reduce</code> 任务进行合并阶段，此阶段会合并 <code>map</code> 输出，维持其顺序排序。<br><small>默认值为 <code>10</code>，通过 <code>mapreduce.task.io.sort.factor</code> 属性控制。</small></p><p>在最后阶段，即直接将数据输入 <code>reduce</code> 函数，从而省略一次磁盘的往返行程，并没有将输出文件合并为一个已排序的文件作为最后一次。</p><p>在 <code>reduce</code> 阶段，对已排序输出中的每个键调用 <code>reduce</code> 函数，此阶段的输出直接写到输出文件系统，一般为 <code>HDFS</code>。如果采用 <code>HDFS</code>，由于节点管理器也运行着数据节点，则第一块数据副本将被写到本地磁盘。</p><h5 id="配置调优"><a href="#配置调优" class="headerlink" title="配置调优"></a>配置调优</h5><ol><li><p><code>map</code> 端的调优属性</p><table><thead><tr><th>属性名称</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>mapreduce.task.io.sort.mb</code></td><td><code>int</code></td><td><code>100</code></td><td>排序 <code>map</code> 输出时所使用的内存缓冲区的大小，以 <code>MB</code> 为单位</td></tr><tr><td><code>mapreduce.map.sort.spill.percent</code></td><td><code>float</code></td><td><code>0.80</code></td><td><code>map</code> 输出内存缓存和用来开始磁盘溢出写过程的记录边界索引，使用比例的阈值</td></tr><tr><td><code>mapreduce.task.io.sort.factor</code></td><td><code>int</code></td><td><code>10</code></td><td>排序文件时，一次最多合并的流数。在 <code>reduce</code> 中使用。</td></tr><tr><td><code>mapreduce.map.combine.minspills</code></td><td><code>int</code></td><td><code>3</code></td><td>运行 <code>combiner</code> 所需的最少溢出文件数</td></tr><tr><td><code>mapreduce.map.output.compress</code></td><td><code>Boolean</code></td><td><code>false</code></td><td>是否压缩 <code>map</code> 输出</td></tr><tr><td><code>mapreduce.map.output.compress.codec</code></td><td><code>Class name</code></td><td><code>org.apache. hadoop.io.compress. DefaultCodec</code></td><td>用于 <code>map</code> 输出的压缩编解码器</td></tr><tr><td><code>mapreduce.shuffle.max.threads</code></td><td><code>int</code></td><td><code>0</code></td><td>每个节点管理器的工作线程数，用于将 <code>map</code> 输出到 <code>reducer</code>。 <code>0</code> 表示使用 <code>Netty</code> 默认值，两倍于可用的处理器数。</td></tr></tbody></table><p> 总的原则是给 <code>shuffle</code> 过程尽可能多提供内存空间。但是 <code>map</code> 函数和 <code>reduce</code> 函数不能无限制使用内存，需要尽可能少用内存。<br> 运行 <code>map</code> 任务和 <code>reduce</code> 任务的 <code>JVM</code> 由 <code>mapred.child.java.opts</code> 属性设置内存大小。<br> 在 <code>map</code> 端通过避免多次溢出写磁盘来获得最佳性能，一次最好；而在 <code>reduce</code> 端，中间数据全部驻留在内存时就能获得最佳性能。</p></li><li><p><code>reduce</code> 端的调优属性</p><table><thead><tr><th>属性名称</th><th>类型</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td><code>mapreduce.reduce.shuffle.parallelcopies</code></td><td><code>int</code></td><td><code>5</code></td><td>用于把 <code>map</code> 输出复制到 <code>reducer</code> 的线程数</td></tr><tr><td><code>mapreduce.reduce.shuffle.maxfetchfailures</code></td><td><code>int</code></td><td><code>10</code></td><td>在声明失败之前 <code>reducer</code> 获取一个 <code>map</code> 输出所花的最大时间</td></tr><tr><td><code>mapreduce.task.io.sort.factor</code></td><td><code>int</code></td><td><code>10</code></td><td>排序文件时一次最多合并的流的数量，</td></tr><tr><td><code>mapreduce.reduce.shuffle.input.buffer.percent</code></td><td><code>float</code></td><td><code>0.70</code></td><td><code>shuffle</code> 复制阶段分配给 <code>map</code> 输出的缓冲区占堆空间的百分比</td></tr><tr><td><code>mapreduce.reduce.shuffle.merge.percent</code></td><td><code>float</code></td><td><code>0.66</code></td><td><code>map</code> 输出缓冲区的阈值使用比例，用于启动合并输出和磁盘溢出写的过程</td></tr><tr><td><code>mapreduce.reduce.merge.inmen.threshold</code></td><td><code>int</code></td><td><code>1000</code></td><td>启动合并输出和磁盘溢出写过程的 <code>map</code> 输出的阈值数，<code>0</code> 或者更小的数意味者没有阈值限制</td></tr><tr><td><code>mapreduce.reduce.input.buffer.percent</code></td><td><code>float</code></td><td><code>0.0</code></td><td><code>reduce</code> 过程中在内存中保存 <code>map</code> 输出的空间占整个堆空间的比例。</td></tr></tbody></table></li></ol><hr><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p>计数器是收集作业统计信息的有效手段之一，用于质量控制和应用级统计，另外计数器还可以辅助诊断系统故障。</p><h5 id="内置计数器"><a href="#内置计数器" class="headerlink" title="内置计数器"></a>内置计数器</h5><p><code>Hadoop</code> 为每个作业维护若干个内置计数器，以描述多项指标。</p><table><thead><tr><th>组别</th><th>名称&#x2F;类别</th></tr></thead><tbody><tr><td><code>MapReduce</code> 计数器</td><td><code>org.apache.hadoop.mapreduce.TaskCounter</code></td></tr><tr><td>文件系统计数器</td><td><code>org.apache.hadoop.mapreduce.FileSystemCounter</code></td></tr><tr><td><code>FileInputFormat</code> 计数器</td><td><code>org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter</code></td></tr><tr><td><code>FileOutputFormat</code> 计数器</td><td><code>org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter</code></td></tr><tr><td>作业计数器</td><td><code>org.apache.hadoop.mapreduce.JobCounter</code></td></tr></tbody></table><p>这些计数器被划分为若干组，各组要么包含任务计数器（在任务处理过程中不断更新），要么包含作业计数器（在作业处理过程中不断更新）。</p><h5 id="Java-自定义计数器"><a href="#Java-自定义计数器" class="headerlink" title="Java 自定义计数器"></a><code>Java</code> 自定义计数器</h5><p><code>MapReduce</code> 允许用户编写程序来定义计数器，计数器的值可以在 <code>mapper</code> 或 <code>reducer</code> 中增加，<strong>计数器由一个 <code>Java</code> 枚举类型来定义</strong>，以便对有关的计数器分组。<br>一个作业可以定义的枚举类型数量不限，各个枚举类型所包含的字段数量也不限。枚举类型的名称即为组的名称，枚举类型的字段就是计数器名称，计数器是全局的。换言之，<code>MapReduce</code> 框架将跨所有 <code>map</code> 和 <code>reduce</code> 聚集这些计数器，并在作业结束时产生一个最终结果。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">enum</span> <span class="title class_">Tem</span> &#123;</span><br><span class="line">MISSING,</span><br><span class="line">MALFORMED</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用</span></span><br><span class="line">context.getCounter(Tem.MISSING).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p>排序是 <code>MapReduce</code> 的核心，<code>MapReduce</code> 使用排序来组织数据，而排序也分为不同的数据集排序方式。</p><h5 id="部分排序"><a href="#部分排序" class="headerlink" title="部分排序"></a>部分排序</h5><p>默认情况下，<code>MapReduce</code> 会根据输入记录的键对数据集排序。</p><h5 id="全排序"><a href="#全排序" class="headerlink" title="全排序"></a>全排序</h5><p>如何生成一个全局排序的文件呢？<br>最简单的方法就是，首先创建一系列排序好的文件；其次串联这些文件；最后生成一个全局排序的文件。其主要思路就是使用一个 <code>patitioner</code> 来描述输出的全局排序。</p><p>该方法的关键点在于如何划分各个分区。在理想情况下，各分区所包含的记录数应大致相等。使作业的总体执行时间不会受制于个别 <code>reducer</code>。</p><h5 id="辅助排序"><a href="#辅助排序" class="headerlink" title="辅助排序"></a>辅助排序</h5><p>通过特定的方法对键进行排序和分组以实现对值的排序。<br>通过设置一个按照键进行分区的 <code>patitioner</code>，这样可以确保相同 <code>patitioner</code> 的记录会被发送到同一个 <code>reducer</code> 中，而在同一个分区中，仍然可以通过键进行分组。</p><p>按值排序的方法总结：</p><ul><li>定义包括自然键和自然值的组合键。</li><li>根据组合键对记录进行排序，即同时用自然键和自然值进行排序。</li><li>针对组合键进行分区和分组时均只考虑自然键。</li></ul><h4 id="边数据"><a href="#边数据" class="headerlink" title="边数据"></a>边数据</h4><p><strong>边数据（<code>side data</code>）是作业所需的额外的只读数据，以辅助处理主数据集</strong> 。其所面临的挑战在于如何使所有 <code>map</code> 或 <code>reduce</code> 任务（散布在集群内不）都能够方便而高效地使用边数据。</p><p><code>Hadoop</code> 的分布式缓存机制能够在任务运行过程中及时地将文件和存档复制到任务节点以供使用，不过为了节约网络带宽，在每一个作业中，各个文件通常只需要复制到一个节点一次。<br>对于使用 <code>GenericOptionsParser</code> 的工具来说，用户可以使用 <code>-files</code> 选项指定待分发的文件，文件内包含以逗号隔开的 <code>URI</code> 列表，如果没有指定文件系统，则这些文件会被默认为本地文件。<code>-archives</code> 选项可以向自己的任务中复制存档文件（<code>JAR</code> 文件、<code>ZIP</code> 文件、<code>tar</code> 文件和 <code>gzipped tar</code> 文件），这些文件会被解档到任务节点。<code>-libjars</code> 选项会把 <code>JAR</code> 文件添加到 <code>mapper</code> 和 <code>reducer</code> 任务的类路径中。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h3&gt;&lt;p&gt;&lt;code&gt;MapReduce&lt;/code&gt; 是一种用于&lt;strong&gt;数据处理的编程模型&lt;/strong&gt;，其本质是&lt;strong&gt;并行运行&lt;/strong&gt;，因此可以将大规模的数据分析任务分发给任何一个拥有足够多机器的数据中心，当然其优势也是&lt;strong&gt;处理大规模数据集&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;MapReduce&lt;/code&gt; 任务过程分为两个处理阶段： &lt;strong&gt;&lt;code&gt;map&lt;/code&gt; 阶段&lt;/strong&gt;和 &lt;strong&gt;&lt;code&gt;reduce&lt;/code&gt; 阶段&lt;/strong&gt;。每个阶段都是以&lt;strong&gt;键值对&lt;/strong&gt;作为输入和输出，其类型由开发者决定，当然 &lt;code&gt;map&lt;/code&gt; 函数和 &lt;code&gt;reduce&lt;/code&gt; 函数也是由开发者实现。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Lombok-constructor is already defined</title>
    <link href="https://blog.vgbhfive.cn/Lombok-constructor-is-already-defined/"/>
    <id>https://blog.vgbhfive.cn/Lombok-constructor-is-already-defined/</id>
    <published>2023-02-14T13:42:33.000Z</published>
    <updated>2023-02-14T13:53:36.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p>今天工作的时候写了这么一段代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@ToString</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ObjectA</span> &#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><span id="more"></span><p>接下来就是一系列的上传代码、发版、打包…..，然后就出现了神奇的异常信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">constructor <span class="title function_">ObjectA</span><span class="params">()</span> is already defined in ObjectA</span><br></pre></td></tr></table></figure><p>回头去看代码，这真是让人百思不得其解啊</p><hr><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ol><li><p>无引用即删除该类<br>该类内部无任何属性，即可认为无用，再判断没有任何引用，最终删除掉这个类就可以了。</p></li><li><p>该类已被引用但无属性<br>分析异常出现的原因即可以判断为 <code>@NoArgsConstructor</code> 与 <code>@AllArgsConstructor</code> 注解生成的构造对象函数因为该对象无属性导致重名冲突，那么删除其中一个注解即可解决这个问题。</p></li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;引入&quot;&gt;&lt;a href=&quot;#引入&quot; class=&quot;headerlink&quot; title=&quot;引入&quot;&gt;&lt;/a&gt;引入&lt;/h3&gt;&lt;p&gt;今天工作的时候写了这么一段代码：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Getter&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Setter&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@ToString&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@NoArgsConstructor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@AllArgsConstructor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title class_&quot;&gt;ObjectA&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="Day" scheme="https://blog.vgbhfive.cn/tags/Day/"/>
    
    <category term="Spring Boot" scheme="https://blog.vgbhfive.cn/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-HDFS</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-HDFS/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-HDFS/</id>
    <published>2023-02-04T08:02:34.000Z</published>
    <updated>2023-03-11T15:16:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a><code>HDFS</code></h3><p>当数据集的大小超过一台计算机的存储上限时，就有必要对数据进行分区然后存储到其他的计算机上。管理网络中跨多台计算机存储的文件系统被称为<strong>分布式文件系统（<code>distributed filesystem</code>）</strong>，该架构于网络之上，势必会引起网络编程的复杂性，因此分布式文件系统比普通磁盘文件系统更为复杂。<br><code>Hadoop</code> 自带一个称为 <code>HDFS</code> 的分布式文件系统，也是 <code>Hadoop</code> 的旗舰级文件系统，即 <code>Hadoop Distributed Filesystem</code>。</p><span id="more"></span><p><code>HDFS</code> 以流式数据访问模式来存储超大文件，运行于商业硬件集群上。</p><ul><li><strong>超大文件</strong>。这里指的是具有几百 <code>MB</code>、几百 <code>GB</code>或者以上大小的文件。</li><li><strong>流式数据访问</strong>。<code>HDFS</code> 采用的是一次写入、多次读写的高效访问模式。</li><li><strong>商业硬件</strong>。运行于各种商业硬件上即可，不需要专业的硬件。</li></ul><hr><h3 id="HDFS-相关概念"><a href="#HDFS-相关概念" class="headerlink" title="HDFS 相关概念"></a><code>HDFS</code> 相关概念</h3><h4 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h4><p>每个磁盘都有默认的数据块大小，这是磁盘进行<strong>数据读&#x2F;写的最小单位</strong>。构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统中的块，该文件系统块的大小可以是磁盘块的整数倍，文件系统块一般为几千个字节，而磁盘块一般为 <code>512</code> 字节。<br><code>HDFS</code> 也同样存在类似<strong>块（<code>block</code>）</strong>的概念，默认为 <code>128MB</code>。与单一磁盘上的文件系统类似，<code>HDFS</code> 上的文件也会被划分为块大小的多个分块（<code>chunk</code>），作为独立的存储单元。<br><small>与单一磁盘文件系统不同之处在于 <code>HDFS</code> 中小于一个块大小的文件不会占据整个块的空间。</small></p><p>那么 <code>HDFS</code> 中的块为什么这么大？有何好处？</p><ol><li><p><code>HDFS</code> 中的块比磁盘的块大，其目的在于<strong>最小化寻址开销</strong>。如果块足够大，那么从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间，因此传输一个由多个块组成的大文件的时间取决于磁盘传输速率。<br>默认的块大小为 <code>128MB</code>，但是很多情况下 <code>HDFS</code> 安装时会使用更大的块，并且随着新一代磁盘驱动器传输速率的提升，块的大小会被设置的更大。但是这个参数也不能被设置的太大，<code>MapReduce</code> 中的 <code>map</code> 任务通常一次只处理一个块中的数据，因此如果任务数太少（小于集群中的节点数量），作业的运行速度就会很慢。</p></li><li><p>对分布式文件系统中的块进行抽象带来的好处如下：</p><ul><li>一个文件的大小可以<strong>大于网络中任意一个磁盘的容量</strong>。文件的所有块并不需要存储在同一块磁盘上，可以利用集群中任意一个磁盘进行存储。</li><li>使用抽象块而非整个文件作为存储单元，大大<strong>简化存储子系统的设计</strong>。将存储子系统的处理对象设置为块，可简化存储管理（块大小已经固定，因此计算存储容量相对容易），同时也消除了对元数据的顾虑（块只需要存储数据即可，并不需要存储文件的元数据，例如权限信息等，这样其它系统可以单独管理这些元数据）。</li><li>块非常适用于数据备份进而提供<strong>数据容错能力</strong>和<strong>提高可用性</strong>。</li></ul></li></ol><p><code>HDFS</code> 中的 <code>fsck</code> 指令可以展示块信息。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck / -files -blocks</span><br></pre></td></tr></table></figure><h4 id="namenode-和-datanode"><a href="#namenode-和-datanode" class="headerlink" title="namenode 和 datanode"></a><code>namenode</code> 和 <code>datanode</code></h4><p><code>HDFS</code> 集群由两类节点以<strong>管理节点-工作节点模式</strong>运行，即<strong>一个 <code>namenode</code>（管理节点）</strong>和<strong>多个 <code>datanode</code>（工作节点）</strong>。</p><p><code>namenode</code> 管理文件系统的命名空间，他维护着文件系统树及整颗树内所有的文件和目录，这些信息以两个文件形式永久保存在本地磁盘上：<strong>命名空间镜像文件</strong>和<strong>编辑日志文件</strong>。同时 <code>namenode</code> 也负责记录每个文件中各个块所在的节点信息，但是并不会永久保存块的位置信息，因为这些信息会在系统启动时根据数据节点信息重建。<br><code>datanode</code> 是文件系统的工作节点，根据需要<strong>存储并检索数据块</strong>（受客户端或者 <code>namenode</code> 调度），并且还需要定期向 <code>namenode</code> 发送所存储块的列表和心跳请求。<br>客户端代表用户通过与 <code>namenode</code> 和 <code>datanode</code> 交互来访问整个文件系统，客户端通过提供一个<strong>类似于 <code>POSIX</code> 的文件系统接口</strong>来实现功能，而不需要知道 <code>namenode</code> 和 <code>datanode</code> 的存在。</p><p>细心的人会发现<strong>一个 <code>namenode</code> 不会存在单点故障</strong>吗？那么 <code>Hadoop</code> 提供了两种机制：</p><ul><li><strong>备份组成文件系统元数据持久状态的文件</strong>。<code>Hadoop</code> 可以通过配置使 <code>namenode</code> 在多个文件系统上保存元数据的持久状态，这些写操作都是实时同步的，并且还是原子操作。一般的配置都是在持久化本地磁盘时同时写入远程挂载的网络文件系统（<code>NFS</code>）。</li><li><strong>运行一个辅助 <code>namenode</code>，</strong>但是他不能被用作 <code>namenode</code>。这个辅助 <code>namenode</code> 的作用在于<strong>定期</strong>合并编辑日志与命名空间镜像，以防止镜像日志过大。另外辅助 <code>namenode</code> 通常部署在另外一台机器上，需要占用大量 <code>CPU</code> 时间和内存来执行合并操作。由于辅助 <code>namenode</code> 是定期同步主节点 <code>namenode</code>，因此会存在保存的状态滞后于主节点，难免会丢失部分数据，这种情况下都是将远程挂载的 <code>NFS</code> 复制到辅助 <code>namenode</code> 作为新的主节点 <code>namenode</code> 运行。</li></ul><h4 id="块缓存"><a href="#块缓存" class="headerlink" title="块缓存"></a>块缓存</h4><p>通常 <code>datanode</code> 从磁盘中读取块，但是对于频繁访问的块，该块可能会被显式地存储在 <code>datanode</code> 的内存中，以<strong>堆外缓存（<code>off-heap block cache</code>）</strong>的形式存在。默认情况下一个块仅缓存在一个 <code>datanode</code> 的内存中（可以针对文件修改 <code>datanode</code> 的数量）。</p><p>通过<strong>块缓存</strong>作业调度器在缓存块的 <code>datanode</code> 上执行任务，可以提高读操作的性能。当然用户可以通过在缓存池（<code>cache pool</code>）中增加一个 <code>cache directive</code> 来告诉 <code>namenode</code> 需要缓存哪些文件缓存多久。<br><small>缓存池是一个用于管理缓存权限和资源使用的管理性分组。</small></p><h4 id="联邦-HDFS"><a href="#联邦-HDFS" class="headerlink" title="联邦 HDFS"></a>联邦 <code>HDFS</code></h4><p><code>namenode</code> 在内存中保存着文件系统中每个文件和每个数据块的引用关系，这也意味着对于一个超大集群来说，<strong>内存将成为限制系统横向扩展的瓶颈</strong>。不过联邦 <code>HDFS</code> 允许系统通过添加多个 <code>namenode</code> 来实现横向扩展，即每个 <code>namenode</code> 管理文件系统命名空间中的一部分。<br><small>例如 <code>A</code> <code>namenode</code> 负责管理 <code>/home/a</code> 下的所有文件，而 <code>B</code> <code>namenode</code> 负责管理 <code>/home/b</code> 下的所有文件。</small></p><p>在联邦环境下<strong>每个 <code>namenode</code> 维护一个命名空间卷（<code>namespace volume</code>）</strong>，由<strong>命名空间的元数据</strong>和<strong>一个数据块池（<code>block pool</code>）</strong>组成，数据块池包含该命名空间下文件的所有数据块。<br>命名空间卷之间是相互独立的，两两之间并不相互通信，甚至某一个 <code>namenode</code> 失效也不会影响到其他 <code>namenode</code> 的可用性；另外命名空间卷下的数据块池不能再切分，这就意味着集群中的 <code>datanode</code> 需要注册到每个 <code>namenode</code> 中，<code>namenode</code> 存储来自多个数据块池中的数据块。</p><p>客户端通过使用<strong>挂载数据表将文件路径映射到 <code>namenode</code></strong> 来访问联邦 <code>HDFS</code>，通过 <code>ViewFileSystem</code> 和 <code>viewfa://URI</code> 进行配置和管理。 </p><h4 id="高可用性"><a href="#高可用性" class="headerlink" title="高可用性"></a>高可用性</h4><p>通过联合使用在多个文件系统中备份 <code>namenode</code> 的元数据和通过备用 <code>datanode</code> 创建监测点防止数据丢失，但这依然无法解决文件系统的高可用性，<code>namenode</code> 依然存在<strong>单点失效（<code>SPOF，single point of failure</code>）</strong>的问题，如果 <code>namenode</code> 失效则 <code>MapReduce</code> 作业均无法读、写文件。<br>在这种情况下从一个失效的 <code>namenode</code> 恢复，管理员需要启动一个拥有文件系统元数据副本的新的 <code>namenode</code> ，并配置 <code>datanode</code> 和客户端以便使用新的 <code>namenode</code>，而新的 <code>namenode</code> 需要达到以下情形才能响应服务：</p><ul><li><strong>将命名空间的镜像导入内存中</strong>。</li><li><strong>重演编辑日志</strong>。</li><li><strong>接收到足够多的来自 <code>datanode</code> 的数据块报告并退出安全模式</strong>。</li></ul><p><small>对于一个大型集群 <code>namenode</code> 的冷启动需要 <code>30</code> 分钟以上，甚至更久。</small></p><p>为了解决上述遇到的问题，<code>HDFS</code> 支持在 <code>2.x</code> 版本支持<strong>高可用性</strong>，即通过配置一对<strong>活动-备用（<code>active-standby</code>）</strong><code>namenode</code>。当活动 <code>namenode</code> 失效，备用 <code>namenode</code> 会接管它的任务并开始服务来自客户端的请求，不会有任何明显中断。不过要支持高可用性还需要以下几个方面的支持：</p><ul><li><code>namenode</code> 之间通过<strong>高可用性共享存储</strong>实现编辑日志的共享。</li><li><code>datanode</code> 需要<strong>同时向两个 <code>namenode</code> 发送数据块处理报告</strong>。</li><li>客户端需要使用<strong>特定的机制</strong>来处理 <code>namenode</code> 失效的问题，不过该机制对用户是透明的。</li><li>辅助 <code>namenode</code> 的角色被备用 <code>namenode</code> 所包含，备用 <code>namenode</code> 为活动 <code>namenode</code> 命令空间<strong>设置周期性检查点</strong>。</li></ul><hr><h3 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h3><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><p>现在通过命令行交互来进一步了解 <code>HDFS</code>，在设置伪分布配置时，有两个属性需要着重说明：</p><ul><li><code>fs.defaultFS</code>：设置为 <code>hdfs://localhost/</code>，用于设置 <code>Hadoop</code> 的默认文件系统。<br> <small>文件系统由 <code>URI</code> 指定，使用 <code>hdfs URI</code> 来配置 <code>HDFS</code> 为 <code>Hadoop</code> 的默认文件系统，<code>HDFS</code> 的守护程序通过该属性项来确定 <code>HDFS namenode</code> 的主机及端口。</small></li><li><code>dfs.replication</code>：默认值为 <code>3</code>，修改配置值设置为 <code>1</code>，不然会出现副本不足的警告，因为 <code>HDFS</code> 无法将数据块复制到三个 <code>datanode</code> 上。</li></ul><p>至此文件系统已经配置完毕，接下来就可以执行常见的文件系统操作。（相关命令行的内容可以看前一篇文章）</p><h5 id="文件访问权限"><a href="#文件访问权限" class="headerlink" title="文件访问权限"></a>文件访问权限</h5><p>针对文件和目录，<code>HDFS</code> 的权限模式和 <code>POSIX</code> 的权限模式非常相似。共提供三类权限模式：</p><ul><li><strong>只读权限（<code>r</code>）</strong>：读取文件或列出目录内容时需要该权限。</li><li><strong>写入权限（<code>w</code>）</strong>：写入一个文件或是在一个目录上新建及删除文件或目录需要该权限。</li><li><strong>可执行权限（<code>x</code>）</strong>：该权限对于文件而言可以忽略，不过在访问目录的子项时需要该权限。</li></ul><p>每个文件和目录都有所属用户（<code>owner</code>）、所属组别（<code>group</code>）及模式（<code>mode</code>），这个模式是由所有用户的权限、组内用户的权限及其他用户的权限组成。</p><h4 id="distcp"><a href="#distcp" class="headerlink" title="distcp"></a><code>distcp</code></h4><p><code>DistCp</code>（分布式拷贝）是用于<strong>大规模集群内部和集群之间拷贝的工具</strong>，使用 <strong><code>Map/Reduce</code> 实现文件分发、错误处理和恢复以及报告生成</strong>。通过把文件和目录的列表作为 <code>map</code> 任务的输入，每个任务会完成源列表中部分文件的拷贝。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop distcp [-p [rbugp]] [-i] [-<span class="built_in">log</span> &lt;logdir&gt;] [-m &lt;num_maps&gt;] [-overwrite] [-update] [-f &lt;urilist_uri&gt;] &lt;source-path&gt; &lt;dest-path&gt;</span><br></pre></td></tr></table></figure><p><small>上述命令仅限于相同版本的 <code>HDFS</code> 之间拷贝数据。</small></p><ol><li><p><code>p</code><br>保存文件信息。<code>rbugp</code> 分别表示 <em>副本数量</em>、<em>块大小</em>、<em>用户</em>、<em>组</em>、<em>权限</em>。</p></li><li><p><code>i</code><br>忽略失败</p></li><li><p><code>log</code><br>记录日志到 <code>&lt;logdir&gt;</code></p></li><li><p><code>m</code><br>同时拷贝的最大数目。</p></li><li><p><code>overwrite</code><br>是否覆盖目标。</p></li><li><p><code>update</code><br>如果源和目标的大小不一样则进行覆盖。</p></li><li><p><code>f</code><br>使用 <code>&lt;urilist_uri&gt;</code> 作为源文件列表。</p></li></ol><hr><h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><p><code>Hadoop</code> 有一个<strong>抽象的文件系统概念</strong>，而 <code>HDFS</code> 只是其中的一个实现。<code>Java</code> 抽象类 <code>org.apache.hadoop.fs.FileSystem</code> 定义了 <code>Hadoop</code> 文件系统的客户端接口，并且该抽象类也有几个具体的实现：</p><table><thead><tr><th>文件系统</th><th><code>URI</code></th><th><code>Java</code> 实现</th><th>描述</th></tr></thead><tbody><tr><td><code>Local</code></td><td><code>file</code></td><td><code>fs.LocalFileSystem</code></td><td><div style="width: 300px">使用客户端校验和的本地磁盘文件系统。其中使用 <code>RawLocalFileSystem</code> 表示无校验和的本地磁盘文件系统</div></td></tr><tr><td><code>HDFS</code></td><td><code>hdfs</code></td><td><code>hdfs.DistributedFileSystem</code></td><td><code>Hadoop</code> 的分布式文件系统。将 <code>HDFS</code> 设计成与 <code>MapReduce</code> 结合使用，可以实现高性能。</td></tr><tr><td><code>WebHDFS</code></td><td><code>Webhdfs</code></td><td><code>Hdfs.web.WebHdfsFileSystem</code></td><td>基于 <code>HTTP</code> 的文件系统，提供对 <code>HDFS</code> 的认证读&#x2F;写访问。</td></tr><tr><td><code>Secure</code> <code>WebHdfs</code></td><td><code>swebhdfs</code></td><td>hdfs.web.SWebHdfsFileSystem</td><td><code>WebHDFS</code> 的 <code>HTTPS</code> 版本</td></tr><tr><td><code>HAR</code></td><td><code>har</code></td><td><code>fa.HarFileSystem</code></td><td>一个构建在其他文件系统之上用于文件存档的文件系统。<code>Hadoop</code> 存档文件通常用于将 <code>HDFS</code> 中的多个文件打包成一个存档文件，以减少 <code>namenode</code> 内存的使用。使用 <code>hadoop</code> 的 <code>achive</code> 命令来创建 <code>HAR</code> 文件。</td></tr><tr><td><code>View</code></td><td><code>viewfs</code></td><td><code>viewfs.ViewFileSystem</code></td><td>针对其他 <code>Hadoop</code> 文件系统的客户端挂载表。通常用于为联邦 <code>namenode</code> 创建挂载点。</td></tr><tr><td><code>FTP</code></td><td><code>ftp</code></td><td><code>fa.ftp.FTPFileSystem</code></td><td>由 <code>FTP</code> 服务器支持的文件系统</td></tr><tr><td><code>S3</code></td><td><code>S3a</code></td><td><code>fa.s3a.S3AFileSystem</code></td><td>由 <code>Amazon S3</code> 支持的文件系统。替代老版的 <code>s3n</code> 实现。</td></tr><tr><td><code>Azure</code></td><td><code>wasb</code></td><td><code>fs.azure.NativeAzureFileSystem</code></td><td>由 <code>Microsoft Azure</code> 支持的文件系统。</td></tr><tr><td><code>Swift</code></td><td><code>swift</code></td><td><code>fs.swift.snative.SwiftNativeFileSystem</code></td><td>由 <code>OpenStack Swift</code> 支持的文件系统。</td></tr></tbody></table><p><code>Hadoop</code> 对文件系统提供了许多接口，一般使用 <code>URI</code> 方案来选取合适的文件系统实例进行交互。例如列出本地文件系统根目录下的文件：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop fs -<span class="built_in">ls</span> file:///</span><br></pre></td></tr></table></figure><h4 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h4><p><code>Hadoop</code> 是用 <code>Java</code> 写的，通过 <code>Java API</code> 可以调用大部分 <code>Hadoop</code> 文件系统的交互操作。</p><ol><li><p><code>HTTP</code><br>非 <code>Java</code> 开发的应用访问 <code>HDFS</code> 会很不方便，因此由 <code>WebHDFS</code> 协议提供的 <code>HTTP REST API</code> 则使得其他语言开发的应用能够更方便地与 <code>HDFS</code> 交互。<br><small><code>HTTP</code> 接口比原生 <code>Java</code> 客户端要慢，因此尽量不要传输特大数据。</small></p><p> 通过 <code>HTTP</code> 访问 <code>HDFS</code> 有两种方法：</p><ul><li>直接访问，<code>HDFS</code> 守护进程直接服务于来住客户端的 <code>HTTP</code> 请求。<br> <img src="https://s2.loli.net/2023/02/09/wFOC1HuLXivtez2.png" alt="hadoop_1_3.jpg"><br> <code>namenode</code> 和 <code>datanode</code> 内嵌的 <strong><code>web</code> 服务器</strong>作为 <code>WebHDFS</code> 的端节点运行。文件元数据操作由 <code>namenode</code> 管理，文件读&#x2F;写操作首先被发往 <code>namenode</code>，由 <code>namenode</code> 发送一个 <code>HTTP</code> 重定向至某个客户端，指示以流方式传输文件数据的目的或源 <code>datanode</code>。（由于 <code>dfs.webhdfs.enabled</code> 被设置为 <code>true</code>，<code>WebHDFS</code> 默认是启用状态）</li><li>通过代理（一个或多个）访问，客户端通常使用 <code>DistributedFileSystem API</code> 访问 <code>HDFS</code>。<br> <img src="https://s2.loli.net/2023/02/09/q8SQF3xibR9dgnM.png" alt="hadoop_1_4.jpg"><br> 依靠一个或多个<strong>独立代理服务器</strong>通过 <code>HTTP</code> 访问 <code>HDFS</code>。所有到集群的网路通信都需要经过代理，因此客户端从不直接访问 <code>namenode</code> 和 <code>datanode</code>。<code>HttpFS</code> 代理提供和 <code>WebHDFS</code> 相同的 <code>HTTP</code> 和 <code>HTTPS</code> 接口，这样客户端能够通过 <code>webhdfs URI</code> 协议访问这类接口。<br> <code>httpFS</code> 代理的启动独立于 <code>namenode</code> 和 <code>datanode</code> 的守护进程，使用 <code>httpfs.sh</code> 脚本启动，默认在一个不同的端口上监听（端口号为 <code>14000</code>）。</li></ul><p> <small>上述两者都使用了 <strong><code>WebHDFS</code></strong> 协议。</small></p></li><li><p><code>C</code><br><code>Hadoop</code> 提供了一个名为 <strong><code>libhdfs</code></strong> 的 <code>C</code> 语言库，该语言库是 <code>Java FileSystem</code> 接口类的一个镜像，使用 <strong><code>Java</code> 原生接口（<code>JNI, Java Native Interface</code>）</strong>调用 <code>Java</code> 文件系统客户端。同时还有一个 <code>libwebhdfs</code> 库，该库是 <code>WebHDFS</code> 接口的实现。</p></li><li><p><code>NFS</code><br>使用 <code>Hadoop</code> 的 <strong><code>NFSv3</code></strong> 网关将 <code>HDFS</code> 挂载为本地客户端的文件系统的想法是可行的。可以通过使用 <code>Unix</code> 程序与该文件系统交互，通过任意一种编程语言调用 <code>POSIX</code> 库来访问文件系统，由于 <code>HDFS</code> 仅能以 <em>追加模式</em> 写文件，因此可以往文件末尾添加数据，但不能随即修改文件。</p></li><li><p><code>FUSE</code><br><strong>用户空间文件系统（<code>FUSE, FileSystem in Userspace</code>）</strong>允许将用户空间实现的文件系统作为 <code>Unix</code> 文件系统进行集成。通过使用 <code>Hadoop</code> 的 <code>Fuse-DFS</code> 功能模块，<code>HDFS</code> 或者任何一个 <code>Hadoop</code> 文件系统均可以作为一个标准的本地文件系统进行挂载。<br><code>Fuse-DFS</code> 是用 <code>C</code> 语言实现的，使用 <code>libhdfs</code> 作为访问 <code>HDFS</code> 的接口。<br><small>对于挂载 <code>HDFS</code> 的解决方案，<code>Hadoop NFS</code> 相比 <code>Fuse-DFS</code> 更优先选择。</small></p></li></ol><hr><h3 id="Java-接口"><a href="#Java-接口" class="headerlink" title="Java 接口"></a><code>Java</code> 接口</h3><p>该部分主要探索 <code>Hadoop</code> 的 <code>FileSystem</code> 类，该类是与 <code>Hadoop</code> 的某一个文件系统进行交互的 <code>API</code>。</p><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><h5 id="Hadoop-URL"><a href="#Hadoop-URL" class="headerlink" title="Hadoop URL"></a><code>Hadoop URL</code></h5><p>从 <code>Hadoop</code> 文件系统读取文件，最简单的办法就是使用 <code>java.net.URL</code> 对象打开数据流，然后从中读取数据。示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CatURL</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        URL.setURLStreamHandlerFactory(<span class="keyword">new</span> <span class="title class_">FsUrlStreamHandlerFactory</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            in = <span class="keyword">new</span> <span class="title class_">URL</span>(args[<span class="number">0</span>]).openStream();</span><br><span class="line">            IOUtils.cpoyBytes(in, System.out, <span class="number">4096</span>, <span class="literal">false</span>);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行示例：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% bin/hadoop CatURL hdfs://localhost/user/vgbh/test.txt</span><br><span class="line">Hello World!</span><br></pre></td></tr></table></figure><p><code>Java</code> 程序要想识别 <code>Hadoop</code> 的 <code>hdfs URL</code>，需要通过 <code>FsUrlStreamHandlerFactory</code> 实例调用 <code>java.net.URL</code> 对象的 <code>setURLStreamHandlerFactory</code> 方法。另外可以调用 <code>Hadoop</code> 中的 <code>IOUtils</code> 类，并在 <code>finally</code> 子句中关闭数据流，同时在输入流和输出流之间复制数据。<br><small><code>setURLStreamHandlerFactory</code> 方法在每一个虚拟机中只能被调用一次，通常在静态方法中调用，该限制则意味着如果程序的其他组件已经声明了 <code>FsUrlStreamHandlerFactory</code> 实例，那么就不能使用该方法从 <code>Hadoop</code> 中读取数据。</small></p><h5 id="FileSystem-API"><a href="#FileSystem-API" class="headerlink" title="FileSystem API"></a><code>FileSystem API</code></h5><p>前一个部分有时候会遇到不能设置 <code>FsUrlStreamHandlerFactory</code> 实例，那么这种情况下可以使用 <strong><code>FileSystem API</code></strong> 来打开一个文件的输入流。<br><code>Hadoop</code> 文件系统通过 <strong><code>Hadoop Path</code> 对象来表示一个文件</strong>，因此可以将<strong>路径视为一个文件系统 <code>URI</code></strong> 。</p><p><code>FileSystem</code> 是一个通用的文件系统 <code>API</code>，那么第一步就是获取对应的文件系统实例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> FileSystem <span class="title function_">get</span><span class="params">(Configuration conf [,URI uri] [,String user])</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><ul><li><code>conf</code>：<code>Configuration</code> 对象封装了客户端或服务器的配置，通过配置文件读取类路径来实现（<code>etc/hadoop/core-site.xml</code>）。</li><li><code>uri</code>：根据给定的 <code>URI</code> 确定使用的文件系统。</li><li><code>user</code>：给定用户来访问文件系统。</li></ul><p>在某些情况下希望获得本地文件系统的运行实例，此时使用 <code>getLocal</code> 方法会更加方便：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> LocalFileSystem <span class="title function_">getLocal</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><p>接下来的第二部就是在获取到 <code>FileSystem</code> 之后，就可以调用 <code>open()</code> 函数来获取文件的输入流：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> FSDataInputStream <span class="title function_">open</span><span class="params">(Path f [,<span class="type">int</span> bufferSize])</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><p><code>bufferSize</code> 表示可设置缓冲区的大小，不设置时默认大小为 <code>4KB</code>。</p><p>完整代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CatFileSystem</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(URI.create(args[<span class="number">0</span>]), conf);</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            in = fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">            IOUtils.cpoyBytes(in, System.out, <span class="number">4096</span>, <span class="literal">false</span>);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行示例：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% bin/hadoop CatFileSystem hdfs://localhost/user/vgbh/test.txt</span><br><span class="line">Hello World!</span><br></pre></td></tr></table></figure><p><small>关于更多的使用方法可以了解下 <code>FileSystem</code> 的接口类 <strong><code>Seekable</code></strong> 和 <strong><code>PositionedReadable</code></strong> </small></p><h4 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h4><p><code>FileSystem</code> 类有一系列新建文件的方法。</p><ol><li><p>最简单的方法就是给准备创建的文件一个 <code>Path</code> 对象，然后返回一个用于写入数据的输出流：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> FSDataOutputStream <span class="title function_">create</span><span class="params">(Path f)</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><p>此方法有多个重载版本，允许指定是否强制覆盖现有文件、文件备份数量、写入文件时缓冲区大小、文件块大小以及文件权限。还有一个<strong>重载方法 <code>Progressable</code> 用于传递回调接口</strong>，可以将数据写入 <code>datanode</code> 的进度通知给应用。<br><small><code>create()</code> 方法能够为需要写入当当前并不存在的文件创建父目录。如果不希望这样，可以在写入时先调用 <code>exist()</code> 方法检查父目录是否存在。</small></p></li><li><p>使用 <code>append()</code> 方法在一个现有文件的末尾追加数据，追加操作允许一个 <code>writer</code> 打开文件后在访问该文件的最后偏移量处追加数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> FSDataOutputStream <span class="title function_">append</span><span class="params">(Path f)</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure></li></ol><p><small><code>FSDataOutputStream</code> 与 <code>FSDataInputStream</code> 不同之处在于，前者不允许在文件中定位。因为 <code>HDFS</code> 只允许对一个已打开的文件顺序写入或者在末尾添加数据。</small></p><p>完整示例代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileCopyWithProgress</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">loc</span> <span class="operator">=</span> args[<span class="number">0</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">dst</span> <span class="operator">=</span> args[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedInputStream</span>(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(loc));</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(URI.create(dst), conf);</span><br><span class="line"></span><br><span class="line">        <span class="type">OutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(dst), <span class="keyword">new</span> <span class="title class_">Progressable</span>() &#123;</span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">progress</span><span class="params">()</span> &#123;</span><br><span class="line">                System.out.print(<span class="string">&quot;.&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        IOUtils.cpoyBytes(in, out, <span class="number">4096</span>, <span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行示例：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% bin/hadoop FileCopyWithProgress input/vgbh/a.txt hdfs://localhost/user/vgbh/test_1.txt</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><p><code>FileSystem</code> 实例提供了创建目录的方法，该方法可以一次性创建所有必要但还没有的目录，均创建成功后返回 <code>true</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">mkdirs</span><span class="params">(Path f)</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><h4 id="查询文件"><a href="#查询文件" class="headerlink" title="查询文件"></a>查询文件</h4><h5 id="文件元数据"><a href="#文件元数据" class="headerlink" title="文件元数据"></a>文件元数据</h5><p>任何文件系统的重要特征其中之一都是提供其<strong>目录结构浏览</strong>和<strong>检索所存文件和目录相关信息</strong>的功能。<code>FileStatus</code> 类封装了文件系统中文件和目录的元数据，包括文件长度、块大小、复本、修改时间、所有者以及权限信息。<br><code>FileSystem</code> 的 <strong><code>getFileStatus()</code></strong> 方法用于获取文件或目录的 <code>FileStatus</code> 对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> FileStatus <span class="title function_">getFileStatus</span><span class="params">(Path f)</span> <span class="keyword">throws</span> FileNotFoundException</span><br></pre></td></tr></table></figure><h5 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h5><p>在查找到文件或目录的相关信息后，那么就下来就是列出目录中的内容。<code>FileSystem</code> 的 <strong><code>listStatus()</code></strong> 方法可以实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> FileStatus[] listStatus(Path f [,PathFilter filter]) <span class="keyword">throws</span> IOException</span><br><span class="line"><span class="keyword">public</span> FileStatus[] listStatus(Path[] f [,PathFilter filter]) <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><p>当 <code>f</code> 传入的参数为一个文件时，那么就会返回数组长度为 <code>1</code> 的 <code>FileStatus</code> 对象；但若是一个目录时，则返回多个 <code>FileStatus</code> 对象，表示此目录中包含的目录和文件。另外<strong>重载方法允许使用 <code>PathFilter</code> 来限制匹配的文件和目录</strong> 。</p><h5 id="文件模式"><a href="#文件模式" class="headerlink" title="文件模式"></a>文件模式</h5><p>偶尔会出现在单个操作中处理一批文件的需求。因此在一个<strong>表达式中使用通配符来匹配多个文件</strong>正是解决办法，无需列举每个文件和目录来指定输入。<br><code>Hadoop</code> 为执行通配符提供了 <strong><code>globStatus()</code></strong> 方法，该方法返回路径格式与指定模式匹配的所有 <code>FileStatus</code> 组成的数组，并按路径排序。<code>PathFilter</code> 参数作为可选项进一步对匹配结果进行限制。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> FileStatus[] globStatus(Path pathPattern [,PathFilter filter]) <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><p>另外 <code>Hadoop</code> 支持的通配符与 <a href="https://www.cnblogs.com/chengmo/archive/2010/10/17/1853344.html"><code>Unix bash shell</code></a> 支持的相同。</p><h5 id="PathFilter-对象"><a href="#PathFilter-对象" class="headerlink" title="PathFilter 对象"></a><code>PathFilter</code> 对象</h5><p>通配符模式有时并不能够精确地描述想要的文件集。因此 <code>FileSystem</code> 的 <code>listStatus()</code> 和 <code>globStatus()</code> 方法均提供了可选的 <strong><code>PathFilter</code> 对象来控制通配符</strong>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">PathFilter</span> &#123;</span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">accept</span><span class="params">(Path path)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h4><p><code>FileSystem</code> 的 <code>delete()</code> 方法可以永久性删除文件或目录。如果 <code>f</code> 是一个文件或空目录，则 <code>recursive</code> 值就会被忽略，只有值为 <code>true</code> 时非空目录及其内容才会被删除，否则会抛出 <code>IOException</code> 异常。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">delete</span><span class="params">(Path f, <span class="type">boolean</span> recursive)</span> <span class="keyword">throws</span> IOException</span><br></pre></td></tr></table></figure><hr><h3 id="数据读取和写入"><a href="#数据读取和写入" class="headerlink" title="数据读取和写入"></a>数据读取和写入</h3><p>了解客户端如何与 <code>HDFS</code>、<code>datanode</code>、<code>namenode</code> 进行交互，明白如何读取和写入文件。</p><h4 id="剖析读取"><a href="#剖析读取" class="headerlink" title="剖析读取"></a>剖析读取</h4><p>下图展示在读取文件时事件的发生顺序：<br><img src="https://s2.loli.net/2023/02/14/jRGN2T9HokPLAtr.png" alt="hadoop_1_6.jpg"></p><ol><li><p><code>open</code><br>客户端通过调用 <code>FileSystem</code> 对象的 <code>open()</code> 方法来打开需要读取的文件，对于 <code>HDFS</code> 来说该文件就是一个 <strong><code>DistributedFileSystem</code></strong> 实例。</p></li><li><p><code>get block locations</code><br><code>DistributedFileSystem</code> 实例通过远程过程调用（<code>RPC</code>）来调用 <code>namenode</code>，以确定文件起始块的位置。<code>DistributedFileSystem</code> 类返回一个 <strong><code>FSDataInputStream</code></strong> 对象（支持文件定位的输入流）给客户端以便读取数据。<br><code>FSDataInputStream</code> 类内部封装 <code>DFSInputStream</code> 对象，该对象管理 <code>datanode</code> 和 <code>namenode</code> 的 <code>I/O</code>。<br>对于每一个数据块，<code>namenode</code> 会返回存有该块副本的 <code>datanode</code> 地址，该地址根据距离客户端的距离进行排序。</p></li><li><p><code>read</code><br>接着客户端对输入流调用 <code>read()</code> 方法，存储着文件起始几个块的 <code>datanode</code> 地址的 <code>DFSInputStream</code> 随即连接距离最近的文件中第一个块所在的 <strong><code>datanode</code></strong> 。<br><small>在读取数据时如果遇到异常（例如通信异常或者检查校验和未通过），会尝试从这个块的另一个邻近的 <code>datanode</code> 读取数据，然后标记这个 <code>datanode</code>，之后不会再在这个 <code>datanode</code> 上读取数据。</small></p></li><li><p><code>read</code><br>通过对数据流反复调用 <code>read()</code> 方法，就可以将数据从 <code>datanode</code> 传输到客户端。</p></li><li><p><code>read</code><br>在读取到数据的末端时，<code>DFSInputStream</code> 关闭与该 <code>datanode</code> 的连接，然后寻找下一个块的最佳 <code>datanode</code>。这些内部操作对于客户端都是透明的，对客户端来说一直在读取一个连续的数据流。<br>客户端从流中读取数据时，块是按照打开 <code>DFSInputStream</code> 与 <code>datanode</code> 新建连接的顺序读取的，同时也会根据需要询问 <code>namenode</code> 来检索下一批数据块的 <code>datanode</code> 的位置。</p></li><li><p><code>close</code><br>一旦客户端读取数据完成，<code>FSDataInputStream</code> 就会调用 <code>close()</code> 方法。</p></li></ol><p><small>关于距离客户端最近的 <code>datanode</code> 的<strong>网络拓扑</strong>可以自行了解。</small></p><h4 id="剖析写入"><a href="#剖析写入" class="headerlink" title="剖析写入"></a>剖析写入</h4><p>下图展示文件是如何写入 <code>HDFS</code><br><img src="https://s2.loli.net/2023/02/14/moECnxvVA2uWPpq.png" alt="hadoop_1_5.jpg"></p><ol><li><p><code>create</code><br>客户端通过对 <code>DistributedFileSystem</code> 对象调用 <code>create()</code> 方法来新建文件。</p></li><li><p><code>create</code><br><code>DistributedFileSystem</code> 对象对 <code>namenode</code> 发起一个 <code>RPC</code> 调用，在文件系统的命名空间中新建一个空文件（还没有相应的数据块），<code>namenode</code> 在收到请求后开始执行各种不同的检查以确保该文件不存在且客户端有创建该文件的权限。接着 <code>DistributedFileSystem</code> 向客户端返回一个 <strong><code>FSDataOutputStream</code></strong> 对象，然后客户端开始写入数据。<br><code>FSDataOutputStream</code> 内部封装了一个 <code>DFSOutputStream</code> 对象，该对象负责处理 <code>datanode</code> 和 <code>namenode</code> 之间的通信。</p></li><li><p><code>write</code><br>在客户端写入数据时，<code>DFSOutputStream</code> 将它分成一个个的数据包，并写入内部队列，称为<strong>数据队列（<code>data queue</code>）</strong>。</p></li><li><p><code>write packet</code><br><code>DataStreamer</code> 处理数据队列，挑选出适合存储数据副本的一组 <code>datanode</code>，并据此来<strong>要求 <code>namenode</code> 分配新的数据块</strong>；一组 <code>datanode</code> 组成一个管线，<code>DataStreamer</code> 将数据包以流式传输到管线中第一个 <code>datanode</code>，该 <code>datanode</code> 存储数据包并将它发送到下一个管线中，以此类推知道 <code>datanode</code> 数量达到设置的复本数量。</p></li><li><p><code>ack packet</code><br><code>DFSOutputStream</code> 内部也维护着一个内部数据包队列来等待 <code>datanode</code> 的收到确认回执，称为<strong>确认队列（<code>ack queue</code>）</strong>。收到管道中所有 <code>datanode</code> 确认信息后，该数据包才会从确认队列中被删除。</p></li><li><p><code>close</code><br>客户端完成数据的写入后，对数据流调用 <code>close()</code> 方法。</p></li><li><p><code>complete</code><br>该操作将剩余的所有数据包写入 <code>datanode</code> 管线，并在联系到 <code>namenode</code> 告知其文件写入完成之前等待确认。<code>namenode</code> 此时已知写入文件有哪些块组成（<code>DataStreamer</code> 请求分配数据块），因此最终只需要等待数据块完成最小量的复制就会返回成功。</p></li></ol><h4 id="一致模型"><a href="#一致模型" class="headerlink" title="一致模型"></a>一致模型</h4><p>文件系统的<strong>一致模型（<code>coherency mode</code>）</strong>描述了<strong>文件读&#x2F;写的数据可见性</strong>。<code>HDFS</code> 为了满足性能牺牲了一些 <code>POSIX</code> 要求，具体如下：</p><ul><li>新建文件后能立即在文件系统的<strong>命名空间中可见</strong>，但是写入<strong>文件的内容并不能立即可见</strong>。</li><li>当写入的数据超过一个块后，第一个<strong>数据块对于新的 <code>reader</code> 是可见的，总之当前正在写入的块对于其他 <code>reader</code> 不可见</strong>。</li><li><code>HDFS</code> 中的 <code>FSDataOutputStream</code> 类使用 <strong><code>hflush()</code> 方法可以将所有缓存刷新到 <code>datanode</code> 中</strong>；<code>hflush()</code> 方法执行成功后，对于所有新的 <code>reader</code> 而言，<code>HDFS</code> 可以保证目前已写入到文件中的数据均到达 <code>datanode</code> 的管道且对所有的 <code>reader</code> 都是可见的。</li><li>注意 <code>hflush()</code> 方法不能保证 <code>datanode</code> 已经将数据写到磁盘上，仅确保数据保存在 <code>datanode</code> 的内存中。 <strong><code>hsync()</code> 方法可以确保将数据写入到磁盘</strong>上，类似于 <code>POSIX</code> 中的 <code>fsync()</code> 系统调用。</li></ul><p><small>关于在何时调用 <code>hflush()</code> 和 <code>hsync()</code> 需要在应用程序和性能之间做平衡，选择最合适的调用频率。</small></p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="https://blog.vgbhfive.cn/Hadoop-%E5%9F%BA%E7%A1%80/">Hadoop-基础</a><br><a href="https://www.cnblogs.com/jagel-95/p/10945317.html">Hadoop源生实用工具之distcp</a><br><a href="https://www.cnblogs.com/chengmo/archive/2010/10/17/1853344.html">Unix bash shell</a></p><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;HDFS&quot;&gt;&lt;a href=&quot;#HDFS&quot; class=&quot;headerlink&quot; title=&quot;HDFS&quot;&gt;&lt;/a&gt;&lt;code&gt;HDFS&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;当数据集的大小超过一台计算机的存储上限时，就有必要对数据进行分区然后存储到其他的计算机上。管理网络中跨多台计算机存储的文件系统被称为&lt;strong&gt;分布式文件系统（&lt;code&gt;distributed filesystem&lt;/code&gt;）&lt;/strong&gt;，该架构于网络之上，势必会引起网络编程的复杂性，因此分布式文件系统比普通磁盘文件系统更为复杂。&lt;br&gt;&lt;code&gt;Hadoop&lt;/code&gt; 自带一个称为 &lt;code&gt;HDFS&lt;/code&gt; 的分布式文件系统，也是 &lt;code&gt;Hadoop&lt;/code&gt; 的旗舰级文件系统，即 &lt;code&gt;Hadoop Distributed Filesystem&lt;/code&gt;。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop-基础</title>
    <link href="https://blog.vgbhfive.cn/Hadoop-%E5%9F%BA%E7%A1%80/"/>
    <id>https://blog.vgbhfive.cn/Hadoop-%E5%9F%BA%E7%A1%80/</id>
    <published>2023-01-15T05:42:34.000Z</published>
    <updated>2023-02-04T07:42:34.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><code>Hadoop</code> 是一个分布式计算开源框架，其提供一个分布式文件系统子项目（<code>HDFS</code>）和支持 <code>MapReduce</code> 分布式计算的软件架构。</p><p>在有了大量数据之后，那么该如何进行存储和分析这些数据呢？<code>Hadoop</code> 需要解决的问题如下：</p><ul><li>硬件故障问题。一旦使用磁盘存储数据，就会遇到磁盘故障；但是为了避免数据丢失，最常见的做法就是复制（<code>replication</code>）；系统保存数据的副本（<code>replica</code>），一旦硬件系统出现故障，就立即使用另外保存的<strong>副本</strong>。</li><li>以某种方式结合大部分数据来共同完成分析。各种分布式系统允许不同来源的数据进行分析，但其数据的正确性是无法保证的。因此 <code>MapReduce</code> 提出了一个编程模型，该模型抽象出这些硬盘读&#x2F;写问题并将其作为对一个数据集（由<strong>键值对</strong>组成）的计算。</li></ul><span id="more"></span><hr><h3 id="架构设计及用途"><a href="#架构设计及用途" class="headerlink" title="架构设计及用途"></a>架构设计及用途</h3><p><code>HDFS</code> 采用<strong>主从架构</strong>。<br><code>HDFS</code> 集群是由 <strong>一个 <code>Namenode</code></strong> 和 <strong>一定数量的 <code>Datanodes</code></strong> 组成。<code>Namenode</code> 是一个中心服务器，负责管理文件系统的名字空间（<code>namespace</code>）即元数据以及客户端对文件的访问；而 <code>Datanode</code> 一般是一个节点部署一个，负责管理所在节点上数据的存储。</p><p><img src="https://s2.loli.net/2023/02/01/agxbF1OwT7yG9Ps.png" alt="hadoop_1_1.jpg"></p><p>集群中单一 <code>Namenode</code> 的结构大大简化了系统的架构，<code>Namenode</code> 是所有 <code>HDFS</code> 元数据的<strong>仲裁者</strong>和<strong>管理者</strong>，因此用户数据永远都不会流过 <code>Namenode</code>。</p><p><code>HDFS</code> 使用 <code>Java</code> 语言开发，因此任何支持 <code>Java</code> 的机器都可以部署 <code>Namenode</code> 或者 <code>Datanode</code>。一个典型的场景就是一台机器上运行一个 <code>Namenode</code>，而集群的其他机器运行 <code>Datanode</code> 实例，另外这种架构并不排斥一台机器上部署多个 <code>Datanode</code>，只不过这种情况比较少见而已。</p><h4 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h4><p><code>HDFS</code> 被设计成能够在一个<strong>大集群中跨机器可靠地存储超大文件</strong>。<code>HDFS</code> 将每个文件存储成一系列的数据块，所有的数据块都是相同的大小（默认为 <code>128M</code>，不包含最后一个数据块）。避免数据丢失文件的所有数据块都会有副本，每个文件的数据块大小和副本系数都可配置；应用程序可以指定某个文件的副本数目，其中副本系数可以在文件创建的时候指定，也可以在之后改变。<br><small><code>HDFS</code> 中的文件都是一次性写入，并严格要求在任何时候只能有一个写入者。</small></p><p><img src="https://s2.loli.net/2023/02/01/O4qDK9esm2n6wfT.png" alt="hadoop_1_2.jpg"></p><p><code>HDFS</code> 暴露了文件系统的名字空间，用户能够以<strong>文件的形式在上面存储数据</strong>。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组 <code>Datanode</code> 上。<code>Namenode</code> 执行文件系统的名字空间操作，比如 <em>打开</em>、<em>关闭</em>、<em>重命名文件</em> 或 <em>目录</em>；同时也负责确定数据块到具体 <code>Datanode</code> 节点的映射。<code>Datanode</code> 负责处理文件系统客户端的读写请求；其在 <code>Namenode</code> 的统一调度下进行数据块的<em>创建</em>、<em>删除</em> 和 <em>复制</em>。</p><p><code>Namenode</code> 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 <code>Namenode</code> 记录下来，另外还可以设置 <code>HDFS</code> 保存的文件的副本数目。文件副本的数目称为文件的副本系数，该信息也是由 <code>Namenode</code> 保存的。</p><h4 id="数据副本复制"><a href="#数据副本复制" class="headerlink" title="数据副本复制"></a>数据副本复制</h4><p>如何<strong>存放数据块副本</strong>是 <code>HDFS</code> 可靠性和性能的关键，优化的副本存放策略是 <code>HDFS</code> 区分于其他大部分分布式文件系统的重要特性（该特性需要做大量的调优，并需要经验的积累）。<br><code>HDFS</code> 目前采用一种被称为<strong>机架感知（<code>rack-aware</code>）</strong>的策略来改进数据的 <em>可靠性</em>、<em>可用性</em> 和 <em>网络带宽的利用率</em>。目前实现的副本存放策略只是在这个方向上的第一步，实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。<br><small>而大型 <code>HDFS</code> 实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。</small></p><p>由于 <strong><code>Namenode</code> 全权管理数据块的复制</strong>，因此会<strong>周期性</strong>地从集群中的每个 <code>Datanode</code> 接收<strong>心跳信号</strong>和<strong>块状态报告（<code>Blockreport</code>）</strong>；接收到心跳信号意味着该 <code>Datanode</code> 节点工作正常，而块状态报告包含该 <code>Datanode</code> 上所有数据块的列表。<br>另外为了降低整体的带宽消耗和读取延时，<code>HDFS</code> 会尽量让读取程序读取离它最近的数据副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本；但是若一个集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。</p><p>当一个<strong>文件的副本系数被减小</strong>后，<code>Namenode</code> 会选择过剩的副本删除，在下次心跳检测时会将该信息传递给 <code>Datanode</code>，收到消息后随即移除相应的数据块，集群中的空闲空间增大。</p><p>所有的 <code>HDFS</code> 通讯协议都是建立在 <strong><code>TCP/IP</code></strong> 协议之上。客户端通过一个可配置的 <code>TCP</code> 端口连接到 <code>Namenode</code>，通过 <strong><code>ClientProtocol</code></strong> 协议与 Namenode 交互，而 <code>Datanode</code> 则是使用 <strong><code>DatanodeProtocol</code></strong> 协议与 <code>Namenode</code> 交互。一个远程过程调用（<code>RPC</code>）模型被抽象出来封装 <code>ClientProtocol</code> 和 <code>Datanodeprotocol</code> 协议，在该设计上，<code>Namenode</code> 不会主动发起 <code>RPC</code>，而是响应来自客户端或 <code>Datanode</code> 的 <code>RPC</code> 请求。</p><h4 id="健壮性"><a href="#健壮性" class="headerlink" title="健壮性"></a>健壮性</h4><p><code>HDFS</code> 的主要目标是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：</p><ul><li><code>Namenode</code> 出错。</li><li><code>Datanode</code> 出错。</li><li>网络割裂（<code>network partitions</code>）。</li></ul><p>每个 <code>Datanode</code> 节点周期性地向 <code>Namenode</code> 发送心跳信号，因此一旦出现网络割裂就会导致一部分 <code>Datanode</code> 跟 <code>Namenode</code> 失去联系，<code>Namenode</code> 若是定期没有收到心跳信号，就会<strong>将这些近期不再发送心跳信号的 <code>Datanode</code> 标记为宕机</strong>，不会再将新的读写请求发给它们。<code>Datanode</code> 宕机会导致任何存储在上的数据将不再有效，会引起一些数据块的副本系数低于指定值，然而 <code>Namenode</code> 会不断地检测这些需要复制的数据块，一旦发现副本系数不匹配就会启动复制操作。在下列情况下会启动复制操作：</p><ul><li>某个 <code>Datanode</code> 节点失效。</li><li>某个副本遭到损坏。</li><li><code>Datanode</code> 上的硬盘错误。</li><li>文件的副本系数增大。</li></ul><p>现实中从某个 <code>Datanode</code> 获取的数据块有可能是损坏的，损坏可能是由 <code>Datanode</code> 的存储设备错误、网络错误或者软件 <code>bug</code> 造成的。为此 <code>HDFS</code> 客户端软件实现了对 <code>HDFS</code> <strong>文件内容的校验</strong>和 <strong><code>checksum</code> 检查</strong>。当客户端创建一个新的 <code>HDFS</code> 文件时会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个 <code>HDFS</code> 名字空间下，当客户端获取文件内容后，它会检验从 <code>Datanode</code> 获取的数据跟相应的校验和文件中的校验和是否匹配；如果不匹配，客户端可以选择从其他 <code>Datanode</code> 获取该数据块的副本。</p><p><strong><code>FsImage</code></strong> 和 <strong><code>Editlog</code></strong> 是 <code>HDFS</code> 的核心数据结构，如果这些文件损坏那么整个 <code>HDFS</code> 实例都将失效。因此 <code>Namenode</code> 可以配置成支持<strong>多个 <code>FsImage</code> 和 <code>Editlog</code> 的副本</strong>，任何对核心数据结构的修改都将同步到它们的所有副本上。尽管这种多副本的同步操作可能会降低 <code>Namenode</code> 每秒处理的名字空间事务数量，但是代价依旧是可以接受的，因为即使 <code>HDFS</code> 的应用是数据密集的，但是并非元数据密集的，因此当 <code>Namenode</code> 重启的时候会选取最近的完整的 <code>FsImage</code> 和 <code>Editlog</code> 来使用。</p><p><code>Namenode</code> 是 <code>HDFS</code> 集群中的<strong>单点故障（<code>single point of failure</code>）</strong>所在，因此如果 <code>Namenode</code> 机器故障，是需要手工干预的。</p><hr><h3 id="搭建集群"><a href="#搭建集群" class="headerlink" title="搭建集群"></a>搭建集群</h3><p>在大多数情况下，副本系数是 <code>3</code>，<code>HDFS</code> 的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。<br>这种策略减少了机架间的数据传输，提高了写操作的效率，同时由于机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。与此同时数据块因为放在两个（不是三个）不同的机架上，所以此策略可以减少读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上，三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写数据的性能。</p><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><ol><li><p>三台机器<br>三台机器包含一台 <code>Namenode</code> 机器和两台 <code>Datanode</code> 机器，机器都拥有自己的内网 <code>IP</code>。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">namenode    10.250.0.1</span><br><span class="line">datanode1   10.250.0.2</span><br><span class="line">datanode2   10.250.0.3</span><br></pre></td></tr></table></figure></li><li><p>全部机器创建相同的用户名和组</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">groupadd hadoop</span><br><span class="line">useradd hadoop -g hadoop</span><br><span class="line">passwd hadoop</span><br><span class="line"><span class="built_in">mkdir</span> /home/hadoop</span><br><span class="line"><span class="built_in">chown</span> -R hadoop:hadoop /home/hadoop</span><br></pre></td></tr></table></figure></li><li><p><code>Java</code> 环境</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_192&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_192-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.192-b12, mixed mode)</span><br></pre></td></tr></table></figure></li></ol><h4 id="下载及配置"><a href="#下载及配置" class="headerlink" title="下载及配置"></a>下载及配置</h4><ol><li><p>下载 <code>jar</code> 包<br>下载这部分就靠自己去网上寻找了，这里就不进行说明。</p></li><li><p>修改默认配置<br>进入到 <code>conf</code> 文件下。</p></li><li><p><code>hadoop.sh</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/etc/java-config-2/current-system-vm</span><br></pre></td></tr></table></figure></li><li><p><code>hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hadoop/conan/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://10.250.0.1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10.250.0.1:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>masters</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.250.0.1</span><br></pre></td></tr></table></figure></li><li><p><code>slaves</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.250.0.2</span><br><span class="line">10.250.0.3</span><br></pre></td></tr></table></figure></li></ol><h4 id="同步配置"><a href="#同步配置" class="headerlink" title="同步配置"></a>同步配置</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入到 hadoop 解压的目录下</span></span><br><span class="line">scp -r ./hadoop hadoop@10.250.0.2:/hadoop/conan</span><br><span class="line">scp -r ./hadoop hadoop@10.250.0.3:/hadoop/conan</span><br></pre></td></tr></table></figure><h4 id="启动-Namenode-节点"><a href="#启动-Namenode-节点" class="headerlink" title="启动 Namenode 节点"></a>启动 <code>Namenode</code> 节点</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入到 bin 目录下</span></span><br><span class="line">bin/hadoop namenode -format</span><br><span class="line">bin/start-all.sh</span><br></pre></td></tr></table></figure><h4 id="检查是否成功"><a href="#检查是否成功" class="headerlink" title="检查是否成功"></a>检查是否成功</h4><ol><li><p><code>jps</code></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">9362 Jps</span><br><span class="line">7756 SecondaryNameNode</span><br><span class="line">7531 JobTracker</span><br><span class="line">7357 NameNode</span><br></pre></td></tr></table></figure></li><li><p><code>netstat -nl</code></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State      </span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     </span><br><span class="line">tcp        0      0 0.0.0.0:5666            0.0.0.0:*               LISTEN     </span><br><span class="line">tcp        0      0 0.0.0.0:8649            0.0.0.0:*               LISTEN     </span><br><span class="line">tcp6       0      0 :::50070                :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 :::39418                :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 :::32895                :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 10.250.0.1:9000         :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 10.250.0.1:9001         :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 :::50090                :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 :::51595                :::*                    LISTEN     </span><br><span class="line">tcp6       0      0 :::50030                :::*                    LISTEN     </span><br><span class="line">udp        0      0 127.0.0.1:8649          0.0.0.0:*  </span><br></pre></td></tr></table></figure></li><li><p><code>hadoop</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入到 bin 目录下</span></span><br><span class="line">bin/hadoop fs -<span class="built_in">mkdir</span> /test</span><br><span class="line">bin/hadoop fs -copyFormLocal README.txt /test</span><br><span class="line">bin/hadoop fs -<span class="built_in">ls</span> /test</span><br><span class="line"><span class="comment"># Found 1 items</span></span><br><span class="line"><span class="comment"># -rw-r--r--   2 hadoop supergroup       1006 2022-02-01 12:05 /test/README.txt</span></span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><h4 id="Web-接口"><a href="#Web-接口" class="headerlink" title="Web 接口"></a><code>Web</code> 接口</h4><p><code>NameNode</code> 和 <code>DataNode</code> 各自启动了一个<strong>内置的 <code>Web</code> 服务器</strong>，显示了集群当前的<strong>基本状态</strong>和<strong>信息</strong>。<br>在默认配置下 <code>NameNode</code> 的首页地址是 <code>http://namenode-name:50070/</code>，这个页面列出了集群里的所有 <code>DataNode</code> 和集群的基本状态，同时该 <code>Web</code> 接口也可以用来浏览整个文件系统（使用 <code>Namenode</code> 首页的 <code>Browse the file system</code> 链接）。</p><h4 id="Shell-命令"><a href="#Shell-命令" class="headerlink" title="Shell 命令"></a><code>Shell</code> 命令</h4><p><code>Hadoop</code> 包括一系列的类 <code>sh</code> 的命令，这些命令可直接和 <code>HDFS</code> 以及其他 <code>Hadoop</code> 支持的文件系统进行交互，支持大多数普通文件系统的操作，比如复制文件、改变文件权限等。另外还支持一些 <code>HDFS</code> 特有的操作，比如改变文件副本数目。</p><p>所有的 <code>Hadoop</code> 命令均由 <code>bin/hadoop</code> 脚本引发，若不指定参数运行 <code>Hadoop</code> 脚本会打印所有命令的描述。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop [--config confdir] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS]</span><br></pre></td></tr></table></figure><h5 id="DFSSh"><a href="#DFSSh" class="headerlink" title="DFSSh"></a><code>DFSSh</code></h5><p>运行一个常规的文件系统客户端，<code>HDFS</code> 以文件和目录的形式组织用户数据，<strong>提供了一个命令行接口 <code>DFSSh</code> 让用户与 <code>HDFS</code> 中的数据进行交互</strong>。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs [GENERIC_OPTIONS] [COMMAND_OPTIONS]</span><br></pre></td></tr></table></figure><p><small><code>bin/hadoop fs -help</code> 命令列出所有 <code>Hadoop Sh</code> 支持的命令。<code>bin/hadoop fs -help command-name</code> 命令能显示关于某个命令的详细信息。</small></p><ol><li><p><code>cat</code><br>将路径指定文件的内容输出到 <code>stdout</code>。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">cat</span> URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>chgrp</code><br>改变文件所属的组。使用 <code>-R</code> 将使改变在目录结构下递归进行，并且命令的使用者必须是文件的所有者或者超级用户。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">chgrp</span> [-R] GROUP URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>chmod</code><br>改变文件的权限。使用 <code>-R</code> 将使改变在目录结构下递归进行，并且命令的使用者必须是文件的所有者或者超级用户。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>chown</code><br>改变文件的拥有者。使用 <code>-R</code> 将使改变在目录结构下递归进行，并且命令的使用者必须是超级用户。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure></li><li><p><code>ls</code><br>展示文件信息。如果参数是文件则展示文件信息；如果是目录则展示子目录的列表。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">ls</span> &lt;args&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>lsr</code><br><code>ls</code> 命令的递归版本，类似于 <code>-R</code>。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -lsr &lt;args&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>du</code><br>显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">du</span> URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>dus</code><br>显示文件的大小。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -dus &lt;args&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>mkdir</code><br>接受路径指定的作为参数，然后创建这些目录。其行为类似于 <code>mkdir -p</code> 会自动创建路径中的各级父目录。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mkdir</span> &lt;paths&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>cp</code><br>将文件从源路径复制到目标路径。这个命令允许有多个源路径，但是目标路径必须是一个目录。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">cp</span> URI [URI …] &lt;dest&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>copyFromLocal</code><br>除了限定源路径是一个本地文件外，其余参数和 <code>put</code> 命令相似。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal &lt;localsrc&gt; URI</span><br></pre></td></tr></table></figure></li><li><p><code>copyToLocal</code><br>除了限定目标路径是一个本地文件外，其余参数和 <code>get</code> 命令类似。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>mv</code><br>将文件从源路径移动到目标路径。这个命令允许有多个源路径，但是目标路径必须是一个目录，另外不支持在不同的文件系统间移动文件。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mv</span> URI [URI …] &lt;dest&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>movefromLocal</code><br>将本地文件上传到 <code>HDFS</code>，之后本地文件会被删除（可以理解为剪切）。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fs -moveFromLocal &lt;src&gt; &lt;dst&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>get</code><br>复制文件到本地系统。可用 <code>-ignorecrc</code> 选项复制 <code>CRC</code> 校验失败的文件；使用 <code>-crc</code> 选项复制文件以及 <code>CRC</code> 信息。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localdst&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>getmerge</code><br>接受一个源目录和一个目标文件作为输入，然后将源目录中的文件合并到本地文件中。<code>addnl</code> 选项是可选的，用于指定在每个文件结尾添加一个换行符。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]</span><br></pre></td></tr></table></figure></li><li><p><code>put</code><br>从本地文件系统中复制单个或多个源路径到目标文件系统，另外也支持从标准输入中读取输入写入目标文件系统。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put &lt;localsrc&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure><p><small><code>localsrc</code> 为 <code>-</code> 表示从标准输入中读取输入。</small></p></li><li><p><code>rm</code><br>删除指定的文件。只删除非空目录和文件。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">rm</span> URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>rmr</code><br><code>rm</code> 的递归版本。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rmr URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>touchz</code><br>创建一个 <code>0</code> 字节的空文件。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -touchz URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>setrep</code><br>改变一个文件的副本系数。<code>-R</code> 选项用于递归改变目录下所有文件的副本系数。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -setrep [-R] &lt;path&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>stat</code><br>返回指定路径的统计信息。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">stat</span> URI [URI …]</span><br></pre></td></tr></table></figure></li><li><p><code>tail</code><br>将文件尾部 <code>1K</code> 字节的内容输出到 <code>stdout</code>。支持 <code>-f</code> 选项。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">tail</span> [-f] URI</span><br></pre></td></tr></table></figure></li><li><p><code>test</code><br>检查文件。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">test</span> -[ezd] URI</span><br></pre></td></tr></table></figure><p>其中 <code>-[ezd]</code> 选项分别代表：</p><ul><li><code>-e</code>，检查文件是否存在。如果存在则返回 <code>0</code>。</li><li><code>-z</code>，检查文件是否是 <code>0</code> 字节。如果是则返回 <code>0</code>。</li><li><code>-d</code>，如果路径是个目录，则返回 <code>1</code>，否则返回 <code>0</code>。</li></ul></li><li><p><code>text</code><br>将源文件输出为文本格式。允许的格式是 <code>zip</code> 和 <code>TextRecordInputStream</code>。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -text &lt;src&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>expunge</code><br>清空回收站。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -expunge</span><br></pre></td></tr></table></figure></li></ol><h5 id="DFSAdmin"><a href="#DFSAdmin" class="headerlink" title="DFSAdmin"></a><code>DFSAdmin</code></h5><p>运行一个 <code>HDFS</code> 的 <code>dfsadmin</code> 客户端。<code>DFSAdmin</code> 命令用来<strong>管理 <code>HDFS</code> 集群</strong>，这些命令只有管理员才能使用。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin [GENERIC_OPTIONS] [-report] [-safemode enter | leave | get | <span class="built_in">wait</span>] [-refreshNodes] [-finalizeUpgrade] [-upgradeProgress status | details | force] [-metasave filename] [-setQuota &lt;quota&gt; &lt;<span class="built_in">dirname</span>&gt;...&lt;<span class="built_in">dirname</span>&gt;] [-clrQuota &lt;<span class="built_in">dirname</span>&gt;...&lt;<span class="built_in">dirname</span>&gt;] [-<span class="built_in">help</span> [cmd]]</span><br></pre></td></tr></table></figure><p><small><code>bin/hadoop dfsadmin</code> 命令支持一些 <code>HDFS</code> 管理相关的操作。<code>bin/hadoop dfsadmin -help</code> 命令能列出所有当前支持的命令。</small></p><ol><li><p><code>report</code><br>报告 HDFS 的基本统计信息，当然有些信息也可以在 <code>NameNode Web</code> 服务首页看到。</p></li><li><p><code>safemode enter | leave | get | wait</code><br>虽然通常并不需要，但是管理员可以手动让 <code>NameNode</code> 进入或离开安全模式。</p></li><li><p><code>refreshNodes</code><br>重新读取 <code>hosts</code> 和 <code>exclude</code> 文件，更新允许连接到 <code>Namenode</code> 但是退出或加入的 <code>Datanode</code> 的集合。</p></li><li><p><code>finalizeUpgrade</code><br>删除上一次升级时制作的集群备份。终结 <code>HDFS</code> 的升级操作，<code>Datanode</code> 删除前一个版本的工作目录并且之后 <code>Namenode</code> 也这样做，这个操作会结束整个升级过程。</p></li><li><p><code>upgradeProgress status | details | force</code><br>请求当前系统的升级状态，状态的细节，或者强制升级操作进行。</p></li><li><p><code>metasave filename</code><br>保存 <code>Namenode</code> 的主要数据结构到 <code>hadoop.log.dir</code> 属性指定的目录下的文件中。</p></li><li><p><code>setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;</code><br>为每个目录设定配额，目录配额应该是一个长整型整数，强制限定了目录树下的名字个数。</p></li><li><p><code>clrQuota &lt;dirname&gt;...&lt;dirname&gt;</code><br>清除每一个目录的配额设定。</p></li></ol><h5 id="fsck"><a href="#fsck" class="headerlink" title="fsck"></a><code>fsck</code></h5><p><code>fsck</code> 命令来<strong>检查系统中的各种不一致状况</strong>，运行 <code>HDFS</code> 文件系统检查工具。这个命令被设计来报告各种文件存在的问题，比如文件缺少数据块或者副本数目不够，另外不同于在本地文件系统上传统的 <code>fsck</code> 工具，这个命令并不会修正它检测到的错误。一般来说 <code>NameNode</code> 会自动修正大多数可恢复的错误。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fsck [GENERIC_OPTIONS] &lt;path&gt; [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]</span><br></pre></td></tr></table></figure><ol><li><p><code>&lt;path&gt;</code><br>检查的起始目录。</p></li><li><p><code>move</code><br>移动受损文件到 <code>/lost+found</code>。</p></li><li><p><code>delete</code><br>删除受损文件。</p></li><li><p><code>openforwrite</code><br>打印出写打开的文件。</p></li><li><p><code>files</code><br>打印出正被检查的文件。</p></li><li><p><code>blocks</code><br>打印出块信息报告。</p></li><li><p><code>locations</code><br>打印出每个块的位置信息。</p></li><li><p><code>racks</code><br>打印出 <code>data-node</code> 的网络拓扑结构。</p></li></ol><h5 id="jar"><a href="#jar" class="headerlink" title="jar"></a><code>jar</code></h5><p><strong>运行 <code>jar</code> 文件</strong>。可以将 <code>Map Reduce</code> 代码写到 <code>jar</code> 文件中，然后使用这个命令执行。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar &lt;jar&gt; [mainClass] args...</span><br></pre></td></tr></table></figure><h5 id="job"><a href="#job" class="headerlink" title="job"></a><code>job</code></h5><p><strong>用于和 <code>Map Reduce</code> 作业交互和命令</strong>。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop job [GENERIC_OPTIONS] [-submit &lt;job-file&gt;] | [-status &lt;job-id&gt;] | [-counter &lt;job-id&gt; &lt;group-name&gt; &lt;counter-name&gt;] | [-<span class="built_in">kill</span> &lt;job-id&gt;] | [-events &lt;job-id&gt; &lt;from-event-<span class="comment">#&gt; &lt;#-of-events&gt;] | [-history [all] &lt;jobOutputDir&gt;] | [-list [all]] | [-kill-task &lt;task-id&gt;] | [-fail-task &lt;task-id&gt;]</span></span><br></pre></td></tr></table></figure><ol><li><p><code>submit</code><br>提交作业。</p></li><li><p><code>status</code><br>打印 <code>map</code> 和 <code>reduce</code> 完成百分比和所有计数器。</p></li><li><p><code>counter</code><br>打印计数器的值。</p></li><li><p><code>kill</code><br>杀死指定作业。</p></li><li><p><code>events</code><br>打印给定范围内 <code>jobtracker</code> 接收到的事件细节。</p></li><li><p><code>history</code><br>打印作业的细节、失败及被杀死原因的细节。更多的关于一个作业的细节比如成功的任务，做过的任务尝试等信息可以通过指定 <code>[all]</code> 选项查看。</p></li><li><p><code>list</code><br>显示所有作业。<code>-list</code> 只显示将要完成的作业。</p></li><li><p><code>kill-task</code><br>杀死任务。被杀死的任务不会不利于失败尝试。</p></li><li><p><code>fail-task</code><br>使任务失败。被失败的任务会对失败尝试不利。</p></li></ol><h5 id="pipes"><a href="#pipes" class="headerlink" title="pipes"></a><code>pipes</code></h5><p><strong>运行 <code>pipes</code> 作业</strong>。用法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop pipes [-conf &lt;path&gt;] [-jobconf &lt;key=value&gt;, &lt;key=value&gt;, ...] [-input &lt;path&gt;] [-output &lt;path&gt;] [-jar &lt;jar file&gt;] [-inputformat &lt;class&gt;] [-map &lt;class&gt;] [-partitioner &lt;class&gt;] [-reduce &lt;class&gt;] [-writer &lt;class&gt;] [-program &lt;executable&gt;] [-reduces &lt;num&gt;]</span><br></pre></td></tr></table></figure><ol><li><p><code>conf</code><br>作业的配置。</p></li><li><p><code>jobconf</code><br>增加&#x2F;覆盖作业的配置项。</p></li><li><p><code>input</code><br>输入目录。</p></li><li><p><code>output</code><br>输出目录。</p></li><li><p><code>jar</code><br><code>Jar</code> 文件名。</p></li><li><p><code>inputformat</code><br><code>InputFormat</code> 类。</p></li><li><p><code>map</code><br><code>Java Map</code> 类。</p></li><li><p><code>partitioner</code><br><code>Java Partitioner</code></p></li><li><p><code>reduce</code><br><code>Java Reduce</code> 类。</p></li><li><p><code>writer</code><br><code>Java RecordWriter</code></p></li><li><p><code>program</code><br>可执行程序的 <code>URI</code>。</p></li><li><p><code>reduces</code><br><code>reduce</code> 个数。</p></li></ol><h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p><code>NameNode</code> 启动时会从 <code>fsimage</code> 和 <code>edits</code> 日志文件中<strong>装载文件系统的状态信息</strong>，然后等待各个 <code>DataNode</code> 向它<strong>报告各自的数据块状态</strong>，这样 <code>NameNode</code> 在副本充足的情况下就不会过早地开始复制数据块。<br>在开始时这个阶段 <code>NameNode</code> 处于<strong>安全模式</strong>，其本质上是 <code>HDFS</code> <strong>集群的一种只读模式</strong>，此时集群不允许任何对文件系统或者数据块修改的操作，通常 <code>NameNode</code> 在开始阶段完成后会自动地退出安全模式。但若是需要可以通过 <strong><code>bin/hadoop dfsadmin -safemode</code></strong> 命令显式地将 <code>HDFS</code> 置于安全模式，另外 <code>NameNode</code> 首页也会显示当前是否处于安全模式。</p><hr><h3 id="常见组件介绍"><a href="#常见组件介绍" class="headerlink" title="常见组件介绍"></a>常见组件介绍</h3><h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a><code>Hive</code></h4><p>是基于 <code>Hadoop</code> 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，通过类 <code>SQL</code> 语句快速实现简单的 <code>MapReduce</code> 统计，不必开发专门的 <code>MapReduce</code> 应用，并且十分适合数据仓库的统计分析。</p><h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a><code>Spark</code></h4><p>是专为大规模数据处理而设计的快速通用计算引擎。</p><h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a><code>HBase</code></h4><p>是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用 <code>HBase</code> 技术可在廉价 <code>PC Server</code> 上搭建起大规模结构化存储集群。</p><h4 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a><code>Pig</code></h4><p>是一个基于 <code>Hadoop</code> 的大规模数据分析工具，它提供的 <code>SQL-LIKE</code> 语言叫 <code>Pig Latin</code>，该语言的编译器会把类 <code>SQL</code> 的数据分析请求转换为一系列经过优化处理的 <code>MapReduce</code> 运算。</p><h4 id="Sqoop-和-DataX"><a href="#Sqoop-和-DataX" class="headerlink" title="Sqoop 和 DataX"></a><code>Sqoop</code> 和 <code>DataX</code></h4><p><code>Sqoop</code> 是一个用来将 <code>Hadoop</code> 和关系型数据库中的数据相互转移的工具。它可以将一个关系型数据库中的数据导进到 <code>Hadoop</code> 的 <code>HDFS</code> 中，也可以将 <code>HDFS</code> 的数据导进到关系型数据库中。<br><code>DataX</code> 是阿里巴巴开源的离线数据同步工具，支持各种异构数据源之间高效的数据同步。</p><h4 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a><code>Hue</code></h4><p>是一个基于 <code>WEB</code> 的监控和管理系统，实现对 <code>HDFS</code>、<code>MapReduce/YARN</code>、<code>HBase</code>、<code>Hive</code>、<code>Pig</code> 的操作和管理。</p><h4 id="Ambari"><a href="#Ambari" class="headerlink" title="Ambari"></a><code>Ambari</code></h4><p>是一种基于 <code>Web</code> 的工具，支持 <code>Hadoop</code> 集群的供应、管理和监控。</p><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Hadoop&lt;/code&gt; 是一个分布式计算开源框架，其提供一个分布式文件系统子项目（&lt;code&gt;HDFS&lt;/code&gt;）和支持 &lt;code&gt;MapReduce&lt;/code&gt; 分布式计算的软件架构。&lt;/p&gt;
&lt;p&gt;在有了大量数据之后，那么该如何进行存储和分析这些数据呢？&lt;code&gt;Hadoop&lt;/code&gt; 需要解决的问题如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;硬件故障问题。一旦使用磁盘存储数据，就会遇到磁盘故障；但是为了避免数据丢失，最常见的做法就是复制（&lt;code&gt;replication&lt;/code&gt;）；系统保存数据的副本（&lt;code&gt;replica&lt;/code&gt;），一旦硬件系统出现故障，就立即使用另外保存的&lt;strong&gt;副本&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;以某种方式结合大部分数据来共同完成分析。各种分布式系统允许不同来源的数据进行分析，但其数据的正确性是无法保证的。因此 &lt;code&gt;MapReduce&lt;/code&gt; 提出了一个编程模型，该模型抽象出这些硬盘读&amp;#x2F;写问题并将其作为对一个数据集（由&lt;strong&gt;键值对&lt;/strong&gt;组成）的计算。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Hadoop" scheme="https://blog.vgbhfive.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>智能风控-风控模型体系</title>
    <link href="https://blog.vgbhfive.cn/%E6%99%BA%E8%83%BD%E9%A3%8E%E6%8E%A7-%E9%A3%8E%E6%8E%A7%E6%A8%A1%E5%9E%8B%E4%BD%93%E7%B3%BB/"/>
    <id>https://blog.vgbhfive.cn/%E6%99%BA%E8%83%BD%E9%A3%8E%E6%8E%A7-%E9%A3%8E%E6%8E%A7%E6%A8%A1%E5%9E%8B%E4%BD%93%E7%B3%BB/</id>
    <published>2022-11-25T14:00:04.000Z</published>
    <updated>2023-01-02T11:26:22.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><strong>风控模型</strong>是风控系统的核心，应用模型进行风险决策是识别风险的主要途径，也是控制风险的重要方法。</p><span id="more"></span><p>在信贷风控领域中 <strong>模型</strong>主要是指预测风险的方法，通常以数学公式或行数的方式存在。<br>这其中<strong>模型</strong>和<strong>算法</strong>不可混为一谈，算法通常是指各种数学、统计或人工智能方法，而模型是指基于这些方法得到的具体实例；因此才会出现使用了某种算法构建多个风险模型。假设两者相比较的话，可以将算法比作<strong>类</strong>，而模型则是 <strong>实例对象</strong>。</p><p>构建风控模型并不是 <em>必须</em> 使用机器学习算法，如策略人员基于人工经验和统计分析，利用 <em>评分卡模型</em> 来汇总各类风险指标，赋予各类风险指标一个分数之后，最后汇总即可。但这种方法也存在弊端，即无法处理更多维度的数据，也很难处理不同维度数据之间的关联信息，评估准确性较低、局限性很大。因此随着技术的发展，机器学习方法逐渐成为主要的建模方法。</p><p><strong>机器学习</strong>是一种从历史数据中学习潜在规律，同时预测未来行为的方法。其核心的三要素：<strong>数据</strong>、<strong>模型</strong>、<strong>算法</strong>。其中数据和算法是搭建机器学习模型的 <em>必要条件</em>，每种算法都包含多个待定的参数或结构，模型是算法在数据上运算得到特定的参数或结构的 <em>结果</em>。<br><img src="https://s2.loli.net/2022/12/01/fRUOPIwSEZQNMJp.png" alt="risk_3_1.jpg"><br>其中根据数据集中是否已知样本标签，机器学习任务又可以分为：<strong>有监督学习</strong>是指从有标签的训练数据中学习；<strong>无监督学习</strong>是指从无标签的训练数据中学习。<br><small>有标签的训练数据是指每个训练样本都包含输入和期望的输出；相反无标签则是指每个训练样本只包含输入不包含输出。</small></p><p>采用多种机器学习算法搭建风控模型的主要步骤都是相同的，主要包含以下几个步骤：</p><ul><li>问题定义</li><li>样本的选择与划分</li><li>模型架构设计</li><li>数据准备与描述分析</li><li>数据清洗</li><li>特征选择</li><li>模型训练与效果评估</li><li>部署上线</li><li>模型监控与异常处理</li><li>模型调优</li></ul><p><img src="https://s2.loli.net/2022/12/01/k7Jb3ACYBSRyLFK.png" alt="risk_3_2.jpg"></p><hr><h3 id="开发方法论"><a href="#开发方法论" class="headerlink" title="开发方法论"></a>开发方法论</h3><p>开发好样本是开发好模型的基础。构建好样本是指从项目需求中<strong>定义问题</strong>、<strong>定义标签</strong>、<strong>选择合适的建模数据集</strong>以及<strong>分析和预处理数据</strong>的过程。<br>构建好模型是指在经过预处理的数据中进行<strong>特征选择</strong>、<strong>特征提取</strong>、<strong>模型训练</strong>、<strong>分数转化</strong>和<strong>效果评估</strong>。</p><hr><h3 id="开发方法论-立项分析"><a href="#开发方法论-立项分析" class="headerlink" title="开发方法论-立项分析"></a>开发方法论-立项分析</h3><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>问题定义旨在明确项目的背景和目标，根据背景和目标将业务问题转化为机器学习建模问题，包括定义预测目标、设计模型方案等。<br>在实际业务中开始分析目标时需要先了解<strong>预测对象粒度</strong>、<strong>标签定义</strong>和<strong>细分客群</strong>。</p><ol><li><p>预测对象粒度<br>在实际业务中会遇到不同层次的问题，基于不同层次的问题，需要将预测对象定义为不同的粒度。因此在实际业务中，需要根据业务模式和模型应用策略来选择合适的粒度进行建模。</p><ul><li><strong>渠道粒度</strong>，某些场景下借款申请人来自于同一个渠道，即一个渠道一条记录。</li><li><strong>客户粒度</strong>，借款人存在多笔借款，从借款人角度考虑风险即任何一笔借款出现逾期都表示风险事件，即一个人是一条记录。</li><li><strong>借款粒度</strong>，借款人的每次申请借款考虑风险，该笔借款出现逾期则表现出风险事件，即一次借款就是一条记录。</li><li><strong>还款粒度</strong>，每个借款人得到每一次借款可能都有不同的还款期限，预测每笔还款是否出现风险，即一次还款就是一条记录。</li></ul></li><li><p>标签定义<br>风控模型用来预测未来的风险是典型的<strong>有监督学习模式</strong>，因此需要定义<strong>样本标签</strong>。<br> <strong>标签</strong>是模型所要预测的结果，可以是二分类结果，例如“好”&#x2F;“坏”、“响应”&#x2F;“不响应”等；也可以是连续变量，例如收益、损失等。风险评估模型通常用来预测未来的表现是好是坏，其中标签定义需要明确的是在什么时间点预测未来多久发生的什么事件。<br>观测点前后分别是<strong>观察窗口</strong>和<strong>表现窗口</strong>。</p><ul><li><strong>观察窗口</strong>，用来观察客户行为的时间区间。观察窗口也称为观察期。</li><li><strong>表现窗口</strong>，用来考察客户的表现，从而确定标签定义的时间区间。表现窗口也称为表现期。</li></ul><p> 在风险标签定义中，对于如何确定好坏程度和表现窗口的长度，需要结合<strong>滚动率分析（<code>roll rate analysis</code>）</strong>和<strong>账龄分析（<code>vintage analysis</code>）</strong>。</p><ul><li><strong>滚动率分析</strong>，通过滚动率分析来确定客户的“好坏”程序。滚动率是指客户从某个观测点之前的一段时间的逾期状态向观测点之后的一段时间的逾期状态转化的比例。</li><li><strong>账龄分析</strong>，信贷行业经常使用 <code>Vintage</code> 曲线分析账户的成熟期、变化规律等。<code>Vintage</code> 曲线是根据账龄绘制的不同时间放款样本的逾期率变化曲线。逾期率有金额逾期和账单逾期两种口径。</li></ul><p> 综上所述，滚动率分析用于分析客户的“好坏”程度；账龄分析用于确定合适的表现期，可以尽可能多地覆盖“坏”客户。</p></li><li><p>细分客群<br>在建模任务中如果客群差异较大则需要进一步<strong>细分客群</strong>。根据不同产品拆分客群时，不同的<strong>进件渠道</strong>、<strong>借款期数</strong>、<strong>区域</strong>和<strong>借款金额</strong>等划分客群。除此之外还可以采用聚类等无监督学习方法划分客群。<br>至此细分客群建模还需要满足下列条件：</p><ul><li>细分客群之间的风险水平差异较大。</li><li>细分客群可以获得的特征维度不同。</li><li>每个细分客群的样本足够多。</li></ul><p> 通过细分客群建模可以使模型更加专注于细分客群风险模式的学习，从而提高模型效果。不过细分客群建模也存在弊端：细分客群建模会导致模型数量增多，需要投入的资源和时间增多，维护成本增加；细分客群将总样本分散到各个客群中，相比总样本量细分客群样本量减少，特别是细分客群的<strong>“坏”</strong>样本量不多，反而会降低模型的预测能力。</p></li></ol><h4 id="样本的选择与划分"><a href="#样本的选择与划分" class="headerlink" title="样本的选择与划分"></a>样本的选择与划分</h4><ol><li><p>样本选择<br>样本选择是指从业务数据选择部分合适的样本进行模型开发。<br>风控模型是一种预测模型，保证模型良好的预测效果的前提是客户未来的行为和过去相似，因此才可以从过去的数据中学习规律并预测未来的表现。其中选取的建模样本需要把握<strong>建模样本必须能够代表总体，与未来模型使用场景下的样本差异尽可能小</strong>，具体体现为以下四点：<strong>代表性</strong>；<strong>充分性</strong>；<strong>时效性</strong>；<strong>排除性</strong>。</p></li><li><p>样本集划分<br> <strong>数据是模型搭建的基础</strong>。<br>在模型开发过程中会将一部分数据划分用于训练模型，另一部分数据划分为验证模型效果。总体数据可以划分为<strong>训练集</strong>、<strong>验证集</strong>和<strong>测试集</strong>，其中训练集用于训练模型，验证集用来模型调参、训练过程中的参数选择或者模型选择，测试集用来验证模型最终表现（通常选用靠近当前时间的样本作为测试集，也称 <code>OOT（Out of Time sample）</code> 样本）。典型的训练样本、验证样本、<code>OOT</code> 样本划分比例是 <code>7:2:1</code>。<br><small>在特殊并且样本较少时，为了让更好的样本参与模型训练，可以将验证样本取消，保留训练样本和 <code>OOT</code> 样本，训练时采用交叉验证的方式进行模型参数选择。另外避免出现异常，可以先 <em>空跑</em> 一段时间观察模型情况。</small></p></li></ol><h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><p>在明确问题定义、确定样本和样本集划分之后，模型搭建的基本任务已经清晰，关于如何更好的预测目标，需要首先从宏观上考虑模型的架构。<br>从模型数据源维度考虑，模型架构可以分为：</p><ul><li>单一模型架构<br> 不区分数据源，将所有数据源特征放在一起进行建模，输出最终模型。</li><li>多子模型融合架构<br>将不同维度的数据源划分为若干个集合，先建立子模型，再将子模型进行二次融合，生成最终模型。<br><img src="https://s2.loli.net/2022/12/22/vb27ckLozjX4q6Z.png" alt="risk_3_6.jpg"></li></ul><p>模型架构除了可以从数据源维度进行划分之外，还可以：</p><ul><li>从目标逾期标签定义或表现期长短的角度，分别建立 <code>DPD10</code> 逾期模型、<code>DPD60</code> 逾期模型，长表现期子模型和短表现期子模型等。</li><li>结合客群细分，建立基于不同细分客群的子模型，再进行二次融合。</li><li>采用不同算法建立不同子模型，再进行二次融合。</li></ul><p>在实际业务条件下，面对的可变条件太多、数据量的大小、模型调参方法的差异和特征维度的差异都会导致模型结果出现偏差，因此在同等条件下可以采用<strong>如无必要，勿增实体</strong>的原则，即采用最简单的方案。</p><hr><h3 id="开发方法论-训练开发"><a href="#开发方法论-训练开发" class="headerlink" title="开发方法论-训练开发"></a>开发方法论-训练开发</h3><h4 id="数据的准备和描述"><a href="#数据的准备和描述" class="headerlink" title="数据的准备和描述"></a>数据的准备和描述</h4><ol><li><p>数据准备<br>数据准备是将构造完整的建模数据集，数据集的每一列为一个特征。风控模型中的特征是根据预测目标的粒度，基于底层的原始数据，通过汇总等方式加工而成的。而由于底层数据的不同，特征一般会分为不同的模块，每个特征模块包含若干个特征。</p><p> 在数据准备阶段将可用的特征模块逐一按照样本选择的范围和每个样本观测点计算出对应的特征。通常将事后计算以前某个时间点的特征的行为称为<strong>回溯</strong>。特别的需要确保特征数据是观测点时刻可以获取的当时状态，这样才能保证模型在应用时才能获取到相同的特征。当原始数据已经被修改，无法追溯到当时的特征时，特征就不能 <em>回溯</em>，因此也就无法使用此特征。</p><p> 特征无法回溯而造成特征值中包含观测点之后的信息，这被称为<strong>特征穿越</strong>或<strong>信息泄露</strong>。这种问题通常导致的后果就是特征效果和模型效果异常好，当真实场景使用时并不能得到相同的效果。因此<strong>特征穿越</strong>问题需要尽可能在数据准备阶段尽力排除，排除此问题共有以下三种方法：</p><ul><li>回溯数据与线上实时计算数据的一致性检查。</li><li>单个变量与预测标签的效果指标分析。</li><li>单个样本特征计算逻辑分析。</li></ul></li><li><p>数据描述<br>数据描述即<strong>探索性数据分析（<code>Exploratory Data Analysis, EDA</code>）</strong>是指对特征进行统计分析，统计每个特征的<strong>缺失率</strong>、<strong>唯一值个数</strong>、<strong>最大值</strong>、<strong>最小值</strong>、<strong>平均值</strong>和<strong>趋势性变化</strong>等指标，使模型开发人员对数据集有清晰、细致的了解。<br>数据描述的目的在于<strong>了解特征分布，确认数据质量</strong>，在得到所有特征的统计指标后，需要首先确认数据质量，分析每个指标是否合理，而非直接进行数据清洗。</p><p> 数据问题通常包含两类：</p><ul><li>由于非正常因素导致的异常，如系统故障导致的数据缺失。</li><li>业务调整导致的异常，业务调整对某些特征是否影响的，会造成特征分布偏移。</li></ul></li></ol><h4 id="数据预处理（清洗）"><a href="#数据预处理（清洗）" class="headerlink" title="数据预处理（清洗）"></a>数据预处理（清洗）</h4><p>在进行特征选择和构建模型之前需要对数据进行预处理，使得数据能够全面反映全体样本信息，以适用于机器学习模型。<br>数据预处理包含<strong>异常值处理</strong>、<strong>特征缺失值处理</strong>、<strong>特征无量纲化</strong>、<strong>连续特征离散化</strong>、<strong>类别特征数值化</strong>和<strong>特征交叉组合</strong>。</p><ol><li><p>异常值处理<br>在实际业务中由于种种因素，通常会遇到<strong>异常值</strong>，如果不处理这些异常值将会导致后续数据分析和模型训练出现严重误差。</p><ul><li><p>异常值检测<br> 异常值检测主要有三种方法：</p><ul><li>基于统计的方法。基于统计的方法一般会构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点。</li><li>基于聚类的方法。基于聚类算法将训练样本划分为若干类，如果某一个类的样本数很少，而且类中心和其他类的距离都很远，那么这个类中的样本极有可能就是异常点。</li><li>专业的异常点检测算法。孤立森林（<code>lsolation Forest</code>）是一种应用官方的异常点检测算法。</li></ul></li><li><p>异常值处理<br> 在检测出异常值后，一般常见的处理方式有两种：</p><ul><li>直接删除包含异常值的样本。</li><li>结合特征含义选择置空异常值，或者填充为其他值。</li></ul><p> 另外有两种<strong>“异常”</strong>是无法通过技术手段检测出来的，需要结合具体业务含义识别</p><ul><li>周期性变化的特征。这类特征会严重影响模型的稳定性，应予以剔除。</li><li>具有明显缺陷的特征，如有些埋点后续不会再有，相关特征应予以剔除。</li></ul></li></ul><p> 需要强调的是处理异常值必须谨慎。通过算法筛选出的异常值是否真正异常，需要从业务含义角度再次确认，避免将正常数据过滤掉。</p></li><li><p>特征缺失值处理<br>在处理<strong>特征缺失值</strong>之前，需要先判断缺失的原因。在实际业务中，出现缺失的原因有两种：</p><ul><li>非正常缺失。是指由于原始数据存储、数据接口出现异常而导致的回溯的特征缺失。</li><li>正常缺失。对于需要客户授权的数据，部分客户拒绝会导致数据缺失；另外特殊的特征计算也会造成特征缺失。特征缺失值是否需要填充，需要根据建模时使用的算法综合考虑。<ul><li>使用线性回归算法时，可以根据特征含义使用均值、众数和中位数等填充。</li><li>在使用逻辑斯谛回归建立传统评分卡模型时，由于模型训练前会对特征进行分箱处理，因此会将特征缺失值单独作为一箱。</li><li>使用决策树建模时，例如 <code>XGBoost</code> 算法会自动处理特征缺失值，因此不需要填充。</li></ul></li></ul><p> 不同缺失的处理方案如下：</p></li></ol><p><img src="https://s2.loli.net/2022/12/29/kpjbL4DmU7XnZ6C.png" alt="risk_3_7.jpg"></p><ol start="3"><li><p>特征无量纲化<br>特征无量纲化主要是通过特征的标准化将<strong>特征“缩小”到同一量纲</strong>。<br>对于建模特征，如果特征的单位或大小相差较大，或者特征的方差比其他几个特征高几个数量级，那么就很容易影响目标结果，使得线性模型无法学习其他特征，此时有必要进行特征标准化处理。常见的标准化处理方式如下：</p><ul><li><code>max-min</code> 标准化<br> <strong><code>max-min</code> 标准化</strong>也称为“归一化”，是通过对原始特征进行变换，把特征值映射到 <code>[0, 1]</code>。其中 <code>Xmax</code> 表示特征最大值，<code>Xmin</code> 表示特征最小值，<code>X</code> 表示原始特征值，变换公式如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X&#x27; = (X - Xmin) / (Xmax - Xmin)</span><br></pre></td></tr></table></figure></li><li><code>z-score</code> 标准化<br> <strong><code>z-score</code> 标准化</strong>是常见的特征预处理方式，线性模型在训练数据之前基本都会进行 <code>z-score</code> 标准化。对原始特征进行变换可以把特征分布变换到均值为 <code>0</code>，标准差为 <code>1</code>，变换公式如下所示，其中 <code>u</code> 为特征均值，<code>a</code> 为特征标准差。 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X&#x27; = (X - u) / a</span><br></pre></td></tr></table></figure></li></ul><p> <small>在使用 <code>max-min</code> 标准化时，如果测试集里的特征存在小于 <code>Xmin</code> 或大于 <code>Xmac</code> 的值，就会导致 <code>Xmin</code> 和 <code>Xmax</code> 发生变化。</small></p></li><li><p>连续特征离散化<br>连续特征离散化也称特征分箱，是指将<strong>连续属性的特征进行分段，使其变成一个个离散的区间</strong>。<br>在使用逻辑斯谛回归建立风控评分卡模型时，通常会对来连续特征进行离散化分箱，离散化后的特征对异常值有很强的<strong>鲁棒性</strong>，降低模型过拟合的风险，模型会更加稳定。此外单个特征离散化为多个分箱之后，再对分箱进行数值转换（例如 <code>WOE</code> 转换），此过程可以对非线性的关系进行线性转化，提高线性模型的表达能力。当建模的样本量较少时，离散化特征就非常重要，经过离散化后可以丢弃数据的细节信息，有效降低过拟合风险。<br>常见的特征分箱如下：</p><ul><li><strong>等频分箱</strong>是指分箱后，每个箱内的样本量相等。等频分箱能够确保每箱有足够的样本量，更有统计意义且在实际种应用广泛。</li><li><strong>等距分箱</strong>是指按照相同宽度将特征值分为若干等份，各箱的特征值跨度相同。等距分箱的缺点是受到异常值的影响比较大，各箱之间的样本量不均衡，甚至有可能出现箱的样本量为 <code>0</code> 的情况。</li><li><strong>卡方分箱</strong>是依赖于卡方校验的分箱方法，其基本思想是判断相邻的两个区间是否有分布差异，基于卡方统计量的结果进行自下而上的合并，直到满足分箱的终止条件为止。终止条件包括分箱个数和卡方阈值。</li><li><strong>决策树分箱</strong>是指利用决策树算法，根据树节点的分割点，将特征划分为不同的分箱区间，属于有监督的分箱方法。</li></ul><p> <small>特征是否需要分箱是跟建模算法有关。风控评分卡模型需要很强的业务可解释性，所以在使用逻辑斯谛回归建模时，通常需要分箱处理；然而在使用 <code>XGBoost</code>、<code>LightGBM</code> 等机器学习算法时，通常是不需要分箱。</small></p></li><li><p>类别特征离散化<br>逻辑斯谛回归和支持向量机等算法要求所有特征是<strong>数值型变量</strong>，但在实际业务中会存在部分特征是<strong>类别型变量</strong>（例如性别、身份和职业等）。类别型变量可以分为两种：第一种是没有任何先后顺序或等级关系的<strong>标称类别变量（<code>nominal category variable</code>）</strong>，例如性别、省份等；第二种则是有先后顺序或等级关系的<strong>有序类别型变量（<code>ordinal category variable</code>）</strong>，例如学历、满意程度等。<br>处理类别特征通常采用的方式是<strong>编码</strong>。编码分为两种：一种是<strong>无监督编码方式</strong>，主要有<strong>序数编码</strong>和 <strong><code>one-hot</code> 编码</strong>；另一种则是<strong>有监督编码方式</strong>，主要有 <strong><code>Binary</code> 编码</strong>、**<code>Hashing</code> 编码<strong>、</strong><code>CatBoost</code> 编码**等。<br><small>当特征类别取值较多时，通常是先进行分箱，合并一些类别后再对分箱进行编码处理。</small><br>无监督编码方式：</p><ul><li>序数编码<br>   <strong>序数编码（<code>ordinal encoding</code>）</strong>是一种简单的编码方式，直接对特征中的每个类别设置一个标号，<strong>将非数值特征转化为数值特征</strong>。一个有 <code>N</code> 种类别的特征可以与 <code>[0, N-1]</code> 中的整数一一对应。<br>   <small>需要注意的是，序数编码只是将类别型变量更换了一种表达方式，其本质上还是离散的，数值化后的大小关系没有实际意义。</small></li><li><code>one-hot</code> 编码<br>   <code>one-hot</code> 编码（<code>one-hot encoding</code>）也称“独热”编码，是指<strong>对每一种分类单独创建一个列，用 <code>0</code> 或 <code>1</code> 填充</strong>。<br>   <img src="https://s2.loli.net/2022/12/29/NMKFmT1eAP6fxSI.png" alt="risk_3_8.jpg"><br>   <small>需要注意的是，对于类别特别多的类别型变量，<code>one-hot</code> 编码会导致特征维度激增，特征更加稀疏，影响模型效果。</small></li></ul><p> 有监督编码方式：</p><ul><li>目标编码<br> <strong>目标编码（<code>target encoding</code>）</strong>也称<strong>均值编码</strong>，是一种有效表示类别型变量的方法。<br> 该方法将<strong>类别型变量的值映射为和标签 <code>y</code> 相关的统计指标</strong>，属于有监督编码方式。具体是将特征中的每个字替换为该类别的标签 <code>y</code> 的均值。但该方法严重依赖因变量的分布，会导致大大减少生成编码后特征的数量。<ul><li>在 <em>分类模型</em> 中，标签 <code>y</code> 的取值一般只有 <code>0</code> 和 <code>1</code> 两种，目标编码公式如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X&#x27; = p(y=1 | X=Xtarget)</span><br></pre></td></tr></table></figure> 计算特征值等于类别 <code>Xtarget</code> 时 <code>y=1</code> 的概率。</li><li>在 <em>回归模型</em> 中，标签 <code>y</code> 是连续数值，目标编码公式如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X&#x27; = sum(y | X=Xtarget) / sum(X=Xtarget)</span><br></pre></td></tr></table></figure> <code>y | X=Xtarget</code> 表示特征值等于类别 <code>Xtarget</code> 是 <code>y</code> 的取值。</li></ul></li><li><code>WOE</code> 编码<br> <strong><code>WOE</code>（<code>Weight of Evidence</code>，证据权重）</strong>是针对对原始特征的一种编码形式。<br> <strong><code>WOE</code> 编码（<code>WOE encoding</code>）</strong>适用于<strong>二分类问题的特征预处理</strong>。具体做法是使用特征中每种类别 <code>y=1</code> 的概率和 <code>y=0</code> 的概率的比值的对数替代每种类别的特征值。计算公式如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WOEi = ln(p(Badi)/p(Goodi)) = ln((Badi/BadT)/(Goodi/GoodT)) = ln(Badi/BadT) - ln(Goodi/GoodT)</span><br></pre></td></tr></table></figure> 其中 <code>Badi</code> 为类别 <code>i</code> 中标签为 <code>1</code> 的样本数，<code>Goodi</code> 为类别 <code>i</code> 中标签为 <code>0</code> 的样本数，<code>BadT</code> 为所有样本中标签为 <code>1</code> 的样本数，<code>GoodT</code> 为所有样本中标签为 <code>0</code> 的样本数。因此 <code>WOE</code> 可以表示<strong>“当前类别中坏样本占所有坏样本的比例”</strong>和<strong>“当前类别中好样本占所有好样本的比例”</strong>的差异。<br> <small><code>WOE</code> 编码不仅可以处理类别特征，也可以处理连续特征。尤其是在评分卡模型中，使用逻辑斯谛回归算法拟合特征与逾期率的关系，首先会对连续特征进行分箱，然后利用各箱的 <code>WOE</code> 值代替特征值。</small></li></ul></li><li><p>特征交叉组合<br>特征交叉组合是数据特征的一种处理方式，该方式可以<strong>增加特征的维度</strong>，组合的特征能够反映更多的非线性关系。<br><small>实践中，通常会对类别特征进行组合；而对于连续特征，可以先进行分箱，再进行组合，也可以直接进行特征交叉衍生。</small></p><ul><li>离散特征分类组合<br>   对于<strong>类别型变量</strong>特征的交叉组合，特征 <code>A</code> 取值类别有 <code>m</code> 种，特征 <code>B</code> 取值类别有 <code>n</code> 种，可以通过<strong>笛卡尔积</strong>的方式进行特征组合，可以重新得到组合后的 <code>m*n</code> 个组合特征。</li><li>连续特征交叉衍生<br> 特征交叉衍生的方式有很多种，常用的方式有：<ul><li>利用数值型特征之间的<strong>加、减、乘、除操作</strong>得到新特征；</li><li>对已选定特征进行<strong>奇异值分解（<code>SVD</code>）</strong>，将奇异值作为新特征；</li><li>根据已选特征进行<strong>聚类</strong>，将所在类别的平均目标值或出现最多的值作为新特征，或者将所在类别与其他类别的距离作为新特征等；</li><li>此外还可以利用<strong>深度学习技术衍生新特征</strong>，利用神经网络中某层的数据作为新特征也是一种思路。</li></ul></li></ul><p> <small>特征交叉组合会导致特征维度激增，组合后的特征因此可能会很稀疏，因此在实践中，需要根据特征含义组合出具有业务含义的特征。</small></p></li></ol><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择（<code>feature selection</code>）是指选择能够使模型获得最佳性能的特征子集。特征的必要性包括：</p><ul><li><p>特征池中的特征并非都对模型有益，如果选取不稳定的的特征训练模型，那么最终生成的模型的稳定性较差。</p></li><li><p>线性模型要求特征间不能多重共线性，因此需要选择无严重共线性的特征建模。</p></li><li><p>选取尽可能对模型有增益的特征，剔除无用特征，从而降低特征维度缩短模型训练时间和减少对机器资源的损耗。</p><p> 特征选择一般都是反复迭代、验证，并且和模型训练过程循环进行，最终可以得到性能优异的模型。特征选择可以从<strong>基于特征属性选择</strong>、<strong>基于特征效果选择</strong>和<strong>基于特征稳定性</strong>三个方面。</p></li></ul><p><img src="https://s2.loli.net/2022/12/04/XzvGq4ukioealpN.png" alt="risk_3_3.jpg"></p><h5 id="基于特征属性选择"><a href="#基于特征属性选择" class="headerlink" title="基于特征属性选择"></a>基于特征属性选择</h5><p>基于特征属性选择特征，不需要任何标签信息，直接根据特征值分布或特征之间的关系进行选择，计算速度快，一般用于特征初筛。主要方法有<strong>缺失率选择法</strong>、<strong>变异系数选择法</strong>、<strong>相关性选择法</strong>和<strong>多重共线性选择法</strong>。</p><ol><li><p>缺失率选择法<br>一般情况下，当特征缺失率超过 <code>95%</code> 时就不再适合参与建模，首先要做的就是剔除特征。而当特征缺失率不超过 <code>95%</code> 时，可以采用缺失值处理方法进行处理。对于缺失率阈值，可以根据具体业务场景灵活调整。</p></li><li><p>变异系数选择法<br> <strong>变异系数（<code>coefficient of variation</code>）又称 <em>离散系数</em></strong> 是概率分布离散程度的一个归一化量度，其定义为标准差与均值之比。<br>变异系数反映了特征分布的离散程度，相比方差，变异系数是一个无量纲量，因此在比较两组量纲不同或均值不同的数据时，应该用变异系数而不是标准差。特征选择过程中会首先过滤变异系数为 <code>0</code> 的特征。<br><small>如果某个特征的变异系数很小，则表示样本在这个特征上没有差异，可能特征中的大多数值都是一样的，甚至整个特征取值全部相同。</small></p></li><li><p>相关性选择法<br>相关性是衡量<strong>两个特征之间依赖关系</strong>的指标。度量特征相关性的指标有很多，常见的有以下三种：</p><ul><li><strong><code>Person</code> 相关系数</strong><br>  用来度量特征的 <em>线性相关性</em>，取值范围为 <code>[-1, 1]</code>，大于 <code>0</code> 表示两个特征正相关，小于 <code>0</code> 表示两个特征负相关，等于 <code>0</code> 则表示两个特征非线性相关。</li><li><strong><code>Spearman</code> 相关系数</strong><br>  用来度量特征 <em>单调相关性</em>，取值范围为 <code>[-1, 1]</code>，大于 <code>0</code> 表示两个特征正相关，小于 <code>0</code> 表示两个特征负相关，等于 <code>0</code> 则表示两个特征非单调相关。</li><li><strong><code>Kendall</code> 相关系数</strong><br>  用来度量特征 <em>有序分类特征相关性</em>，取值范围为 <code>[-1, 1]</code>，大于 <code>0</code> 表示两个特征正相关，小于 <code>0</code> 表示两个特征负相关，等于 <code>0</code> 则表示两个特征排名独立。</li></ul><p>  <small>逻辑斯谛回归等算法要求特征之间不得具有很强的相关性，否则会导致无法用特征系数解释最终模型的入模特征（即模型使用的特征）与目标变量之间的关系。</small></p></li><li><p>多重共线性选择法<br>多重共线性描述的是<strong>一个自变量与其他自变量（可以多个）</strong>之间的完全线性关系。<strong>方差膨胀系数（<code>Variance Inflation Factor, VIF</code>）</strong>是一种衡量共线性程度的常用指标，表示回归系数估计量的方差与假设特征间不线性相关时的方差的比值。<br><code>VIF</code> 计算公式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VIF = 1 / (1 - R^2)</span><br></pre></td></tr></table></figure><p> <code>R^2</code> 是某个特征对其余特征做回归分析的复相关系数。<code>VIF</code> 越大，该特征与其他特征的关系越复杂，多重共线性越严重。<code>VIF &lt; 10</code> 则认为不存在多重共线性；<code>10 &lt;= VIF &lt; 100</code> 则认为存在较强的多重共线性；<code>VIF &gt;= 100</code> 则认为存在严重的多重共线性。</p></li></ol><h5 id="基于特征效果选择"><a href="#基于特征效果选择" class="headerlink" title="基于特征效果选择"></a>基于特征效果选择</h5><ol><li><p><code>IV</code> 选择<br> <strong><code>IV</code>（<code>Information Value</code>，信息价值）</strong>是衡量特征预测能力的关键指标。<code>IV</code> 和 <code>WOE</code> 之间的关系可以描述为：<code>WOE</code> 描述了特征和目标变量之间的关系；<code>IV</code> 用来衡量这种关系的强弱程度。<br><code>WOE</code> 分析特征各个分箱对于目标变量的预测能力，<code>IV</code> 用来反映特征的总体预测能力。<code>IV</code> 的计算公式即在第 <code>i</code> 箱 <code>WOE</code> 的基础上乘以系数，该系数表示该分箱坏样本比例和好样本比例的差。</p><table><thead><tr><th><code>IV</code> 范围</th><th>描述</th></tr></thead><tbody><tr><td>iv &lt; 0.02</td><td>无预测能力，需放弃</td></tr><tr><td>[0.02, 0.1)</td><td>较弱的预测能力</td></tr><tr><td>[0.1, 0.3)</td><td>预测能力一般</td></tr><tr><td>[0.3, 0.5)</td><td>预测能力较强</td></tr><tr><td>iv &gt; 0.5</td><td>预测能力极强，需检查</td></tr></tbody></table></li><li><p>卡方校验<br>卡方校验是一种以<strong>卡方分布</strong>为基础的检验方法，主要用于类别变量，根据样本数据推断总体分布与期望分布是否有显著差异，或者推断两个类别变量是否相关或相互独立。其原假设为：观察频数与期望频数没有差别。</p></li><li><p>包裹法</p><ul><li><p>逐步回归<br>   <strong>逐步回归（<code>stepwise regression</code>）</strong>是一种筛选并剔除引起多重共线性变量的方法，广泛应用于逻辑谛斯回归模型。<br>  其基本思想是将解释变量逐个引入模型，每引入一个解释变量都进行统计性假设检验，当原来引入的解释变量由于后来解释变量的引入变得不再显著时则将其删除，以确保每次引入新变量之前，回归方程中只包含显著性变量。<br>  逐步回归共有三种方式：</p><ul><li>前向逐步回归，将特征逐步加入。</li><li>后向逐步回归，从所有特征集中将特征逐步剔除。</li><li>双向逐步回归，前向加入与后向剔除同时进行，即在每次加入新特征的同时，将显著性水平低于阈值的特征剔除。</li></ul><p>  <small>一般情况下，双向逐步回归的效果比前向逐步回归和后向逐步回归好。</small></p></li><li><p>递归特征消除<br>   <strong>递归特征消除（<code>Recursive Feature Elimination, RFE</code>）</strong>也是常用的包裹法特征选择方法，其基本思想是使用一个 <em>基模型</em> 进行多轮训练，每轮训练之后消除若干重要性低的特征（线性模型特征归一化后使用特征系数衡量其重要性），再基于特征集进行下一轮训练。</p></li></ul></li><li><p>嵌入法</p><ul><li>基于 <code>L1</code> 范数<br>  <strong>线性模型</strong>可以被看作是多项式模型，其中每一项的系数都可以表征这一维特征的重要性，越重要的特征在模型中对应的系数越大，而与输出变量相关性越小的特征，对应的系数越接近 <code>0</code>。<br>  <code>L1</code> 正则化将系数的 <code>L1</code> 范数作为 <em>惩罚</em> 项加到损失函数中，由于正则项非零，则会导致不重要的特征系数变为 <code>0</code>，因此使用 <code>L1</code> 正则化的模型往往稀疏，这使得 <code>L1</code> 正则化成为很好的特征选择方法。<br>  <small>在使用线性模型进行 <code>L1</code> 正则化特征选择时，应先消除多重共线性。</small></li><li>基于树模型<br>  建立<strong>树模型</strong>的过程就是特征选择。基于树模型的特征选择会根据信息增益或基尼不纯度的准则来选择特征进行建模，输出各个特征的重要度，依此进行特征筛选。</li></ul></li></ol><h5 id="基于特征稳定性"><a href="#基于特征稳定性" class="headerlink" title="基于特征稳定性"></a>基于特征稳定性</h5><ol><li><p><code>PSI</code> 选择<br><code>PSI</code> 指特征的<strong>稳定性指标</strong>，用于识别分布变化大的特征，充分了解其背后的特征分布变化的原因，判断是否可接受。其中稳定性是有参照的，在建模时将训练样本的分布作为<strong>预期分布（<code>expected distributiopn</code>）</strong>，将 <code>OOT</code> 样本作为作为<strong>实际分布（<code>actual distributiopn</code>）</strong>。<br>在计算 <code>PSI</code> 时需要先将特征值分箱。<code>PSI</code> 计算公式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PSI_i = (p(Actual_i) - p(Expected_i)) * ln(p(Actual_i)/p(Expected_i)) </span><br><span class="line">      = (Actual_i/Actual_t - Expected_i/Expected_t) * ln((Actual_i/Actual_t)/(Expected_i/Expected_t))</span><br></pre></td></tr></table></figure><p>其中 <code>PSI_i</code> 表示第 <code>i</code> 个分箱稳定性指标的结果，<code>Actual_i</code> 为第 <code>i</code> 个分箱实际样本个数，<code>Expected_i</code> 为第 <code>i</code> 个分箱期望样本个数，<code>Actual_t</code> 为实际样本总数，<code>Expected_t</code> 为期望样本总数。</p><p> <code>PSI</code> 表示实际样本分布和期望样本分布的差异。</p><table><thead><tr><th><code>PSI</code> 范围</th><th>稳定性</th></tr></thead><tbody><tr><td>iv &lt; 0.1</td><td>变化不太显著</td></tr><tr><td>[0.1, 0.25]</td><td>有比较显著的变化</td></tr><tr><td>iv &gt; 0.25</td><td>变化剧烈，需要特殊关注</td></tr></tbody></table></li><li><p>逾期率变化选择<br>在风控业务中，有些特征的不稳定性表现在对逾期率排序的衰减上，随着时间的变化特征对预测变量的排序会发生颠倒，称之为<strong>“倒箱”</strong>。<br><small><code>PSI</code> 反映的主要是特征分布的不稳定性，而“倒箱”体现特征对预测变量区分能力的不稳定性。</small></p></li></ol><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p>特征提取（<code>feature extraction</code>）是指从原有较多的特征中计算出较少的特征，用新特征替换原有特征，达到降维的目的，即通过从样本中学习一个映射函数 <code>f</code>，将原特征矩阵 <code>X1</code> 映射到 <code>X2</code>，其中 <code>X2</code> 的维度小于 <code>X1</code>。<br><small>特征选择和特征提取都是为了特征降维，二者实现效果相同，当采用的方法不同。特征提取采用的是通过属性间的关系，如组合不同属性得到新属性以此改变原有的特征空间；特征选择采用的是从原始特征数据集中选用子集，这是一种包含关系并没有改变原始特征空间。</small><br>特征提取方法分为两大类：</p><ul><li>线性特征提取方法<ul><li><strong>主成分分析方法（<code>Principal Component Analysis, PCA</code>）</strong>，映射后的样本具有更大的发散性。</li><li><strong>线性判别分析法（<code>Linear Discriminant Analysis, LDA</code>）</strong>，映射后的样本具有较好的的分类性能。</li></ul></li><li>非线性特征提取方法<ul><li><strong>局部线性嵌入（<code>Locally Linear Embedding, LLE</code>）</strong>，保持邻域内样本之间的线性关系。</li><li><strong>多维尺度变换（<code>Multiple Dimensional Scaling, MDS</code>）</strong>，保持降维后的样本间距离不变。</li></ul></li></ul><h5 id="线性特征提取"><a href="#线性特征提取" class="headerlink" title="线性特征提取"></a>线性特征提取</h5><ol><li><p><code>PCA</code><br><code>PCA</code> 将<strong>高维的特征向量合并为低维的特征向量</strong>，是一种<strong>无监督</strong>的特征提取方法。其基本原理是通过线性投影，将高维数据映射到低维空间中表示，并且期望在所投影的维度上数据的方差最大（最大方差理论），以此使用较少的数据维度，留存较多的原始数据特性。<br><code>PCA</code> 是我们常用的特征提取方法，其优点如下：</p><ul><li>仅需要以<strong>方差</strong>衡量信息量，不受数据集以外的因素影响。</li><li>各主成分之间正交，可消除原始数据成分间相互影响的因素。</li></ul><p> 缺点如下：</p><ul><li>主成分各个特征维度的含义不如原始特征的解释性强。</li><li>非主成分也可能含有重要信息，丢弃后会降低模型效果。</li></ul></li><li><p><code>LDA</code><br><code>LDA</code> 是一种<strong>基于分类模型进行特征属性合并</strong>的操作，是一种<strong>有监督</strong>的特征提取方法。其基本原理是将带有标签的数据投影到维度更低的空间中，使得投影后的点按类别区分，相同类别的点会在投影后的空间中更接近，用一句话概括就是：投影后相同类间方差最小，不同类间方差最大。<br><code>LDA</code> 的优点如下：</p><ul><li>在特征提取的过程中，可以使用类别的先验知识。</li><li>在分类过程中，依赖<strong>均值</strong>而不是方差的时候，其优于 <code>PCA</code> 之类的算法。</li></ul><p> 缺点如下：</p><ul><li>不适合对<strong>非高斯分布样本</strong>进行特征提取。</li><li>可能过度拟合数据。</li></ul><p> 除了 <code>PCA</code> 和 <code>LDA</code> 之外，线性特征提取方法还有因子分析（<code>Factor Analysis, FA</code>）、奇异值分解（<code>Singular Value Decomposition, SVD</code>）和独立成分分析（<code>Independent Component Analysis, ICA</code>）等。</p></li></ol><h5 id="非线性特征提取"><a href="#非线性特征提取" class="headerlink" title="非线性特征提取"></a>非线性特征提取</h5><ol><li><p><code>LLE</code><br><code>LLE</code> 是一种基于 <em><strong>流形学习</strong></em> 的方法（流形学习假设所处理的数据点分布在嵌入外围欧氏空间的一个潜在的流形体上，或者说这些数据点可以构成这样的一个潜在的流形体），其能够使特征提取后的数据较好地保存原有流形结构。<code>LLE</code> 假设数据在较小的局部是线性的，即每一个数据点都可以由其近邻点线性表示。<br><code>LLE</code> 主要分为三步，首先寻找每个样本点的 <code>k</code> 个近邻点；然后由每个样本点的近邻点计算出该样本点的权重；最后由该样本点的权重在低维空间中重构样本数据。至此就可以将特征映射到低维空间中。<br><code>LLE</code> 的优点如下：</p><ul><li>可以学习任意维度局部线性的低维流形。</li><li>该方法归结为<strong>稀疏矩阵特征分解</strong>，计算复杂度较小且实现容易。</li></ul><p> 缺点如下：</p><ul><li>学习的流形只能是不闭合的，且样本集是稠密、均匀的。</li><li>该方法对最近邻接样本数的选择敏感，不同近邻数对最终结果有很大影响。</li></ul></li><li><p><code>MDS</code><br><code>MDS</code> 是将<strong>高维空间中的样本点投影到低维空间</strong>中，让样本彼此之间的距离尽可能不变。其基本原理是首先计算得到高维空间中样本之间的<strong>距离矩阵</strong>，接着计算得到低维空间的<strong>内积矩阵</strong>，然后对低维空间的内积矩阵进行特征值分解，并按照从大到小的顺序取前 <code>d</code> 个（<code>d</code> 表示低维空间的维度）特征值和特征向量，最后得到在 <code>d</code> 维空间中的距离矩阵。<br><code>MDS</code> 的优点如下：</p><ul><li>不需要先验知识，计算简单。</li><li>保留样本在原始空间的相对关系，可视化效果较好。</li></ul><p> 缺点如下：</p><ul><li>当有样本的先验知识时，他无法被充分利用，因此无法得到预期效果。</li><li>该方法认为各维度对目标的贡献相同，忽略了维度间的差剧。</li></ul><p> 除了 <code>LLE</code> 和 <code>MDS</code> 之后，非线性特征提取方法还有等度量映射（<code>Isometric Feature Mapping, ISOMAP</code>）和 <code>t-SNE</code> 等。</p></li></ol><h4 id="训练、概率转化和效果评估"><a href="#训练、概率转化和效果评估" class="headerlink" title="训练、概率转化和效果评估"></a>训练、概率转化和效果评估</h4><p>特征选择和特征提取之后就是关键的模型训练环节，首先<strong>模型训练基础知识</strong>，其次<strong>概率转化方法</strong>，最后<strong>评价模型效果</strong>以及<strong>选择合适的模型</strong>。</p><h5 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h5><p>机器学习的模型训练其本质是<strong>参数优化过程</strong>。其参数可以分为两种：一种是<strong>模型参数（<code>parameter</code>）</strong>；另一种是<strong>超参数（<code>typerparameter</code>）</strong>。<br><strong>模型参数</strong>是可以直接通过数据估计得到的，如线性回归模型中的回归参数。<strong>超参数</strong>是用来定义模型结构或优化策略的，通常需要在模型训练前根据经验给定，如正则化系数。因此模型训练的目标是找到使得最终模型最好的超参数组合。</p><p>模型调参是寻找最优超参数组合的过程。常见的模型调参方法有以下几种：<strong>网格搜索（<code>grid search</code>）</strong>；<strong>随机搜索（<code>random search</code>）</strong>；<strong>贝叶斯优化（<code>bayesian optimization</code>）</strong>。</p><p>交叉验证的方法有以下几种：<strong>留 <code>p</code> 法交叉验证</strong>；<strong>留一法交叉验证</strong>；**<code>K</code> 折交叉验证**。</p><h5 id="概率转化"><a href="#概率转化" class="headerlink" title="概率转化"></a>概率转化</h5><p>风控模型（如 <code>XGBoost</code> 模型、<code>LighrGBM</code> 模型、<code>LR</code> （逻辑斯谛回归）模型）直接输出的是客户逾期概率，在风控信贷场景中，需要<strong>将概率转化为评分，通过分数量化客户的风险等级</strong>。<br>转换为评分的额方法如下：设 <code>p</code> 为客户逾期概率，那么客户逾期概率与未逾期概率的比值 <code>p/(1-p)</code> 记为 <code>oods</code>。转换为评分的计算公式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score = A - B * log(odds) = A - B * log(p / 1-p)</span><br></pre></td></tr></table></figure><p> 客户逾期概率越低，评分越高。在通常情况下，这是分值的变动方式，即高分值代表低风险，而低分子代表高风险。其中 <strong><code>A</code> 和 <code>B</code> 都是常数</strong>，在计算时通常需要做出两个假设：给定 <code>odds = Ratio</code> 时，预期分数为 <code>Base</code>；<code>odds</code> 翻倍时，分数减少值为 <code>PDO(Point of Double Odds)</code>。由此可以得到 <code>A = Base + B * log(2 * Ratio), B = PDO / log2</code>。</p><h5 id="模型效果评估"><a href="#模型效果评估" class="headerlink" title="模型效果评估"></a>模型效果评估</h5><p>根据模型对样本的预测分数和样本的真实标签，可以通过不同角度的指标来评估模型的效果。<br>通过多种指标对模型性能进行评价，不同的评价指标往往产生不同的评价效果，这表明模型的 <em>好坏</em> 是相对的，具体使用何种指标取决于实际使用场景。<br>样本根据其<strong>真实类别和模型预测类别</strong>可形成以下四种组合：</p><ul><li><strong>真正例（<code>True Positive, TP</code>）</strong>：真实类别为正例，预测类别为正例。</li><li><strong>假正例（<code>False Positive, FP</code>）</strong>：真实类别为负例，预测类别为正例。</li><li><strong>假负例（<code>False Negative, FN</code>）</strong>：真实类别为正例，预测类别为负例。</li><li><strong>真负例（<code>True Negative, TN</code>）</strong>：真实类别为负例，预测类别为负例。<br><img src="https://s2.loli.net/2022/12/22/DMNZFGvq5iTQen8.png" alt="risk_3_5.jpg"></li></ul><p>对于回归任务，常见的评价指标有 <code>RMSE</code>（平方根误差）、<code>MAE</code>（平均绝对误差）、<code>MSE</code>（平均平方误差）和 <code>coefficient of determination</code>（决定系数）。对于分类任务，常见的评价指标有准确率、精确率、召回率、<code>F1</code> 值、<code>AUC</code> 和 <code>KS</code> 等。</p><ul><li><strong>准确率</strong><br>  准确率（<code>accuracy</code>）是指正确预测的正负例样本数和总样本数的比值。<code>accuracy = (TP + TN) / (TP + FP + FN + TN)</code>。</li><li><strong>精确率</strong><br>  准确率（<code>precision</code>）又称查准率，是指预测为正例的样本中真正是正例的样本比例。<code>precision = TP / (TP + FP)</code>。</li><li><strong>召回率</strong><br>  召回率（<code>recall</code>）又称查全率，是指实际正例样本中模型预测为正例的样本比例。<code>recall = TP / (TP + FN)</code>。</li><li><strong><code>F1</code> 值</strong><br>  <code>F1</code> 值是精确率和召回率的调和值，更接近于两个数中较小的那个，因此精确率和召回率接近时，<code>F1</code> 值更大。<code>F1 = (2 * precision * recall) / (precision + recall)</code>。</li><li><strong><code>AUC</code></strong><br>  <code>AUC(Area Under Curve)</code> 为 <code>ROC</code> 曲线下的面积。<code>ROC(Receiver Operating Characterstic, 接收者工作特征)</code> 曲线源于雷达信号分析技术，<code>ROC</code> 曲线的横坐标为 <code>FPR</code>（假正率），<code>FPR = FP / (FP + TN)</code>，即被预测为正例的负样本数与真实负样本数的比值；纵坐标为 <code>TPR</code>（真正率），<code>TPR = TP / (TP + FN)</code>，即被预测为正例的正样本数与实际正样本数的比值。<code>AUC</code> 的取值范围为 <code>0~1</code>。</li><li><strong><code>KS</code></strong><br>  <code>KS（Kolmogorov-Smirnov）</code> 指标主要用来验证模型对客户 <em>好坏</em> 的区分能力，用以检验两个经验分布是否不同，或者一个经验分布与一个理想分布是否不同。<br>  在计算风控模型 <code>KS</code> 指标时，通常是在模型对样本打分后，对分数进行分箱，然后分别统计每箱累积 <em>好</em> 客户和累积 <em>坏</em> 客户与 <em>好</em> 客户和 <em>坏</em> 客户总体的比值，累计 <em>坏</em> 客户比例与累计 <em>好</em> 客户比例的差值即为每箱对应的 <code>KS</code> 值。<br>  模型 <code>KS</code> 定义为各分箱 <code>KS</code> 值得最大值：<code>KS = max(Pcum(Bad) - Pcum(Good))</code>，<code>KS</code> 值越高，模型越好。但过高的 <code>KS</code> 值可能意味者过度拟合或特征 <em>穿越</em> 等。<br>  <small>相比 <code>KS</code>，<code>AUC</code> 更加稳健。而相比准确率、召回率、<code>F1</code> 值等指标，<code>AUC</code> 指标优势在于不需要设定分类阈值，只需要关注预测概率的排序，因此一般在二分类模型中主要将 <code>AUC</code> 作为模型效果的评价指标。</small></li></ul><hr><h3 id="开发方法论-上线和维护"><a href="#开发方法论-上线和维护" class="headerlink" title="开发方法论-上线和维护"></a>开发方法论-上线和维护</h3><p>模型训练通常在本地环境中进行，训练完成后，首先选择最优模型并部署到线上环境，然后验证模型在线上环节运行是否准确无误，确认无误后，才会使用。</p><h4 id="部署及上线验证"><a href="#部署及上线验证" class="headerlink" title="部署及上线验证"></a>部署及上线验证</h4><ol><li>模型部署<br>模型部署是将训练完成的模型部署到线上环境。考虑其是否可以跨语言部署，模型文件通常可以选择保存为以下两种方式：</li></ol><ul><li><code>pickle</code> 格式<br>  <code>pickle</code> 是 <code>Python</code> 语言独有的格式。若线上为 <code>Python</code> 环境，那么就可以通过 <code>pickle</code> 格式实现模型的快速读取。  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 pickle 格式保存和读取模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_model_as_pkl</span>(<span class="params">model, path</span>):</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">pickle.dump(model, f, protocol=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_model_from_pkl</span>(<span class="params">path</span>):</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">model = pickle.load(f)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>  <small>保存模型时设置 <code>protocol=2</code> ，表示以二进制协议对模型内容进行序列化存储，以此解决 <code>Python3</code> 环境中保存的模型在 <code>Python2</code> 环境中部署。</small></li><li><code>PMML</code> 格式<br>  预测模型标记语言（<code>Predictive Model Markup Language, PMML</code>）是一套与平台和环境无关的模型标记语言，可实现跨平台的机器学习模型部署。</li></ul><ol start="2"><li>上线验证<br>模型部署到线上环境后，通常先作为<strong>空跑</strong>一段时间使用，当积累一定样本量时可以进行上线验证。上线验证的目的在于确认模型在线上环境中按照预期运行。验证方式主要有以下三种：<strong>预测分数的一致性</strong>；<strong>模型分分布的差异性</strong>；<strong>模型效果的一致性</strong>。</li></ol><h4 id="监控和异常处理"><a href="#监控和异常处理" class="headerlink" title="监控和异常处理"></a>监控和异常处理</h4><p>模型上线之后，为了保证模型有效运行，需要对模型相关指标进行监控。当遇到异常状况时，可以通过多种途径发出预警。</p><h5 id="模型监控"><a href="#模型监控" class="headerlink" title="模型监控"></a>模型监控</h5><p>模型监控主要以报表方式展示各项监控指标。</p><ul><li>模型监控内容<br>  模型监控包含<strong>准确性</strong>、<strong>稳定性</strong>和<strong>有效性</strong>三个方面。<ul><li>准确性是模型有效运行的基础。模型打分准确性监控是要确保<strong>线上模型的结果与线下模型计算的结果一致</strong>。</li><li>稳定性是模型有效运行的保障。稳定性监控主要是监测模型分和特征是否稳定，可以从以下两个方面来判断：<strong>模型分布变化</strong>；<strong>特征值的分布变化</strong>。</li><li>有效性是模型运行的目标。模型有效性监控是指持续监控模型的预测能力，识别其是否有衰减。监控指标主要依赖于 <strong><code>KS</code></strong> 和 <strong><code>AUC</code></strong> 指标，以及特征的 <strong><code>IV</code></strong> 指标。</li></ul></li><li>模型监控形式<br>  模型监控可以按照<strong>日报</strong>、<strong>周报</strong>和<strong>月报</strong>形式，采用<strong>邮件</strong>或<strong>可视化页面</strong>的方式展示监控结果。</li></ul><h5 id="模型预警"><a href="#模型预警" class="headerlink" title="模型预警"></a>模型预警</h5><p>模型预警主要是根据预警条件触发预警信息，提示模型异常，因此需要定义预警指标触发条件和预警形式。<br>对每一项监控设定预警指标并定义预警阈值，当监控的指标值偏离该范围时就会发出预警。对应模型的<strong>准确性</strong>、<strong>稳定性</strong>和<strong>有效性</strong>监控，预警指标有<strong>一致性</strong>、**<code>PSI</code>** 和 **<code>KS</code>**，然后进一步根据指标的风险程度划分预警等级。<br>预警指标阈值没有统一的设置规范，不同业务场景对风险的容忍度有所差别，因此需要结合具体场景和业务相关人员共同确定阈值。</p><h5 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h5><p>模型异常处理是指模型发生异常时，需要快速分析问题和解决问题，将影响尽可能降到最低。导致模型的准确性、稳定性和有效性异常的原因有很多，下面为常见的原因：</p><ul><li>模型准确性异常处理<br>  导致模型准确性异常的原因通常有以下两种：<strong>运行环境发生改变</strong>；<strong>特征预处理逻辑发生改变</strong>。</li><li>模型稳定性异常处理<br>  导致模型稳定性异常的原因通常有以下两种：<strong>数据源异常</strong>；<strong>客群变化</strong>。</li><li>模型有效性异常处理<br>  导致模型有效性异常的原因通常有：<strong>数据源异常</strong>；<strong>客群变化</strong>；<strong>模型自身效果衰减</strong>。</li></ul><h4 id="迭代优化"><a href="#迭代优化" class="headerlink" title="迭代优化"></a>迭代优化</h4><p>模型上线之后，客群的变化、数据维度的增加、业务调整某些数据无法使用等都可能导致模型效果出现波动。面对这些情况对模型的迭代和优化就显得格外重要，当然模型迭代优化的目的在于提升线上模型效果，使得模型在近期样本上表现更好。<br>模型的迭代优化可以从<strong>模型融合</strong>、<strong>建模时效</strong>和<strong>拒绝推断</strong>三个方面进行。</p><h5 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h5><p>模型融合角度优化是指将多个模型的结果相互组合或再训练，以提升最终模型效果。<br>不同样本之间的信息千差万别，不同算法从样本中学到的信息也各不相同，因此可以利用这些差异结合不同类型、不同时间段模型相互融合，提炼出更加丰富的客群信息。融合模型突破了以往单一模型的局限性，具有多个子模型的优点，比单一模型具有更好的效果。</p><ol><li><p>模型融合方法<br>模型融合的方法有很多种，常用的有以下三种：</p><ul><li><strong>模型结果简单加权</strong>是指直接给各个子模型分配权重，通过加权求和得到融合模型的输出。</li><li><strong>模型结果再训练</strong>是指将各个子模型的结果作为特征，采用机器学习算法再次建模，最终得到融合模型。</li><li><strong>集成学习</strong>通过构建多个学习器来完成建模任务。集成学习中常用的四种模型融合方法：<ul><li><strong><code>Bagging</code></strong> 是在多轮采样获取的数据子集上训练多个个体学习器，然后通过投票法或平均法对个体学习其进行集成的方法。</li><li><strong><code>Boosting</code></strong> 是一种在训练过程中，不断对训练样本分布进行调整，基于调整后的样本分布训练下一轮个体学习器的集成学习方法。</li><li><strong><code>Stacking</code></strong> 是一种将多种个体学习器整合在一起来取得更好表现的集成学习方法。一般情况下，<code>Stacking</code> 的训练过程分为两步，第一步训练第一层的多个不同模型，第二部以第一层各个模型的输出来训练得到最终模型。</li><li><strong><code>Blending</code></strong> 与 <code>Stacking</code> 类似，区别体现在于：第一步在训练集上，<code>Blending</code> 不是通过 <code>K</code> 折交叉验证策略得到预测值，而是采用 <code>Hold-Out</code> 策略，即保留固定比例的样本并将其作为验证集，在其余样本上训练出多个模型，分别在验证集和测试集上进行预测，将预测值作为新特征；第二步基于验证集和测试集的新特征，训练得到最终模型。</li></ul></li></ul></li><li><p>模型融合方式<br>模型融合方式有很多，包括<strong>不同标签模型融合</strong>、<strong>不同样本模型融合</strong>和<strong>不同数据源模型融合</strong>。</p><ul><li>不同标签模型融合是指将<strong>不同样本标签</strong>开发模型继续宁融合。基于不同样本标签，模型能够学到样本不同维度的信息，融合后的模型学到的信息更加丰富。</li><li>不同样本模型融合是指将<strong>不同样本</strong>开发的模型进行融合。根据产品、时间等信息，将样本分群，分别训练子模型，再将子模型融合。</li><li>不同数据源模型融合是指先根据<strong>不同数据源特征</strong>分别建立子模型，再将不同数据源子模型融合。</li></ul></li></ol><h5 id="建模时效"><a href="#建模时效" class="headerlink" title="建模时效"></a>建模时效</h5><p>从建模时效角度优化是指快速迭代模型，及时根据线上客群变化做出调整。因为通常情况下在模型上线后，模型更新周期会较长，不能快速反映客群变化，即使发现了线上模型效果衰减，再次开发新的模型也需要一段时间。为此解决建模时效问题的常见做法是应用<strong>模型自动快速迭代</strong>和<strong>在线学习</strong>。</p><ol><li><p>模型自动快速迭代<br><strong>模型自动快速迭代</strong>是指加入最新有表现样本，快速更新迭代模型。<br>核心部分主要包含样本选择、数据准备、数据预处理、特征选择、模型训练和效果评估。为此可以构建一套完整的模型自动化平台，将样本选择、数据准备、自动测试和部署上线也实现自动化，这样就构建了完整的模型自动快速迭代体系。该模型自动快速迭代体系主要包括 <em>特征自动回溯系统</em>、<em>自动建模系统</em>、<em>自动测试系统</em> 和 <em>自动上线系统</em> 四个部分。</p></li><li><p>在线学习<br><strong>在线学习（<code>online learning</code>）</strong>是指根据线上的实时数据，快速进行模型调整，使得模型及时反映线上的变化，提高线上模型的效果。<br>在线学习种具有代表性的算法：**<code>Bayesian Online Learning</code>** 和 **<code>Follow The Regularized Leader(FTRL)</code>**。<br><small>传统模型开发是使用离线数据处理方式进行开发的，开发完成后再部署到线上，这种模型上线后一般是静态的，不会与线上的业务数据有任何互动。</small><br><small>相比模型自动快速迭代，在线学习方法不需要每次使用全量样本重建模型，只需要使用新增样本更新模型参数，建模成本低。</small></p></li></ol><h5 id="拒绝推断"><a href="#拒绝推断" class="headerlink" title="拒绝推断"></a>拒绝推断</h5><p>拒绝推断通常是基于放款样本，但贷前风险预测模型使用场景是所有的授信申请客户，其中包含拒绝样本，即训练模型使用的客群仅是预测人群中的一部分，存在<strong>“样本偏差”</strong>问题。如果能在建模样本中加入被拒绝的样本，那么模型的效果可以得到保障，但是问题在于，被拒绝的样本没有标签，而推测被拒绝样本的标签就是<strong>“拒绝推断”</strong>的主要内容。</p><ol><li><p>使用场景<br>拒绝推断经常使用的场景：<strong>总体风险异质</strong>；<strong>风险通过率很低</strong>；<strong>历史数据与当前数据显著不同</strong>。<br><strong>已知推断比（<code>known-to-inferred odds ratio, KI</code>）</strong>常用来衡量拒绝推断的风险是否合理，定义式如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KI = (Gk/Bk)/(Gi/Bi)</span><br></pre></td></tr></table></figure><p>其中 <code>Gk</code>、<code>Bk</code>、<code>Gi</code>、<code>Bi</code> 分别表示已知好客户、已知不良客户、推断好客户、推断不良客户的样本数量。<code>KI</code> 值越高，说明推断人群中不良客户的比例越高，推断人群的风险也就越高。通常<strong>合理的 <code>KI</code> 值在 <code>2 ~ 4</code> 之间</strong>，而业界倾向于更大的值。</p><p> 拒绝推断场景中常见的三种模型如下：</p><ul><li><strong><code>AR(Accept Reject)</code> 模型</strong>：以是否放贷为标签，是在全量样本上构建的模型。</li><li><strong><code>KGB(known Good Bad)</code> 模型</strong>：以逾期表现为标签，是在已知好坏标签的样本上构建的模型。</li><li><strong><code>AGB(All Good Bad)</code> 模型</strong>：以逾期表现为标签，是在全部授信申请样本上构建的模型。建模样本包含已知好坏标签（真实标签）的样本和推断出“伪标签”的样本。</li></ul></li><li><p>常用方法<br>解决样本偏差问题的方法有两类：</p><ul><li>数据法<ul><li><strong>增量下探法</strong><br> 增量下探法是指从本该拒绝的样本中，随机选取部分样本授信通过，以便获取该部分样本的真实标签。</li><li><strong>同生表现法</strong><br> 同生表现法是指通过客户在其他产品或机构的贷后表现，推断出本产品上的伪标签。</li></ul></li><li>推断法<ul><li><strong>硬截断法</strong><br> 硬截断法（<code>hard cutoff</code>）也称简单展开法（<code>simple augmentation</code>），是指根据通过样本构建 <code>KGB</code> 模型，利用 <code>KGB</code> 模型对拒绝样本预测打分，设置截断阈值，高于该阈值的样本认定为正样本，低于该阈值的样本认定为负样本，最终将有真实标签的通过样本和推测得到的伪标签的拒绝样本合并，构建最终模型。</li><li><strong>模糊展开法</strong><br> 模糊展开法（<code>fuzzy augmentation</code>）是指根据通过样本构建 <code>KGB</code> 模型，利用 <code>KGB</code> 模型预测拒绝样本的逾期概率，然后将每条拒绝样本复制为不同类别、不同权重的两条，每条通过样本的权重为 <code>1</code>，最终将有真实标签的通过样本和推断得到伪标签的决绝样本合并，考虑不同样本的权重，构建最终模型。</li><li><strong>重新加权法</strong><br> 重新加权法（<code>reweighting</code>）是指根据通过样本构建 <code>KGB</code> 模型，利用 <code>KGB</code> 模型对全部样本预测打分，然后分箱统计不同分数段的通过样本数和拒绝样本数，计算每箱的权重，添加通过样本不同分数段的权重，然后利用通过样本构建最终模型。<br> 权重方式计算如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = (Accepti + Rejecti) / Accepti</span><br></pre></td></tr></table></figure> 其中 <code>Accepti</code> 表示第 <code>i</code> 分箱通过样本数，<code>Rejecti</code> 表示第 <code>i</code> 分箱拒绝样本数。</li><li><strong>外推法</strong><br> 外推法（<code>extrapolation</code>）是指根据通过样本构建 <code>KGB</code> 模型，利用 <code>KGB</code> 模型对拒绝样本预测打分，然后对通过样本模型分等频分箱，统计每箱的逾期率，将逾期率的 <code>KI</code> 倍数设为拒绝样本相同分箱的期望逾期率，并按照期望逾期率对拒绝样本随机标记伪标签，最终和通过样本一起构建最终模型。</li><li><strong>迭代再分类法</strong><br> 迭代再分类法（<code>iterative reclassification</code>）是指通过多次迭代，使得最终模型参数逐步趋于稳定。迭代再分类的具体做法：首先利用硬截断法获得拒绝样本的伪标签，然后训练得到最终模型，并利用最终模型重新给拒绝样本预测打分，更新伪标签，直到任何一个有价值的指标收敛。<br> 迭代再分类法利用启发式思想，经过多次迭代，可以保证修正偏差后的最终模型的效果。其中设置的迭代终止条件可以是任何一个有价值的指标收敛。</li><li><strong>双变量推断法</strong><br> 双变量推断法（<code>bivariate inference</code>）是指首先分别利用通过样本构建 <code>KGB</code> 模型和全部样本构建 <code>AR</code> 模型，然后利用两个模型分别对拒绝样本预测打分，并将加权求和的结果作为最终预测打分，然后利用 <em>“外推法”</em> 设置伪标签，最终和通过样本一起构建模型。<br> 拒绝样本的模型分计算公式如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Rejects = a * ARs + (1-a) * KGBs</span><br></pre></td></tr></table></figure> 其中 <code>ARs</code> 是 <code>AR</code> 模型的打分，<code>KGBs</code> 是 <code>KGB</code> 模型的打分，<code>0 &lt; a &lt; 1</code> 表示权重。</li></ul></li></ul></li></ol><hr><h3 id="常见风控建模算法"><a href="#常见风控建模算法" class="headerlink" title="常见风控建模算法"></a>常见风控建模算法</h3><p>风控建模可用的算法有很多种，但是所有算法遵循的原则都是类似的，即从数据中提取一种无限接近于<strong>数据内部真实联系的映射关系</strong>。换言之就是给定特征集 <code>x</code> 和标签 <code>y</code>，存在某种未知的映射关系 <code>f</code>，使得 <code>x</code> 和 <code>y</code> 能够一一对应，即 <code>y = f(x)</code>。抽象出来映射关系 <code>f</code> 是数据内部存在的真实联系。<br>预测结果 <code>h(x)</code> 和标签 <code>y</code> 之间的误差用损失函数来衡量。其中<strong>损失函数</strong>、<strong>代价函数</strong>和<strong>目标函数</strong>是三个非常容易混淆的概念，其含义分别如下：</p><ul><li>损失函数（<code>loss function</code>）：定义在单个样本上，是指<strong>一个样本的误差</strong>。</li><li>代价函数（<code>cost function</code>）：定义在整个训练集上，是指<strong>所有样本误差的平均</strong>，也就是所有损失函数值的平均。</li><li>目标函数（<code>object function</code>）：是指<strong>最终需要优化的函数</strong>，一般来说在代价函数的基础上，增加了正则项。</li></ul><p>各种算法通过构造不同的假设空间定义目标函数，通过优化目标函数在假设空间中寻找最优的映射关系 <code>h</code>，其本质还是<strong>最优化问题</strong>。</p><p><small>这里对下面的算法不会有太深的研究，有兴趣的可以自行学习。</small></p><h4 id="基础学习算法"><a href="#基础学习算法" class="headerlink" title="基础学习算法"></a>基础学习算法</h4><ol><li><p>逻辑斯谛回归</p></li><li><p>支持向量机</p></li><li><p>决策树</p></li></ol><h4 id="集成学习算法"><a href="#集成学习算法" class="headerlink" title="集成学习算法"></a>集成学习算法</h4><ol><li><p>随机森林</p></li><li><p><code>GBDT</code></p></li><li><p><code>XGBoost</code></p></li><li><p><code>LightGBM</code></p></li></ol><h4 id="深度学习算法"><a href="#深度学习算法" class="headerlink" title="深度学习算法"></a>深度学习算法</h4><ol><li><p>深度神经网络</p></li><li><p>卷积神经网络</p></li></ol><hr><h3 id="模型体系搭建"><a href="#模型体系搭建" class="headerlink" title="模型体系搭建"></a>模型体系搭建</h3><p>信贷产品生命周期主要包括<strong>营销</strong>、<strong>贷前</strong>、<strong>贷中</strong>和<strong>贷后</strong>四个阶段，每个阶段都有风险控制的需求。</p><ul><li>贷前阶段主要获取并筛选客户流量，以营销响应模型和流量筛选模型为主。</li><li>贷前阶段一般会通过部署多种风控规则和模型来识别风险，风控规则可以识别小部分高风险客户，大部分仍然需要模型预测。贷前阶段的模型主要有反欺诈模型、信用风险模型。</li><li>贷中阶段会通过贷中行为预测客户的风险情况，制订账户管理等策略，同时利用交易风险模型对客户贷中的提现、消费等交易行为进行风险判断。</li><li>贷后阶段通过还款预估模型预测客户的还款可能性，制订合理的催收策略，提升还款率；利用失联预估模型预测客户的失联概率，优化催收策略。</li></ul><p>完整的风控体系需要对信贷产品生命周期的每一个阶段进行有效的风险控制，以下为信贷产品生命周期的不同阶段包含的主要风控模型。<br><img src="https://s2.loli.net/2022/12/22/1AXbaVD75ieRCBp.png" alt="risk_3_4.jpg"></p><h4 id="营销阶段"><a href="#营销阶段" class="headerlink" title="营销阶段"></a>营销阶段</h4><p>营销是业务开展的第一步，获取优质客群是营销阶段得到重要目标，通常是利用营销响应模型评估客户的响应概率，再利用流量筛选模型选择目标客群。</p><ol><li><p>营销响应模型<br>营销响应模型是通过营销时对客户进行评估来预测客户响应概率的模型。营销场景主要包含<strong>纯新客户名单营销</strong>、<strong>流失客户营销召回</strong>和<strong>存量客户的交叉销售</strong>等，采用营销响应模型将客户分级，对于响应概率高的客群，业务人员可以重点营销，即可以大幅提升营销效率，降低营销成本。</p></li><li><p>流量筛选模型<br>流量筛选模型也称前筛模型，用来识别资质明显差的客群。拦截这部分高风险客群是风险控制的第一项任务同时节省后续申请授信环节的数据成本。其中流量筛选模型一般用于与其他流量提供方合作的场景中。</p></li></ol><h4 id="贷前阶段"><a href="#贷前阶段" class="headerlink" title="贷前阶段"></a>贷前阶段</h4><p>贷前是有效控制风险的重要阶段，因为一旦高风险客户通过贷前授信，后期在贷中和贷后阶段会面临被动局面，因此贷前阶段一般通过部署多种模型来尽可能多识别风险。</p><ol><li><p>反欺诈模型<br>反欺诈模型用于识别欺诈风险高的客户。常见的欺诈类型有：<strong>第一方欺诈</strong>，即利用不实信息欺诈，欺诈者故意提供虚假申请信息以获得授信审批；<strong>第三方欺诈</strong>，即冒用他人身份欺诈，欺诈者偷取他人信息，以他人名义申请。<br>因此在实践中可通过<strong>身份验证</strong>、<strong>活体识别</strong>和<strong>第三方数据验证</strong>等方式识别欺诈，并且可以通过反欺诈模型综合多维度信息进行识别。</p></li><li><p>信用风险模型<br>信用风险模型是通过对信贷申请人的信用状况进行评估来预测其未来逾期概率的模型，即<strong>申请评分卡</strong>，也称 <strong><code>A</code> 卡（<code>Application scorecard</code>）</strong>。信用风险模型有重要作用，该模型的预测评分不仅可以审批准入，还可以用于额度和费率的设定。</p></li></ol><h4 id="贷中阶段"><a href="#贷中阶段" class="headerlink" title="贷中阶段"></a>贷中阶段</h4><p>贷中阶段可以获得客户交易、还款和 <code>APP</code> 使用等行为数据，通过这些数据可以全面、客观、准确地预测客户的未来表现，从而制订有针对性的贷中管理策略。</p><ol><li><p>贷中行为模型<br>贷中行为模型即<strong>行为评分卡</strong>，也称 <strong><code>B</code> 卡（<code>Behavior scorecard</code>）</strong>是根据客户借款后的行为表现，预测其未来逾期概率的模型。</p></li><li><p>交易风险模型<br>交易行为模型是在已经获得授信的客户发生支用或消费交易等行为时进行风险预估的模型。该模型用于拦截高风险交易，及时止损，交易风险模型与信用风险模型类似，只是交易风险模型在获得的特征维度上会包含更多的贷中行为数据。</p></li></ol><h4 id="贷后阶段"><a href="#贷后阶段" class="headerlink" title="贷后阶段"></a>贷后阶段</h4><p>贷后阶段的模型根据客户放贷后的行为表现，预测客户的还款概率。原始催收表现为尽可能多地联系客户，然后依靠客户近期的逾期行为调整策略。</p><ol><li><p>还款预估模型<br>还款预估模型即<strong>催收评分卡</strong>，也称 <strong><code>C</code> 卡（<code>Collection scorecard</code>）</strong>是预测已逾期的客户在未来一段时间的还款概率的模型。通常逾期客户在早期还款的可能性较大，越往后还款越困难，因此可以利用还款预估模型制订差异化的催收策略，提高还款率。</p></li><li><p>失联预估模型<br>失联预估模型预测已逾期的借款人在未来一段时间是否会失联。在催收后期，通常会出现无法联系到客户的情况出现，这对催收有很大的影响，如果可以在早期获得客户失联的可能性，即可以对催收工作的开展提供指导。</p></li></ol><hr><h3 id="术语介绍"><a href="#术语介绍" class="headerlink" title="术语介绍"></a>术语介绍</h3><ol><li><p>样本、特征、标签<br>样本是构建机器学习模型时需要一个数据集。<br>特征是用来表征关注对象的特点或属性的一系列数据。<br> <strong>标签</strong>是机器学习模型将要学习和预测的目标。</p></li><li><p>账龄<br><strong>账龄（<code>Month on Book, MOB</code>）</strong>是指多期信贷产品从首次放款起所经历的月数。<br>通常使用 <code>MOBn</code> 表示账龄，以月末时间点来看放款日经历 <code>n</code> 个完整的月数，具体如下：</p><ul><li><code>MOB0</code>：放款日到当月月底，观察时间点为放款当月月末。</li><li><code>MOB1</code>：放款后第二个月，观察时间点为放款第二个月月末。</li><li>以此类推 <code>MOB</code> 的最大值取决于信贷产品的 <em>账期</em>。</li></ul></li><li><p>逾期<br>逾期的概念有以下几种：</p><ul><li><strong>逾期天数（<code>Days Past Due, DPD</code>）</strong>：实际还款日与应还款日的相差天数。</li><li><strong>首期逾期天数（<code>First Payment Deliquency, FPD</code>）</strong>：分期产品中第一期实际还款日与应还款日的相差天数。</li><li><strong>逾期期数</strong>：贷款产品中客户的逾期期数，也指将逾期期数按区间划分后的逾期状态。<br>  通常以 <code>30</code> 天为区间划分，英文字母 <code>M</code> 来表示，具体如下：<ul><li><code>M0</code>：当前未逾期。</li><li><code>M1</code>：逾期一期，或逾期 <code>1~30</code> 天。</li><li><code>M2</code>：逾期两期，或逾期 <code>31~60</code> 天。</li><li>以此类推，<code>M2+</code> 表示逾期两期以上，或逾期天数未 <code>61</code> 天以上，和 <code>DPD60+</code> 含义一致。</li></ul></li><li><strong>逾期率</strong>：分为订单逾期率和金额逾期率。订单逾期率是指逾期订单数与总放款订单数的比值。金额逾期率是指逾期金额与总放款金额的比值。</li></ul></li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;风控模型&lt;/strong&gt;是风控系统的核心，应用模型进行风险决策是识别风险的主要途径，也是控制风险的重要方法。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Risk Control" scheme="https://blog.vgbhfive.cn/tags/Risk-Control/"/>
    
  </entry>
  
  <entry>
    <title>智能风控-特征画像体系</title>
    <link href="https://blog.vgbhfive.cn/%E6%99%BA%E8%83%BD%E9%A3%8E%E6%8E%A7-%E7%89%B9%E5%BE%81%E7%94%BB%E5%83%8F%E4%BD%93%E7%B3%BB/"/>
    <id>https://blog.vgbhfive.cn/%E6%99%BA%E8%83%BD%E9%A3%8E%E6%8E%A7-%E7%89%B9%E5%BE%81%E7%94%BB%E5%83%8F%E4%BD%93%E7%B3%BB/</id>
    <published>2022-11-12T15:07:59.000Z</published>
    <updated>2023-02-06T12:37:58.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>特征挖掘是从原始数据构造特征的过程。<br>特征是数据和模型之间的纽带，数据和特征决定机器学习的上限，而模型和算法只是无限逼近这个上限。<br>特征挖掘的完整流程包含<strong>原始数据分析</strong>、<strong>数据清洗</strong>、<strong>中间数据集构建</strong>、<strong>特征设计和生成</strong>、<strong>特征评估</strong>和<strong>特征的上线、监控、维护和下线</strong>。</p><span id="more"></span><p><small>在实际运行中，特征挖掘不一定是严格线性的，某些环节可能存在反复多次进行的情形。</small><br><img src="https://s2.loli.net/2022/11/13/WMcXFUxuZ9lzKtL.png" alt="risk_2_1.jpg"></p><hr><h3 id="挖掘方法论"><a href="#挖掘方法论" class="headerlink" title="挖掘方法论"></a>挖掘方法论</h3><p>业务中的数据类型繁多，不同的数据类型需要采用不同的方法进行挖掘，下面是通用的特征挖掘方法。</p><h4 id="原始数据分析"><a href="#原始数据分析" class="headerlink" title="原始数据分析"></a>原始数据分析</h4><p>原始数据分析是为了提取原始数据中有用的信息而对其加以分析的过程。原始数据分析的目的是对数据价值进行初步判断，避免错误使用数据，为后续的数据清洗与处理提供依据，最大化利用原始数据。<br>原始数据分析可以利用从<strong>数据流转分析</strong>、<strong>数据质量分析</strong>和<strong>数据时效性分析</strong>方面进行。</p><ol><li><p>数据流转分析<br>数据流转分析是指对数据来源、中间处理和最终存储环节的数据进行分析。通过数据流转分析，可以了解数据在业务流程中的演变过程，从而全面认识数据并发现潜在问题。数据流转分析可以从<strong>业务逻辑角度</strong>和<strong>实际数据角度</strong>分别进行：</p><ul><li>业务逻辑角度<br>  从业务逻辑角度分析是基于业务梳理出数据的产生、中间处理、最终存储和数据的更新机制。业务逻辑分析主要是为了整体把控底层数据的完整生命周期变化情况，也是为了补充数据层面分析无法获知的信息。</li><li>实际数据角度<br>  从实际数据角度分析是指利用业务中产生的实际数据与理解的业务逻辑进行交叉比对，并且对其变化进行详细分析。这样就可以发现实际数据和业务逻辑中不一致的地方并加以确认和纠正，同时可以发现数据源的稳定性问题、计算问题和存储问题等异常，以此来保证数据的准确性和完整性。</li></ul></li><li><p>数据质量分析<br>数据质量分析可以从数据的<strong>覆盖率</strong>、<strong>规范性</strong>和<strong>准确性</strong>方面进行。</p><ul><li>覆盖率是指数据中非空记录的占比。</li><li>规范性是指数据取值是否符合一定的规范。</li><li>准确性是指数据接近真实值的程度。</li></ul></li><li><p>数据时效性分析<br>数据时效性分为<strong>采集时效性</strong>和<strong>获取时效性</strong>两个方面。获取时效性不仅受采集时效性影响，而且数据传输、中间处理和存储都会影响。</p><ul><li>采集时效性是指数据从产生到采集的时间间隔。</li><li>获取时效性是指从数据产生到风控生产系统中实际获取的时间间隔。</li></ul></li></ol><h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p>数据清洗一般包含<strong>重复数据处理</strong>、<strong>缺失数据处理</strong>、<strong>异常值处理</strong>和<strong>时间数据处理</strong>。数据清洗是为了数据质量达到特征挖掘使用的标准，避免因数据质量问题而导致特征挖掘阶段，甚至建模阶段的返工。</p><ol><li><p>重复数据处理<br>业务流程中产生的数据一般都需要完整保存，在维持数据完整性的同时，可能会引入重复数据，至此需要判断重复数据是否有业务含义，如果有业务含义则需要挖掘与业务含义相关的特征；若没有业务意义则需要冗余处理。<br>冗余数据处理包括直接过滤和整合应用：直接过滤是指随机选取一条数据，丢弃其他；整合应用则是将多条数据整合或校准之后形成完整和可靠的记录，之后记录并使用。<br><img src="https://s2.loli.net/2022/11/21/9gY8OB56MhuiHKZ.png" alt="risk_2_8.jpg"></p></li><li><p>缺失数据处理<br>在特征挖掘阶段根据数据确实情况，尽早发现隐藏的数据问题，有很多的机会采取措施以降低甚至消除数据缺失的影响，产出稳定的特征。<br>缺失数据处理需要先判断数据是否未正常缺失，再根据判断结果采取合适的处理方式，如填充缺失值、修复数据和丢弃数据等。<br><img src="https://s2.loli.net/2022/11/21/QcKxwOSaJUI1YfZ.png" alt="risk_2_9.jpg"></p></li><li><p>异常值处理<br>异常值即数据中存在的不合理的值，同时异常值也称为<strong>离群点</strong>。对于发现的异常值，一方面可以及时研究数据是否可以修复；另一方面在特征开发阶段，可以增加对应处理逻辑，降低特征异常值出现的概率。<br>异常值的处理方式通常如下：</p><ul><li>删除含有异常值的记录。</li><li>将异常值视为缺失，使用缺失值填充的方式来处理。</li><li>用特定值（如平均值、中位数或固定值等）来填充。</li></ul></li><li><p>时间数据处理<br>特征挖掘通常使用时间窗口来切分数据，而时间切分错误就会导致整个模块绝大部分特征无法使用。<br>时间数据处理主要包含两个方面：<strong>对时间格式做统一的规范化处理</strong>；<strong>对数据进行时间维度上的过滤</strong>。<br>时间格式的规范化需要注意以下三个方面：</p><ul><li>统一时区。</li><li>统一时间格式。</li><li>选择合适的时间跨度。</li></ul><p> 数据在时间维度上的过滤主要从两个方面进行：</p><ul><li>避免引入未来数据。</li><li>避免时间未对齐问题。</li></ul></li></ol><h4 id="中间数据集构建"><a href="#中间数据集构建" class="headerlink" title="中间数据集构建"></a>中间数据集构建</h4><p>中间数据集构建是将清洗完成的原始数据初步处理成结构化的数据或者适用于某些特定算法的数据格式。<br>结构化数据是高度组织、格式整齐的数据，通常是可以用统一的结构（二维表）来表达的数据。结构化数据一般使用关系型数据库且以行为单位表示，与之对应的是非结构化数据，非结构化数据是数据结构不规则或不完整，没有预定义的数据模型，不方便用数据库二维逻辑表来表达的数据，通常存储在非关系型数据库中。</p><ol><li><p>结构化数据<br>结构化数据本身是适合特征计算的，但是需要注意数据本身的粒度。风控业务中的原始数据按照粒度从大到小依次为渠道、客户、借款、还款等，另外在不同的场景下特征挖掘需要不同的数据粒度。<br>结构化数据应用时一般会遇到两种情况：一种情况是假设数据是合适的粒度，那就可以直接作为特征在模型或规则中应用；另一种情况则是数据需要经过聚合汇总才能转换成建模可用的粒度。</p><ul><li>客户行为埋点数据<br>  同一个客户存在多条埋点数据，并且客户每次登录的操作序列可能不同，业务通常以客户每次登录的维度来生成客户行为埋点数据，以客户 <code>ID</code> 作为本次登录的所有埋点行为数据的唯一标识。</li><li>客户历史订单数据<br>  业务一般以订单维度保存订单数据，每个客户可能存在多个订单数据。将历史订单数据整理成包含客户 <code>ID</code> 的中间数据集，后续挖掘特征时可以基于中间数据集进行对比、分组、聚合等。</li><li>客户账单数据<br>  挖掘贷后特征应用于贷后 <code>C</code> 卡模型需要订单粒度的特征，中间数据集为账单粒度的数据。</li></ul></li><li><p>文本数据<br>文本数据就是用文本形式表示的数据。文本数据的特征挖掘方法常用的有以下三种：</p><ul><li>提取关键字并将文本数据转化为结构化数据，再进行特征挖掘。<br>  具体做法是构建关键字集合，再根据关键字在每条文本中出现的次数构建中间数据集。关键字集合的构建一般是基于业务经验，并结合原始数据的分析。</li><li>基于机器学习或深度学习算法从文本中提取特征。<br>  首先对每条文本做清洗和预处理，包括过滤标点符号、特殊字符、删除停用词；然后做分词形成文本序列；最后合并一个客户的多条文本序列并作为输入。</li><li>使用文本分类算法训练文本模型，然后将模型输出的概率值作为特征使用。</li></ul></li><li><p>关系网络数据<br>关系网络数据通常是指用来描述实体之间关系的数据。关系网络数据中的实体可能存在多种类型，实体之间也可能存在多种关系。处理关系网络数据通常分为以下两步：</p><ul><li>从复杂的现实关系网络中抽取有价值的实体和关系并将其表达为图结构。<br> <img src="https://s2.loli.net/2022/11/20/QNg4hixtTEmZycs.png" alt="risk_2_2.jpg"></li><li>构建中间数据集，转化为结构化数据或者构建适用于图算法的中间数据。<br>  在将关系网络数据用于传统特征挖掘时，构建中间数据集通常需要三步：计算所有节点的特征，可以使用结构化数据特征挖掘的办法；针对每个节点抽取子图结构，基于计算效率的考虑，抽取子图结构目前只针对一度和二度邻居节点进行；将所有节点的特征按照子图中心节点来整理，形成中间数据集。</li></ul></li></ol><h4 id="特征设计与生成"><a href="#特征设计与生成" class="headerlink" title="特征设计与生成"></a>特征设计与生成</h4><p>在特征的设计与生成阶段会完成从原始数据到特征的转化，对于那些取自规范、含义清晰、汇总力度符合需求的字段可以直接当作特征输出，其他的需要进行汇总计算以产生新特征。<br>在风控业务中特征设计最重要的点在于客户风险的区分度上，而特征设计通过采用不同的方法：<strong>基于业务逻辑生成特征</strong>；<strong>半自动化方法生成特征</strong>；<strong>基于智能算法生成特征</strong>。<br><img src="https://s2.loli.net/2022/11/20/nIjHyJu2vapd1b7.png" alt="risk_2_3.jpg"></p><h4 id="特征评估"><a href="#特征评估" class="headerlink" title="特征评估"></a>特征评估</h4><p>特征评估是指选取特定的数据集对特征进行综合评估，以决定对特征模块的下一步处理方式。特征评估一般包括<strong>覆盖率</strong>、<strong>离散度</strong>、<strong>时间相关性</strong>、<strong>稳定性</strong>和<strong>效果</strong>等方面。</p><ol><li><p>特征覆盖率<br>特征覆盖率检查首先可以检查出覆盖率较低的特征，避免输出；其次可以发现覆盖率异常的特征，进而反推出检查原始数据字段是否有之前的问题。<br>特征覆盖率检查的实现一般是比较简单的，直接计算样本集中非空特征占比即可。</p></li><li><p>特征离散度<br>特征离散度是指特征值分布的离散程度。<br>计算特征离散程度时通常使用变异系数，与极差、方差和标准差相比，变异系数不受数据量纲的影响，但是只在平均值不为 <code>0</code> 时才有意义。</p></li><li><p>特征时间相关性<br>特征时间相关性衡量特征值与时间的相关性。特征时间相关性检查能发现一些与时间强相关当无意义的特征。<br><small>实际中 <strong><code>Pearson</code> 相关系数</strong>大于 <code>0.8</code>，表明特征与时间存在强相关性，应当谨慎使用。</small></p></li><li><p>特征稳定性<br>特征稳定性主要使用 <strong><code>PSI</code>（<code>Population Stability Index</code>，群体稳定性指标）</strong>来表示。<br>在实际风控业务中，对于 <code>PSI &gt; 0.1</code> 的特征需要关注并分析原因，然后根据原因是否可接受来决定特征是否继续使用。<br><small>在样本量很小的时候，<code>PSI</code> 的波动情况可能会因为随机情况导致不能表示真实的业务情况。</small></p><table><thead><tr><th><code>PSI</code> 范围</th><th>稳定性</th><th>表现&#x2F;建议</th></tr></thead><tbody><tr><td><code>0</code> ~ <code>0.1</code></td><td>好</td><td>特征基本稳定</td></tr><tr><td><code>0.1</code> ~ <code>0.25</code></td><td>略不稳定</td><td>持续监控后续变化</td></tr><tr><td>大于 <code>0.25</code></td><td>不稳定</td><td>剧烈变化，分析原因，找到应对方案</td></tr></tbody></table></li><li><p>特征效果<br>特征效果通常使用 <code>IV</code> 值来衡量。<br>在评价特征区分度时，为了消除样本数据本身差异的影响，可以预先选择基准特征作为参考。基准特征有两种选择方式：</p><ul><li>选择效果已知的特征，在同一个数据集上对比两个特征的效果。</li><li>在特征子模型中引入随机变量，查看随机变量的重要性排序，方便评估特征模块的整体效果。<br> <small>重要性排在随机变量之后的特征可以被视为无区分度而不输出。</small></li></ul></li></ol><h4 id="特征上下线"><a href="#特征上下线" class="headerlink" title="特征上下线"></a>特征上下线</h4><p>特征开发完成并评估有效后会部署上线，在上线运行期间需要持续监控，当数据源不可用或者特征版本更新时，就会涉及到特征下线操作。</p><ol><li><p>特征上线<br>特征上线一般分为两种方式：</p><ul><li>实时计算部署，即接受计算请求后在线获得原始数据并实时计算特征。实时计算的特征需要同时在线上系统和离线回溯系统中部署。</li><li>离线批量计算方式部署，即离线计算好所有客户的特征，并推送到线上等待调用。为此需要保证特征更新机制正常运行，上线前需要进行充分的测试。</li></ul><p> 特征上线之后通常需要先<strong>空跑</strong>，而不先应用于模型或规则。在线上积累足够多的样本之后，此时需要进行线上验证。上线验证通常包含三个方面：</p><ul><li>数据源接入验证<br>  首先确认特征依赖的数据源是否已上线或同时上线；其次检查所有数据是否已正常接入且数据格式正确；接下来确认数据调用位置是否正确；最后确认数据源的更新频率是否符合预期。</li><li>特征统计分析<br>  主要包含三个方面：特征线上维度、覆盖率、缺失值填充方式和分布是否符合预期；特征监控配置是否正确；特征离线回溯是否正常运行。</li><li>特征稳定性验证<br>  取近期的线下样本计算特征，然后与线上样本特征计算 <code>PSI</code>。</li></ul></li><li><p>特征下线<br>特征下线通常发生在数据不可用或特征升级后新版本特征已经覆盖旧版本时，及时下线特征可以节省线上资源。<br>特征的下线需要注意以下几点：</p><ul><li>无策略或特征引用此特征。</li><li>不影响原始数据落表。</li><li>若后续评估特征效果，则需要判断是否积累足够的样本。</li></ul></li></ol><hr><h3 id="挖掘特征"><a href="#挖掘特征" class="headerlink" title="挖掘特征"></a>挖掘特征</h3><h4 id="特征衍生"><a href="#特征衍生" class="headerlink" title="特征衍生"></a>特征衍生</h4><p>在现有特征的基础之上，可以使用 <code>GBDT</code>、神经网络等算法构建模型，而模型的中间产出或输出结果作为新的特征。</p><ol><li><p>树模型算法<br>使用已有特征训练 <code>GBDT</code> 模型，再利用模型中的树的叶子节点构造新特征，此思路源于 <code>Facebook</code> 发表的 <code>Practical Lessons from Predicting CLicks on Ads at Facebook</code> 论文。<br>按照这种思路构造的新特征向量取值是 <code>0</code> 或 <code>1</code>，向量中的每个元素对应 <code>GBDT</code> 模型中的树的叶子节点，特征长度等于集成模型中所有树的叶子节点之和。当一个样本点通过某棵树最终落在其一个叶子节点上时，新特征向量中的这个叶子节点对应的元素取值为 <code>1</code>，而这棵树的其他叶子节点对应的元素取值为 <code>0</code>。</p></li><li><p>聚类算法<br>聚类算法在特征挖掘中的主要应用是基于已有特征进行样本聚类，并将聚类结果作为新特征。<br>算法原理：聚类算法是一种无监督算法。<code>K-means</code> 是典型的聚类算法，原理如下：</p><ul><li>初始时，随机选择 <code>k</code> 个质心。</li><li>把每个观测划分到离他最近的质心，并与质心形成新的类。</li><li>重新计算每个类的质心。</li><li>重复第二、三步骤。迭代停止条件为质心不变或达到最大迭代次数。</li></ul></li></ol><p> 聚类完成之后，可以针对最终生成的 <code>N</code> 个聚类算法，输出样本 <em>是否属于聚类 <code>X</code></em> 特征或样本 <em>与聚类质心得距离</em> 特征。</p><h4 id="文本特征挖掘"><a href="#文本特征挖掘" class="headerlink" title="文本特征挖掘"></a>文本特征挖掘</h4><p>文本特征挖掘是指把文本数据转换为特征。文本数据加工成特征的方法包括常规的提取关键词和直接使用文本挖掘类算法将文本转换为向量。</p><ol><li><p>文本特征提取方法<br>词袋（<code>bag of words</code>）模型是最初的将文本表示成向量的方法。<br>词袋模型将文本看作一系列单词的集合，即把一段文本当作一个 <em>袋子</em>，里面装的是 <em>单词</em>。词袋模型一般需要收集一些文本，并将它们作为模型建立的基础，而这些文本被称为语料（<code>corpus</code>），经过筛选、加工和标注等处理后，大批语料构成的数据库称为 <strong>语料库</strong>。<br>词袋模型的基本原理是先构建词典，再根据文本中的单词在词典中出现的频率生成文本的向量，生成的向量与单词在原文本中出现的次序没有关系。生成向量主要有两种方法：基于词频统计的方法和基于 <code>TF_IDF</code>（<code>Term Frequency - Inverse Document Frequency</code>）算法的方法。前者简单统计文本中的单词出现的次数，后者综合 <em>考虑</em> 单词出现的频率和在整个语料库中的 <em>稀有程度</em>。<br><small><code>TF_IDF</code> 等于 <code>TF</code> 和 <code>IDF</code> 的乘积，其中 <code>TF</code> 表示单词出现的频率，即某个单词在当前文本中出现的次数；<code>IDF</code> 是逆文档频率，<code>DF</code> 表示语料库中包含某个单词的文档的数目，<code>IDF</code> 即反映某个单词在整个语料库中的重要性。</small></p></li><li><p>文本分类算法<br>再除了将文本表达为向量方式外，还有一些直接基于文本进行分类的算法，算法会输出一个概率，这个概率可以在后续的模型中使用。</p><ul><li>朴素贝叶斯算法</li><li><code>fastText</code> 算法</li></ul></li></ol><h4 id="图特征挖掘"><a href="#图特征挖掘" class="headerlink" title="图特征挖掘"></a>图特征挖掘</h4><p>之前介绍了使用邻接矩阵表示图结构，当邻接矩阵通常是 <em>高维且稀疏</em> 的，为了利用图的优势并构建有效的机器学习模型，需要得到高效的关系网络数据表示方法，这正是<strong>图表示学习</strong>的范畴。<br><small><strong>表示学习</strong>是指机器学习模型自动学习数据中隐含的有效特征。</small></p><p>图表示学习也称<strong>图嵌入（<code>graph embedding</code>）</strong>，其主要目标是将图转换为 <em>低维且稠密</em> 的向量，并近可能保持图原有的拓扑关系。图表示学习生成的图特征向量可以作为图任务学习的输入。<br>图表示学习主要包含三种方法：</p><ul><li>基于矩阵分解的方法<br> 通过对邻接矩阵进行矩阵分解，将节点转换到低维向量空间，同时保留图结构。</li><li>基于随机游走的方法<br> 借鉴词向量的表示方法将图的节点看作词，将在图中随机游走而产生的序列看作句子，然后借助 <code>Word2Vec</code> 算法学习得到图节点的表示，该方法使用的典型算法有 <code>DeepWalk</code> 和 <code>Node2Vec</code>。</li><li>基于深度学习的方法<br> 基于图神经网络的图表示学习，可以用于图表示学习的图神经网络算法有图卷积神经网络、图自编码器和图注意力网络。</li></ul><ol><li><p>基于随机游走的方法<br>基于随机游走（<code>random walk</code>）的方法将在图中随机游走而产生的序列看作句子，之后借助 <code>Word2Vec</code> 算法学习得到图节点的表示。在随机游走序列的生成方面，共有两种不同的思路：</p><ul><li><code>DeepWalk</code> 算法。</li><li><code>Node2Vec</code> 算法。</li></ul></li><li><p>图卷积神经网络<br>图卷积神经网络是图神经网络（<code>Graph Neural Network, GNN</code>）的一种，是将卷积神经网络应用于图表示学习而得到的。<br>卷积神经网络处理的图像数据是整齐的矩阵格式，转换成关系网络结构来看其节点的邻居数量是固定的；而图网络属于非欧几里得空间结构，节点的邻居数量不固定。因此在欧几里得空间内，不能直接将用固定大小的卷积核抽象图像像素特征的操作迁移到图结构，其本质是找到适用于图的可学习卷积核；而图卷积神经网络则是以图卷积层为主体，堆叠多层的神经网络模型。</p></li></ol><hr><h3 id="特征画像体系"><a href="#特征画像体系" class="headerlink" title="特征画像体系"></a>特征画像体系</h3><p>风控特征画像是从多个角度描述客户风险的工具。为了描述客户风险需要对客户有全面准确的认识，其中风控特征画像可以从多个维度尽量全面地描述客户在多个维度的风险属性，其中维度的细分更加有助于准确地刻画每个具体维度的差异，从而达到准确认识客户的目的。</p><h4 id="营销阶段"><a href="#营销阶段" class="headerlink" title="营销阶段"></a>营销阶段</h4><p>在营销特征数据中，对于历史存量客户，包含有客户基本信息、历史申请信息和多头借贷信息；对于新客户，数据较少，当包含有浏览行为数据、客户的基本部分信息和第三方数据。<br>客户基本信息主要是客户在历史申请时自填的信息，通常包含客户本人的学历、年龄、性别、从事行业和居住地等；对于多头借贷信息，通常包含从第三方数据中的客户在多个机构的申请、放款和预期情况；历史申请记录是指客户在本机构的历史申请情况。<br><img src="https://s2.loli.net/2022/11/20/XR56qJrCynM2hBu.png" alt="risk_2_4.jpg"></p><h4 id="贷前阶段"><a href="#贷前阶段" class="headerlink" title="贷前阶段"></a>贷前阶段</h4><p>贷前特征画像主要应用在反欺诈、信用风险评估和风险定价阶段，主要考虑的目标是客户的还款能力、守约概率等。<br><img src="https://s2.loli.net/2022/11/20/dJknRCaK4Y1shfM.png" alt="risk_2_5.jpg"></p><h4 id="贷中阶段"><a href="#贷中阶段" class="headerlink" title="贷中阶段"></a>贷中阶段</h4><p>在数据维度上，贷中特征画像可以使用贷前特征画像的所有数据，另外还能使用当前未完结（或已完结）订单的数据、贷中行为埋点数据、审批结果和还款提醒数据等。<br><img src="https://s2.loli.net/2022/11/20/GXLlVS8JsRxYfFa.png" alt="risk_2_6.jpg"></p><h4 id="贷后阶段"><a href="#贷后阶段" class="headerlink" title="贷后阶段"></a>贷后阶段</h4><p>贷后特征画像主要反映客户在贷后的违约风险（主要体现客户还款的意愿和能力），可以将其应用于贷后风险模型或规则中。<br><img src="https://s2.loli.net/2022/11/20/deDc1bOEYiUxT5l.png" alt="risk_2_7.jpg"></p><hr><h3 id="监控和异常处理"><a href="#监控和异常处理" class="headerlink" title="监控和异常处理"></a>监控和异常处理</h3><p>特征监控是指监控特征的准确性、有效性、稳定性和一致性，以及特征依赖的原始字段的分布情况。通过特征监控能够及时发现原始字段或特征分布的偏移，以便分析原因并采取合理的方式来处理，避免特征异常带来的损失。</p><h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><ol><li><p>一致性<br>特征的一致性监控是指监测<strong>特征离线回溯与线上调用是否一致</strong>，以及线上不同时间点的调用是否一致，其通常包括特征的<strong>线上与线下一致性</strong>，以及特征的<strong>前后一致性</strong>。</p><ul><li>特征的线上与线下不一致是指对同一个客户，基于相同业务时间，离线回溯计算的特征值和线上调用特征计算的结果不一致。</li><li>特征前后不一致是指对于同一个客户，基于相同业务时间点，在不同时间点回溯计算特征时会得到不同结果。</li><li>特征的一致性监控方法通常定期采样一定比例的客户离线回溯特征，并将其与线上调用特征进行对比。</li></ul><p> <small>特征监控以小时、天、周或月为周期，监控结果可采用邮件方式反馈和可视化报表方式展示。</small></p></li><li><p>原始字段分布<br>原始字段分布监控是指监测原始字段分布的变化情况。原始字段的分布变化通常会带来相关特征取值分布的变化，对原始字段监控可以进行监控可以直接、迅速地发现潜在的数据问题。<br>原始字段分布监控包含覆盖度监控和取值分布监控。覆盖度监控即将最近一段时间的客户按天汇总，监控空值占比的变化。<br>原始字段的覆盖率可以用偏差率 <code>r</code> 来表示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = |x - base| / base</span><br></pre></td></tr></table></figure><p><small><code>x</code> 为监控时段覆盖度，<code>base</code> 为基准覆盖度。通常设置 <code>r &gt; 0.1</code> 时触发预警。</small></p></li></ol><h4 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h4><p>特征异常处理是指在发现特征异常时，需要快速分析原因并给出解决方案，尽量减少异常对线上业务的影响。</p><ol><li><p>特征不一致<br>特征不一致的原因通常包含三种：</p><ul><li>在线数据和离线数据不一致。</li><li>在线特征和离线特征的处理逻辑不同。</li><li>数据状态曾发生变化。</li></ul></li><li><p>原始字段异常<br>原始字段的覆盖率及取值分布出现异常的原因会有多种，通常数据采集、处理、存储和应用环节都有可能出现上述异常，业务团队需要和技术团队配合，具体问题具体分析。</p></li></ol><hr><h3 id="术语介绍"><a href="#术语介绍" class="headerlink" title="术语介绍"></a>术语介绍</h3><ol><li><p>原始数据<br>原始数据是业务中产生的各类数据，通常是为了业务目的而组织和保存的底层数据，相对于建模使用的特征，原始数据一般是<strong>未经汇总处理的数据</strong>。</p></li><li><p>特征工程<br>特征工程是在给定数据、模型和任务的额情况下设计合适特征的过程。特征工程包含<strong>特征挖掘</strong>、<strong>特征筛选</strong>、<strong>特征组合应用</strong>等。</p></li><li><p><code>IV</code> 值<br><code>IV</code> 即信息价值（<code>Information Value</code>）也可称为信息量。 <code>IV</code> 值是用来衡量变量的预测能力，IV值越大，表示该变量的预测能力越强。</p><table><thead><tr><th><code>IV</code> 范围</th><th>描述</th></tr></thead><tbody><tr><td>iv &lt; 0.02</td><td>无预测能力，需放弃</td></tr><tr><td>0.02 &lt;&#x3D; iv &lt; 0.1</td><td>较弱的预测能力</td></tr><tr><td>0.1 &lt;&#x3D; iv &lt; 0.3</td><td>预测能力一般</td></tr><tr><td>0.3 &lt;&#x3D; iv &lt; 0.5</td><td>预测能力较强</td></tr><tr><td>iv &gt; 0.5</td><td>预测能力极强，需检查</td></tr></tbody></table></li><li><p><code>WOE</code> 值<br><code>WOE</code> 即证据权重（<code>Weight of Evidence</code>），<code>WOE</code> 是对原始自变量的一种编码形式。要对一个变量进行 <code>WOE</code> 编码，需要首先把这个变量进行分组处理（也叫离散化、分箱），分组后，对于第 <code>i</code> 组 <code>WOE</code> 的计算公式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WOEi = ln((Yi/Yt) / (Ni/Nt)) = ln(Pyi/Pni)</span><br></pre></td></tr></table></figure><p><small><code>Pyi</code> 是该组中响应客户在该组中的比例。<code>Pni</code> 是该组中未响应客户在该组中的比例。<code>Yi</code> 是该组中响应客户数据量。<code>Ni</code> 是该组中该组中未响应客户数据量。<code>Yt</code> 是该组中响应客户总数据量。<code>Nt</code> 是该组中未响应客户总数据量。响应客户指正样本，未响应客户指负样本。</small></p></li><li><p><code>PSI</code> 值<br>特征稳定性主要使用 <strong><code>PSI</code>（<code>Population Stability Index</code>，群体稳定性指标）</strong>来表示。<br>在实际风控业务中，对于 <code>PSI &gt; 0.1</code> 的特征需要关注并分析原因，然后根据原因是否可接受来决定特征是否继续使用。<br><small>在样本量很小的时候，<code>PSI</code> 的波动情况可能会因为随机情况导致不能表示真实的业务情况。</small></p><table><thead><tr><th><code>PSI</code> 范围</th><th>稳定性</th><th>表现&#x2F;建议</th></tr></thead><tbody><tr><td><code>0</code> ~ <code>0.1</code></td><td>好</td><td>特征基本稳定</td></tr><tr><td><code>0.1</code> ~ <code>0.25</code></td><td>略不稳定</td><td>持续监控后续变化</td></tr><tr><td>大于 <code>0.25</code></td><td>不稳定</td><td>剧烈变化，分析原因，找到应对方案</td></tr></tbody></table></li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;特征挖掘是从原始数据构造特征的过程。&lt;br&gt;特征是数据和模型之间的纽带，数据和特征决定机器学习的上限，而模型和算法只是无限逼近这个上限。&lt;br&gt;特征挖掘的完整流程包含&lt;strong&gt;原始数据分析&lt;/strong&gt;、&lt;strong&gt;数据清洗&lt;/strong&gt;、&lt;strong&gt;中间数据集构建&lt;/strong&gt;、&lt;strong&gt;特征设计和生成&lt;/strong&gt;、&lt;strong&gt;特征评估&lt;/strong&gt;和&lt;strong&gt;特征的上线、监控、维护和下线&lt;/strong&gt;。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Risk Control" scheme="https://blog.vgbhfive.cn/tags/Risk-Control/"/>
    
    <category term="Feature" scheme="https://blog.vgbhfive.cn/tags/Feature/"/>
    
  </entry>
  
  <entry>
    <title>智能风控-策略体系</title>
    <link href="https://blog.vgbhfive.cn/%E6%99%BA%E8%83%BD%E9%A3%8E%E6%8E%A7-%E7%AD%96%E7%95%A5%E4%BD%93%E7%B3%BB/"/>
    <id>https://blog.vgbhfive.cn/%E6%99%BA%E8%83%BD%E9%A3%8E%E6%8E%A7-%E7%AD%96%E7%95%A5%E4%BD%93%E7%B3%BB/</id>
    <published>2022-10-31T04:03:37.000Z</published>
    <updated>2023-01-02T11:26:06.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>风控策略是指根据<strong>不同业务场景和客群</strong>，通过<strong>一系列规则策略与模型策略的组合</strong>，对<strong>客户的风险进行判断</strong>，从而实现<strong>准入</strong>、<strong>反欺诈</strong>、<strong>授信</strong>、<strong>风险定价</strong>和<strong>催收</strong>等阶段目标，最终<strong>达成风险控制</strong>的目的。</p><span id="more"></span><p>风控策略的核心目标是将风险控制在合适的范围。但是风险并不是越低越好，应该在遵守监管政策和满足客户利益的前提下，实现收益的最大化。要想收益的最大化，因此也要了解信贷业务的利润组成，其中<strong>信贷业务的利润 &#x3D; 息费收入 - 运营成本 - 坏账损失</strong>。</p><p><strong>息费收入</strong>是金融机构的主要收入来源。但是息费不是越高越好，因为首先需要满足金融行业的政策要求，其次息费的高低会直接影响信贷产品吸引的客群质量。因此我们需要制定合理的风险定价策略给予不同客户合适的费率。</p><p><strong>运营成本</strong>是指金融机构运营过程中产生的各项成本。运营成本主要包含有获客成本、数据成本、人力成本和资金成本等，因此需要在风险可控的前提下持续优化经营成本。</p><p><strong>坏账损失</strong>是评估金融机构中的业务是否健康的重要指标。当然坏账并不是越低越好，需要在业务的发展速度和盈利水平之间均衡。</p><p>因此金融机构应该通过合理定价以提高收入、优化流程以降低运营成本、改进风控策略以降低风险，找到风险与收益的平衡点，从而实现收益的最大化。</p><hr><h3 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h3><h4 id="规则分析方法"><a href="#规则分析方法" class="headerlink" title="规则分析方法"></a>规则分析方法</h4><p>规则是基于特征的一系列判断条件的组合，例如 <em>男性</em>、<em>年龄</em> 特征，制定 <em>年龄大于50岁且为男性</em> 规则。</p><p>规则策略是指通过一系列规则对客户进行细分筛选，使得筛选出来的客户在风险或其他维度上与未被选中的客户存在明显差异，从而可以拒绝客户或接受规则命中的客户。当然风控规则的作用也就是准确识别出高风险人群，然后拒绝这部分用户，从而有效规避特定风险。</p><p>相较于模型，规则的优势如下：</p><ul><li>规则能够识别出特定的风险点。</li><li>规则明确，具有可解释性。</li><li>规则的的灵活性高，可以根据风险变化对规则快速调整。</li></ul><p>关于规则如何产生通常有两种方法：</p><ul><li>人工。</li><li>量化。</li></ul><p><img src="https://s2.loli.net/2022/11/02/PsRdcLSo4VwbM3j.png" alt="risk_1_1.jpg"></p><h5 id="制订人工规则"><a href="#制订人工规则" class="headerlink" title="制订人工规则"></a>制订人工规则</h5><p>制订<em>人工</em>规则共分为两步：第一步就是寻找风险点。而第二步则是根据已知风险点制订人工规则。</p><ol><li><p>寻找风险点</p><ul><li>市场调研。指通过公开信息、同业反馈等方式进行调研，为风控策略提供可参考与借鉴的风险信息。</li><li>信审人员和催收人员反馈。信审人员与催收人员会审阅贷款申请人的相关资料，或者通过电话与其直接沟通，此过程容易发现异常案件。</li><li>关联图谱识别。关联图谱是一种基于图的数据结构，是借款人之间有效的关系表达式。通过分析关联图谱，可以及时发现复杂关系中存在的潜在风险。</li><li>黑产分析。指通过互联网或者线下渠道收集相关得到欺诈情报或者线索，并基于此开展专题分析。</li><li>实时数据监控。通过分析和监控设备指纹（<code>IMEI</code>、<code>WIFI</code>）聚集性、地点（<code>GPS</code>、申请中的自填地址）聚集性发现其潜在的风险。</li></ul></li><li><p>根据风险点制订人工规则<br>制订人工规则是指将风险点识别的过程中的潜在风险点量化成规则。其大致过程就是设计特征和规则阈值，为了使规则具有更好的灵活性，需要考虑特征的兼容性和可塑性。</p></li></ol><h5 id="制订量化规则"><a href="#制订量化规则" class="headerlink" title="制订量化规则"></a>制订量化规则</h5><p><em>人工</em>规则的制订主要依靠业务经验和市场调查结果为依据，<em>量化</em>规则则是基于数据与事实。</p><ol><li><p>样本选取<br>样本选取需要遵守<strong>代表性</strong>、<strong>充分性</strong>、<strong>时效性</strong>和<strong>排除性</strong>四个原则。其中对时效性的要求最高，需要选择使用最近的有表现的样本集，因为只有近期样本的贷后风险才能代表当前环境下真实的风险情况。<br>在样本集选取结束后还需要对样本集进行划分，一般划分为<strong>训练集（<code>Train</code>）</strong>和<strong>验证集（<code>OOT</code>）</strong>，其中训练集用于规则开发，而验证集则用于规则效果验证。当然数据集的划分也是有其必要性，训练集保证规则有效的同时，验证集校验规则未来的适用性。</p></li><li><p>单规则的制订<br>单规则是指由单个特征形成的风险规则。其优点是可解释性较强，便于线上监控和调整。单规则的制订有 <strong><code>IV</code> 分析法</strong>和<strong>极端值检测法</strong>两种方式：</p><ul><li><code>IV</code> 分析法<br>  <code>IV</code> 分析法是指基于<strong>特征分箱</strong>后的结果与目标变量进行交叉统计，通过 <code>IV</code> 值的大小选择对目标变量区分度大的特征，计算特征每一分箱对应的逾期率，发现特征中高风险的分箱区间，从而提取有效的风险规则。<br>  <small>特征分箱是指将连续特征离散化，通常选择<strong>等距分箱</strong>或<strong>决策树分箱</strong>方式进行特征分箱，不使用<strong>等频分箱</strong>是因为等频分箱会将少数极端样本与正常样本划分到一箱，<strong>掩盖</strong>可能的高风险群体。</small></li><li>极端值检测法<br>  极端值检测方式假定不良（<code>bad</code>）客户区别于其他客户，不良客户在特征上的表现集中在极端值处，即特征值越小或越大，不良客户的<em>比例</em>越高。基于此可以使用<strong>分位数</strong>，枚举可能的极端值将其作为阈值，然后制订单规则。<br>  但是使用极端值检测方式容易受到样本量小的影响而产生波动，因此需要均衡考虑<strong>规则命中率（<code>hit_rate</code>）</strong>和<strong>命中坏样本率（<code>hit_bad_rate</code>）</strong>的关系。</li></ul></li><li><p>组合规则的制订<br>组合规则是指基于常识、业务经验和数据挖掘技术，将两个或多个不同特征进行组合而形成的规则。相比单规则，组合规则可以筛选出同时满足多个特征的细分人群，实现人群的精准刻画。</p></li></ol><h5 id="规则评估"><a href="#规则评估" class="headerlink" title="规则评估"></a>规则评估</h5><p>在制订规则后需要对规则进行合理评估。评估规则可以帮助我们从不同的角度发现规则的价值和不足。</p><ol><li><p>规则效果<br>规则效果是规则评估的重要维度。在不同的时间窗口，规则效果评估的差异较大，则需要判断是规则无效还是客群变化引起的，必要时需要及时调整规则阈值，然后再次进行评估。<br>规则的效果主要体现在<strong>样本逾期率（<code>hit_bad_rate</code>）</strong>和<strong>整体逾期率（<code>total_bad_rate</code>）</strong>的倍数差异，即<strong>提升度（<code>lift</code>）</strong>。因此需要根据业务经验，确定提升都或者判断命中样本逾期率的值是否达到业务拒绝阈值，以确定规则是否有效。<br><small><code>left</code> 为规则提升度，表示此规则对不良客户的识别能力高于随机识别的倍数。</small></p></li><li><p>规则的稳定性<br>规则的稳定性主要体现在时间窗口的规则命中率、命中量是否稳定。在训练集中命中样本较少的规则一般来说都不太稳定，有可能是随即导致的，因此需要重点验证其在验证集样本上的命中率和逾期率的稳定性。</p></li><li><p>规则的收益性<br>规则能够拒绝一部分不良客户，但是有时候还是会 <em>误伤</em> 一部分的好客户，另外规则本身可能有额外的数据成本，因此我们需要对规则的收益性进行评估。<br>规则的收益性评估是指从 <em>利润最大化</em> 角度出发，评估引入的规则是否能真正为业务带来利润。<br><strong>规则收益性评估</strong>主要包含以下方面：</p><ul><li>数据单价：规则所使用的数据源单词调用价格。</li><li>命中率：外部规则在放款样本上的命中率。</li><li>规则命中坏账率：命中放款样本上的坏账率。</li><li>盈亏平衡坏账率：当坏账率为此数值时，放款的收益等于成本与损失之和。</li><li>件均：放款金额平均值。</li></ul></li></ol><h5 id="规则上线"><a href="#规则上线" class="headerlink" title="规则上线"></a>规则上线</h5><p>规则上线是将已经制订完成且评估有效的规则在决策引擎中进行配置并发布到生产环境中。<br><strong>决策引擎</strong>是一套用于部署风控规则、机器学习模型，进行风控策略实验并输出决策结果的系统。决策引擎为风控策略的快速实施带来了极大便利，其主要功能包括配置规则、决策表、评分卡、决策流、模型管理、额度管理和账期利率管理等。</p><p>在规则上线之后我们需要对已上线的规则进行及时的验证，确保规则执行与预期一致，通常从以下四个方面进行验证：</p><ul><li>阈值是否正确<br>  判断线上应用的规则阈值与线下制订规则时的阈值是否一致。</li><li><code>A/B</code> 测试分流比例是否正确<br>  <code>A/B</code> 测试分流比例也是验证项之一，如果决策引擎中 <code>A/B</code> 测试分流比例配置正确，那么正常情况下实际分流比例与配置比例基本一致。</li><li>规则命中率是否正常<br>  <strong>规则命中率</strong>是触发规则与进入规则数量的比值。规则命中率异常将直接导致影响线上实际业务的<strong>风控转化率</strong>。</li><li>规则回顾<br>  规则回顾是指对线上测试中的 <strong>空跑</strong> 或者 <strong>阈值实验</strong> 分流组再次进行效果评估，目的是验证规则上线后的实际效果是否得到延续。</li></ul><h4 id="模型策略分析方法"><a href="#模型策略分析方法" class="headerlink" title="模型策略分析方法"></a>模型策略分析方法</h4><p>模型策略是基于已有风控模型制订最优决策的整体方法，已有风控模型决定了模型价值是否能够被充分发挥，直接影响信贷业务的盈利水平。<br><strong>模型策略分析流程</strong>主要包含<strong>样本提取</strong>、<strong>模型策略的制订</strong>、<strong>模型策略评估</strong>、<strong>模型策略的上线与验证</strong>和<strong>模型策略回顾</strong>。<br><img src="https://s2.loli.net/2022/11/07/4clL1UGfB7sj2dI.png" alt="risK_1_2.jpg"></p><ol><li><p>样本选取<br>样本选取是指选取制订模型策略所需的样本集，通常包括风控模型开发的<strong>跨时间验证集（<code>OOT</code>）</strong>和<strong>近期授信样本集（<code>BackScore</code>）</strong>。跨时间验证集需要包含订单标识、模型分和逾期标签列，近期授信样本集需要包含订单标识和模型分列。</p></li><li><p>模型策略的制订<br>模型策略的制订主要决定模型的组合方式和阈值。在制订方案时需要平衡转化率和坏账率之间的关系，以实现收益最大化。<br>模型策略应用方案可分为以下两种：</p><ul><li>单模型策略<br>  单模型策略是指利用<strong>单一模型分</strong>进行决策，故只需要确定单一模型的最优决策点。通常单模型适用于以下场景：信贷业务开展前期，线上只有一个模型；信贷业务开展中期，虽然线上模型增多，但模型间关联性较强。<br>  单模型通常有以下几种制订方式：<ul><li>基于模型通过率和坏账率的决策点设定<br>  在模型通过率和坏账率之间寻找一个决策点，理想的状态是该决策点的设立可以提高通过率并降低坏账率，但是在现实中则会出现其他情况：保持目标模型通过率，降低坏账率；提升模型通过率，保持坏账率；提高模型通过率，同时降低坏账率。</li><li>基于 <code>lift</code> 的决策点设定<br>  <code>lift</code> 表示风控模型对预测目标中不良客户的识别比例高于随机识别比例的倍数。通常情况下，<code>lift</code> 的值越大越好。</li></ul></li><li>多模型组合策略<br>  多模型组合策略是基于<strong>两个或多个以上模型分</strong>组合生成的模型应用方案。多模型的优势在于：能够充分发挥多个模型之间的性能互补；内外部模型组合的使用能够有效降低数据成本。<br>  多模型组合策略的应用方式有以下几种：<ul><li>多模型融合准入，通常是指利用加权或者其他方式将多个模型分融合成一个模型分，再划分风险等级上线决策。</li><li>多模型串行准入，通常是指将多个模型以串行的方式按照先后顺序依次决策准入，前一个模型决策通过的样本再经过下一个模型决策进行评估，以此类推，直至最后一个模型生成风险等级。</li><li>多模型交叉准入，通常分为两个阶段：准入阶段，由前置模型完成；交叉阶段，由后置的两个模型共同生成风险等级。</li></ul></li></ul></li><li><p>模型策略评估<br>从业务角度关注新模型和旧模型之间的性能差异，通常会用到<strong>交换集分析（<code>swap set analysis</code>）</strong>和<strong>拒绝推断</strong>：</p><ul><li>交换集分析<br>  交换集分析是指利用<strong>新旧模型通过和拒绝客户不一致</strong>的情况，通过<strong>分析这些不一致的客户对坏账率和通过率的影响</strong>以评价模型策略的效果。<br>  <strong>换出（<code>swap out</code>）</strong>是指新模型拒绝而旧模型通过的客群，<strong>换入（<code>swap in</code>）</strong>是指新模型通过而旧模型拒绝的客群。通常希望新模型能够换出更多的不良客户，换入更多的好客户，从而用好客户代替不良客户，以降低整体的坏账率。</li><li>拒绝推断<br>  如何进行合理的拒绝推断呢？最直接的方式就是<strong>利用新模型各分数段在有表现样本上的坏账率来估算旧模型拒绝样本上的坏账率</strong>。<br>  但是上述结果也是存在问题的，因为此时的坏账率是在旧模型通过的条件下计算出来的，而要想推断新模型的坏账率则需要进行一定的处理：<code>Universe Test</code> 推断；<code>A/B</code> 测试组推断；线性拟合推断。</li></ul></li><li><p>模型策略上线和验证<br>与规则上线类似，模型策略上线确保其线上实际执行效果与预期一致，主要有以下三个方面：</p><ul><li>模型<strong>阈值</strong>是否正确。</li><li><code>A/B</code> 测试的<strong>分流比例</strong>是否正确。</li><li><strong>模型通过率</strong>是否符合预期。</li></ul></li><li><p>模型策略回顾<br>模型策略回顾就是<strong>使用线上数据定期验证模型策略 <code>A/B</code> 测试</strong>的方案，最终基于比对的情况，选择适合当前环境的方案来进行决策。</p></li></ol><h4 id="额度-利率-账期策略分析方法"><a href="#额度-利率-账期策略分析方法" class="headerlink" title="额度&#x2F;利率&#x2F;账期策略分析方法"></a>额度&#x2F;利率&#x2F;账期策略分析方法</h4><p>额度&#x2F;利率&#x2F;账期策略是基于产品属性或客户的某些特性制订的差异化方案。在符合监管的要求下差异化的额度&#x2F;利率&#x2F;账期可以保证金融机构的收益最大化。从金融机构的收益角度来看，差异化的额度&#x2F;利率&#x2F;账期可以保持件均额度不变的情况下，有效降低金额损失率；或者在贷后损失率不变的情况下，提升件均额度，从而提高金融机构的利润。<br><img src="https://s2.loli.net/2022/11/08/yXwU8K3TmqupnNH.png" alt="risk_1_3.jpg"></p><ol><li><p>额度&#x2F;利率&#x2F;账期策略的制订<br>额度&#x2F;利率&#x2F;账期策略是设定借款人的授信额度方案，而额度策略又可以分为：</p><ul><li>单一额度策略。对于首贷用户可以设置相同的额度；而对于复贷用户，可以随着贷款次数逐步增加额度。</li><li>单因子额度策略。将单个维度数据作为额度差异化的依据，基于信用评分模型给出的风险等级给出差异化的额度。</li><li>多因子额度策略。将客户更多维度数据作为额度差异化的依据，使用多项数据组成额度矩阵根据用户的风险等级和还款能力组合出最适合的额度。</li></ul></li><li><p>策略评估<br>策略评估即对比使用前和使用后的真实贷后差异，如在件均不变的情况下，观察能够降低多少的坏账率。</p></li><li><p>策略的上线与验证<br>在明确策略的 <code>A/B</code> 测试方案后，需要在决策引擎中配置相应的方案，设置分流比例等其他参数，确保及时、准确地分布。<br>为了确保策略上线后的实际运行情况与预期一致，需要从以下三个方面进行验证：配置是否正确；<code>A/B</code> 测试分流比例是否正确；件均额度是否符合预期。</p></li><li><p>策略回顾<br>在策略的 <code>A/B</code> 测试方案线上稳定且有贷后数据表现时，需要及时进行策略回顾。因为即使同一风险等级，在有额度差异的情况下，坏账率也会存在显著差异，因此需要从利润最大化的角度来选择最适合的方案。另外随着信贷市场的变化，也需要重新定制新的 <code>A/B</code> 测试方案，如此才会适应市场变化，满足客户需求。</p></li></ol><h4 id="A-B-测试"><a href="#A-B-测试" class="headerlink" title="A/B 测试"></a><code>A/B</code> 测试</h4><p><code>A/B</code> 测试也可以被成为冠军挑战者实验，是指在同一时间、同一对象上测试多种方案，并通过分析找到最优的方案。也就是说需要提前式设计多种方案，然后对同一客群的不同客户应用不同方案，分别记录每种方案对应客户的使用或转化指标，最后通过分析选择出最优的方案，并确定是否要推广到全部流量中。</p><ol><li><p>方案设计<br><code>A/B</code> 测试方案设计主要包含：确定实验组和对照组的内容；流量分配。<br>在金融风控领域，<code>A/B</code> 测试主要用来验证新旧模型或规则的效果是否存在显著差异，即实验组对应新模型策略，对照组对应旧模型策略。在流量分配方面，因为新模型策略的实际效果未知，因此一旦出错容易造成资金流失。最终在流量分配方面，实验组会获得少量流量，对照组会获得较多流量。<br><img src="https://s2.loli.net/2022/11/09/HrSZNfTems8L1ob.png" alt="risk_1_4.jpg"></p></li><li><p>测试结果分析<br><code>A/B</code> 测试结果分析主要基于两部分：</p><ul><li>实验有效性判断，主要包含：判断测试的样本量是否达到所需的最小样本量，从而可以尽快可能地避免两类统计（无效判断为有效，有效判断为无效）错误的发生；判断样本的有效性，即判断采用 <code>A/A</code> 测试结果得到两组之间是否存在显著差异，若不存在显著差异，则任务测试效果有效。</li><li>测试结果的比较，在对测试有效性判断之后，就可以对结果进行比较，通常会比对实验组和对照组的结果，判断他们之间是否存在显著差异，从而判断新旧方案在业务方面是否有显著提升。</li></ul></li><li><p>注意事项</p><ul><li>流量分配，要保证同时性、同质性、唯一性和均匀性。</li><li>上线后的数据验证，确保实验组和对照组指标符合预期，否则需要排查和修复异常并重新开始测试。</li><li>多测试同时展开，不是同时开展一个测试，在确保其他变量可控和流量可分的情况下，可以开展多个测试。</li></ul></li></ol><hr><h3 id="策略体系搭建"><a href="#策略体系搭建" class="headerlink" title="策略体系搭建"></a>策略体系搭建</h3><p>风控策略体系的搭建是指贯穿营销、贷前、贷中和贷后的完整策略体系架构，因此需要在各个阶段设置合适的风控策略，灵活运用规则和模型的组合，才能做到有效的风险控制并取得收益最大化。</p><h4 id="营销阶段"><a href="#营销阶段" class="headerlink" title="营销阶段"></a>营销阶段</h4><p>营销策略是指在营销获客阶段执行的策略，用于排除高风险客户，选择高响应客户，以达到降本增效的目的。<br>从客户的类型来划分，营销可以分为针对<strong>纯新客的营销</strong>和<strong>存量客户的营销</strong>。纯新客是指没有在本金融机构完成借款的客户，金融机构可以获取的客户信息较少；而存量客户是指在本金融机构已经存在借款记录的客户，金融机构可以获得的信息较为丰富。<br>从客户的来源来划分，可以分为已有名单的客户和第三方导流的客户，虽然客户的来源不同，但是基本的营销策略类似，都是排除高风险客户，然后选择高响应客户，最后针对高响应客户进行营销和接下来的授信。</p><ol><li><p>排除高风险客户<br>排除高风险客户可以采用以下措施：</p><ul><li>黑名单。此部分数据来源于营销名单中的黑名单和逾期客户的黑名单。</li><li>风险规则。</li><li>流量筛选模型。</li></ul></li><li><p>选择高响应客户<br>排除高风险客户之后，可以进一步对营销概率更高的客户进行筛选，选择高响应客户可以采用以下措施：</p><ul><li>高响应规则。</li><li>营销响应模型。</li></ul></li></ol><h4 id="贷前阶段"><a href="#贷前阶段" class="headerlink" title="贷前阶段"></a>贷前阶段</h4><p>贷前策略是针对客户的信贷申请制订的策略，用于拦截逾期概率高的客户，并对客户匹配合适的产品。贷前策略主要包括<strong>风险准入策略</strong>、<strong>反欺诈策略</strong>、<strong>信用评估策略</strong>和<strong>贷前额度策略</strong>。</p><ol><li><p>风险准入策略<br>风险准入策略是判断借款客户身份是否符合当前业务准入条件的策略。风险准入策略可以降低非目标客户带来的风险，其主要是依据信贷产品定位和政策要求制订，主要包含<strong>身份信息认证</strong>、<strong>基础信息准入</strong>、<strong>黑&#x2F;白名单策略</strong>和<strong>其他准入策略</strong>。</p><ul><li>身份信息认证。<br>  身份认证信息包含<strong>身份证信息认证</strong>、<strong>人脸比对认证</strong>、<strong>银行卡四要素认证</strong>和<strong>运营商三要素</strong>。</li><li>基础信息准入。<br>  基础信息准入是判断借款人是否符合当地的信贷政策，以验证客户身份是否符合法律法规和相关的政策要求。基础信息的准入可以归纳为：<strong>年龄准入</strong>；<strong>贷款用途准入</strong>；<strong>地域准入</strong>；<strong>行业准入</strong>。</li><li>黑&#x2F;白名单策略。<br>  黑名单包含金融机构拒绝放款的客户，主要包含有<strong>历史严重逾期客户</strong>、<strong>存在欺诈行为客户</strong>、<strong>存在违法行为客户</strong>和<strong>恶意投诉客户</strong>。<br>  白名单是金融机构将资产信用良好的客户单独整理出来的名单，与黑名单相比，白名单是命中即通过。</li><li>其他准入策略。<br>  金融机构根据自身的风险偏好，根据历史记录得出的一些简单、可靠的判断准则纳入风险准入策略。</li></ul></li><li><p>反欺诈策略<br>反欺诈策略是为了防范恶意客户采取欺诈行为谋取利益的策略，目的是通过对欺诈行为的识别，遏制欺诈风险，及时止损。<br>目前应对欺诈风险的有效措施包括<strong>反欺诈规则</strong>和<strong>反欺诈模型</strong>。</p><ul><li>反欺诈规则。<br>  反欺诈规则的优点：能够有效遏制特定的欺诈行为；可解释性强，应对欺诈手段可以快速调整。<br>  常见的反欺诈规则如下：<ul><li><code>ID</code> 关联异常。例如身份证号、手机号、银行卡号和设备号存在一对多的异常关联。</li><li><code>App</code> 操作行为异常。例如操作时间过短或某些操作之间的时间间隔过短。</li><li>位置行为异常。例如短时间内 <code>GPS</code> 移动距离过大。</li><li>安装高风险类 <code>App</code>。例如安装作弊类、欺诈类、赌博类等 <code>App</code>。</li><li>移动设备异常。例如设备有 <code>root</code> 记录、安装有模拟器等。</li><li>交叉验证信息不一致。例如 <code>GPS</code> 定位地址、工作地址、居住地等地址信息对比不一致。</li><li>特殊手机号。例如虚拟手机号。</li><li>特殊银行卡。例如虚拟银行卡。</li><li>紧急联系人异常。例如借款人和其他借款人有相同的紧急联系人。</li><li>团伙欺诈特征。例如相同公司、相同地址等。</li><li>社交关系网络风险。例如一度&#x2F;二度联系人申请比例过高。</li><li>疑似撸贷。例如短期内多次提前还款然后再次立刻借款。</li><li>身份欺诈。例如同一人像却对应不同的证件。</li></ul></li><li>反欺诈模型。<br>  反欺诈模型是通过机器学习算法将客户各个维度的数据特征与欺诈行为建立关联关系，并给出欺诈的概率。<br>  反欺诈模型的优点：可以充分利用弱特性；对抗性好，可增加欺诈成本。<br>  常见的欺诈模型包含<strong>有监督学习</strong>和<strong>无监督学习</strong>。</li></ul></li><li><p>信用风险策略<br>信用风险是指客户在有偿还意愿的前提下因偿还能力不足或其他原因而产生的风险。即为了防范正常客户因偿还能力不足导致逾期风险而制订的策略，其目标就是合理评估客户的偿还能力，保证客户能在借款到期时能够及时履约。<br>目前有效的信用风险策略包含：</p><ul><li>信用风险规则。<br>  信用风险规则侧重客户的资产负债情况，如<strong>收入水平</strong>、<strong>负债水平</strong>、<strong>借贷信用历史</strong>等。其优点在于：识别准确性较高；可根据业务变化及时调整。</li><li>信用风险模型。<br>  信用风险模型是将客户多维数据特征整合，利用机器学习算法训练得到，通常采用有监督学习方式。</li></ul></li><li><p>贷前额度策略<br>贷前额度策略是为新客户授予初始额度的策略。信贷产品的设计共分为两类：一类是对客户单次授信单次借款的模式，另一种则是对客户授予额度并可以多次支用的的循环额度模式。</p></li></ol><h4 id="贷中阶段"><a href="#贷中阶段" class="headerlink" title="贷中阶段"></a>贷中阶段</h4><p>贷中策略是针对在贷客群制订的一系列策略，用于降低在贷客户风险，提高在贷客户价值。<br>当申请人通过了贷前审核，成为金融机构的客户，因此我们希望客户可以持久和更多的使用我们的信贷产品，为了<strong>持续带来营收</strong>、<strong>最大限度留住客户</strong>、<strong>延长使用期限</strong>，随着时间的变化，客户的还款能力可能发生变化，因此金融机构要及时做出调整，这体现了贷中策略的重要性。<br><small>贷中策略主要针对的是使用<strong>循环额度模式</strong>的信贷产品。</small></p><ol><li><p>贷中支用策略<br>客户在获得授信之后，可能在贷中发生多次支用行为，但是客户的资质是在持续变化，因此每次支用时都应当检查客户的风险情况。</p><ul><li>支用风险规则。</li><li>支用风险模型。</li></ul></li><li><p>贷中额度策略<br>对在贷客户进行风险评估，重新确定客户的授信额度并加以调整，以提升客户满意度，提高客户价值。对于低风险客户，最直接激励的方式就是调额。而对于高风险客户最有效控制损失的方式就是降低额度。<br>对于贷中额度的调整可以从以下方面入手：</p><ul><li>风险规则。对于潜在风险较高的客户，应该禁止调额，可以通过类似贷前和贷中支用的风险规则进行拦截。</li><li>使用规则。额度使用率较低的客户没必要进行调额，可以通过制订规则将这部分客户排除。</li><li>贷中行为模型。</li></ul></li></ol><h4 id="贷后阶段"><a href="#贷后阶段" class="headerlink" title="贷后阶段"></a>贷后阶段</h4><p>贷后模型主要针对的是逾期客户的一系列策略，用于<strong>提高催收效率</strong>，<strong>提升催收回款率</strong>。<br>常见的催收还款方式有：</p><ul><li>短信。</li><li>电话。</li><li>电子邮件。</li><li>上门催收。</li><li>法院诉讼。</li></ul><p>按照渠道催收可以分为内部催收和委外催收，内部催收即金融机构安排内部员工催收，而委外催收就是委托三方机构或人员来催收。</p><p>根据逾期持续的时长，催收可以被划分为<strong>早期催收</strong>、<strong>中期催收</strong>和<strong>晚期催收</strong>三个阶段，具体时间是根据金融机构的产品形态和回款率衰减情况设定的：</p><ul><li>早期催收。早期催收阶段是整个催收管理流程中最为重要的一个阶段。该阶段的逾期客户的特点：逾期客户量很大；还款率较高。其中风险等级较低的大部分客户并非恶意拖欠，可能是忘记还款日期或者临时资金短缺，一旦提醒就会立刻还款。</li><li>中期催收。中期催收阶段的客户风险较高，相较短期逾期客户，还款率大幅降低。在此阶段应对各个风险等级的客户加大催收力度，多采用电话催收方式。</li><li>晚期催收。在晚期催收阶段的逾期客户数量已经大幅减少，但每个逾期客户的还款率都极低。一般来说逾期至晚期催收阶段的客户没有还款意愿或还款能力。此阶段应该关注风险等级较低的客户，对于风险等级较高的客户可以委外处理或资产转让。</li></ul><p>另外还可以将需要催收的客户进行等级划分，采用催收规则和催收模型结合的方式：</p><ul><li>催收规则。根据客户的行为制订规则，以识别逾期后难以催收的客户。</li><li>催收模型。根据客户的贷中和贷后行为，建立催收模型，可以很好地预测客户在一定时间内是否还款，同时根据模型预测的还款概率，可以轻松的进行风险等级划分。</li></ul><hr><h3 id="监控、预警和异常处理"><a href="#监控、预警和异常处理" class="headerlink" title="监控、预警和异常处理"></a>监控、预警和异常处理</h3><p>风控策略在制订并上线部署之后，其执行的准确性和稳定性对信贷业务至关重要。</p><h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><p>为了能够在第一时间内发现风控策略的问题，因此需要全面的动态监控，可以从<strong>贷前转化监控</strong>、<strong>贷后逾期监控</strong>和<strong>资产监控</strong>三个维度进行监控</p><ol><li><p>贷前转化监控<br>贷前转化监控是对客户从激活 <code>App</code> 到放款的整个流程中各个环节转化率的变化进行监控。转化主要涉及<strong>激活 <code>App</code><strong>、</strong>注册</strong>、<strong>进件</strong>和<strong>放款环节</strong>，根据信贷业务流程将其归纳之后可以分为<strong>产品转化（激活 <code>App</code> 到进件）监控</strong>和<strong>风控转化（进件到放款）监控</strong>。</p><ul><li>产品转化监控<br>  产品转化监控一般是指对客户从激活 <code>App</code> 到申请借款之前的转化率进行监控。通过产品转化监控可以发现客户在使用产品过程中出现流失的主要环节，然后可以优化对应步骤从而提升产品效能和优化体验。<br>  产品转化监控主要包含<strong>激活量</strong>、<strong>注册量</strong>和<strong>进件量</strong>等指标，时间周期可以是天、周或月等。必要时可以增加实时监控。</li><li>风控转化监控<br>  风控转化监控是指监控客户从进件到放款的转化率，涉及到的有<strong>规则策略</strong>、<strong>模型策略</strong>和<strong>人审策略</strong>等。<br>  风控转化监控的对象主要包含<strong>规则命中率</strong>、<strong>模型通过率</strong>、<strong>机审转化率</strong>、<strong>人审通过率</strong>和<strong>风控转化率</strong>等，时间周期一般以天、周、月和小时为主。</li></ul></li><li><p>贷后逾期监控<br>贷后逾期监控是指对放款客户的逾期率进行相关指标进行监控。其可以及时的反应风控策略的有效性，发现市场风险的变化，以利于及时制订和调整风控策略，保证风险持续可控。<br>在针对不同的信贷产品，其贷后逾期监控的指标有所不同，因此在设计监控方案时应根据<strong>新老客户</strong>、<strong>不同渠道客群</strong>等分别监控，对于监控指标的设计可以考虑以下几点：</p><ul><li>统计样本的口径。</li><li>统计的表现时间范围。</li><li>统计的对象。</li><li>统计的数值。</li><li>监控的更新周期。</li></ul></li><li><p>资产监控<br>资产监控是指在一个周期内（一般是月或者季度）内对信贷业务放款量、放款金额和贷款余额等指标进行监控。其可以看到各个阶段的放款实际情况，从而控制放款节奏。资产监控包含有<strong>交易量监控</strong>和<strong>资产余额监控</strong>。</p><ul><li>交易量监控<br>  交易量监控主要关注信贷业务的实际放款情况，监控周期一般以日为单位，监控指标有<strong>不同渠道的放款金额</strong>、<strong>放款量</strong>、<strong>平均放款额度</strong>、<strong>平均期限</strong>和<strong>预计全月放款额度</strong>等。</li><li>资产余额监控<br>  资产余额监控主要关注的是贷款余额变化，监控周期一般以周为单位，监控指标有<strong>贷款余额</strong>、<strong>未到期余额</strong>和<strong>逾期余额（可以分为不同逾期天数的在逾金额）</strong>等。</li></ul></li></ol><h4 id="预警"><a href="#预警" class="headerlink" title="预警"></a>预警</h4><p>风险预警是在监控指标变化超过合理阈值时进行报警，主要包含有<strong>贷前转化预警</strong>和<strong>贷后风险预警</strong>。风险预警可以通过企业微信、电子邮件、短信、飞书、钉钉、电话等方式进行报警。</p><ul><li>贷前转化预警<br>  贷前转化预警一般是指某一时段内的转化率指标值超过预警阈值，而预警阈值需要根据业务的实际运行情况进行设置，可以根据产品、渠道等拆分。</li><li>贷后风险预警<br>  贷后风险预警的触发条件是到期日的风险指标超过预警阈值。预警阈值可以设置为过去某一段周期内平均值的 <code>120%</code> ，当实际值的变化波动超过 <code>25%</code> 时会触发预警。</li></ul><h4 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h4><p>风险监控和预警只能通知在风控流程中发生的异常以及所处的阶段，而异常处理则是根据预警针对性的分析异常并给出对应的解决方案。<br>解决方案需要根据具体原因来确定，一般可以分为临时性解决方案、根本性解决方案和防止复发性解决方案。在没有找到根本原因时一般先采用临时性解决方案，而后基于数据、事实的分析找到真正原因，并根据真正原因制订对应的方案从根本上解决问题。</p><ol><li><p>转化异常处置<br>转化异常处置通常是<strong>某时段的转化率突然升高或降低</strong>，因此需要分析对应原因并给出解决方案。<br>转化异常处理目前可以分为以下几个方面：</p><ul><li>统计问题。确认各项指标的统计逻辑和数据报表是否正常。</li><li>产品异常。产品流程变更导致客户进件流程异常等。</li><li>规则策略问题。检查相应日期是否有规则调整且调整是否符合预期。</li><li>模型策略问题。检查相应日期是否有模型调整且调整是否符合预期。</li><li>数据问题。检查规则或模型使用的数据是否异常，包含三方数据和自有数据。</li><li>客群变化。在其他环节都没有问题的时候，判断是否为客群变化导致的异常。</li></ul></li><li><p>贷后逾期异常处置<br>贷后逾期异常处置的情况通常是根据<strong>某日的贷后逾期率大幅提高</strong>，因此需要分析对应原因并给出解决方案。<br>贷后逾期异常处置的分享原因分析维度和处置方案如下：</p><ul><li>统计问题。首先检查当日逾期率计算逻辑是否正常、报表数据显示是否正常、样本量是否满足统计显著性需求，以排除统计问题。</li><li>产品异常。检查客户还款或产品是否遇到其他问题。</li><li>风控策略问题。检查放款当日是否有策略调整。</li><li>催收问题。检查当日是否有催收策略调整或者催收人员管理问题。</li><li>客群或市场问题。若上述问题均未发现异常，则需要对客群进一步拆分以分析原因。</li></ul></li></ol><hr><h3 id="术语介绍"><a href="#术语介绍" class="headerlink" title="术语介绍"></a>术语介绍</h3><ol><li><p>坏账<br>坏账是指金融机构发放的贷款未能按预先约定的期限、利率收回，并且很大程度上被认定为将来也无法收回。</p></li><li><p>转化率<br>转化率是指在一个统计周期内，完成某过程的次数与参与该过程的总次数的比值。转化率主要有下列三种：</p><ul><li>机审转化率：在一个统计周期内，风控系统自动审核通过的订单与申请订单数的比例。</li><li>人审转化率：在一个统计周期内，信审人员审核通过的订单数与进入人工审核阶段的订单数的比例。</li><li>风控转化率：在一个统计周期内，放款订单数与总申请订单数的比例。</li></ul></li><li><p>有&#x2F;无监督学习<br>机器学习包含有监督学习、无监督学习和强化学习。</p><ul><li>有监督学习从有标记的训练数据中推导出预测函数。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话概括就是：给定数据，预测标签。</li><li>无监督学习是从没有标记的训练数据中推断结论。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话概括就是：给定数据，寻找隐藏的结构。</li><li>强化学习关注的是软件代理如何在一个环境中采取行动以便最大化某种累积的回报。一句话概括就是：给定数据，学习如何选择一系列行动，以最大化长期收益。</li></ul></li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;风控策略是指根据&lt;strong&gt;不同业务场景和客群&lt;/strong&gt;，通过&lt;strong&gt;一系列规则策略与模型策略的组合&lt;/strong&gt;，对&lt;strong&gt;客户的风险进行判断&lt;/strong&gt;，从而实现&lt;strong&gt;准入&lt;/strong&gt;、&lt;strong&gt;反欺诈&lt;/strong&gt;、&lt;strong&gt;授信&lt;/strong&gt;、&lt;strong&gt;风险定价&lt;/strong&gt;和&lt;strong&gt;催收&lt;/strong&gt;等阶段目标，最终&lt;strong&gt;达成风险控制&lt;/strong&gt;的目的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Risk Control" scheme="https://blog.vgbhfive.cn/tags/Risk-Control/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse-管理与运维</title>
    <link href="https://blog.vgbhfive.cn/ClickHouse-%E7%AE%A1%E7%90%86%E4%B8%8E%E8%BF%90%E7%BB%B4/"/>
    <id>https://blog.vgbhfive.cn/ClickHouse-%E7%AE%A1%E7%90%86%E4%B8%8E%E8%BF%90%E7%BB%B4/</id>
    <published>2022-09-12T15:35:16.000Z</published>
    <updated>2023-01-01T15:44:16.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本文将对 <code>ClickHouse</code> 管理与运维相关的知识进行说明，主要包含<strong>用户</strong>、<strong>权限</strong>、<strong>熔断机制</strong>、<strong>数据备份</strong>和<strong>服务监控</strong>等知识。</p><span id="more"></span><hr><h3 id="用户配置"><a href="#用户配置" class="headerlink" title="用户配置"></a>用户配置</h3><p><code>user.xml</code> 配置文件默认位于 <code>/etc/clickhouse-server</code> 路径下，<code>ClickHouse</code> 使用他来定义用户相关的配置项，包括系统参数的设定、用户的定义、权限以及熔断机制等。</p><h4 id="用户-profile"><a href="#用户-profile" class="headerlink" title="用户 profile"></a>用户 <code>profile</code></h4><p>用户 <code>profile</code> 的作用类似于用户角色，可以预先在 <code>user.xml</code> 中为 <code>ClickHouse</code> 定义多组 <code>profile</code>，并为每组 <code>profile</code> 定义不同的配置项，以实现配置得到复用。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span> <span class="comment">&lt;!-- 配置 profile --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">default</span>&gt;</span> <span class="comment">&lt;!-- 用户自定义角色 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">max_memory_usage</span>&gt;</span>100000000<span class="tag">&lt;/<span class="name">max_memory_usage</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">use_uncompressed_cache</span>&gt;</span>0<span class="tag">&lt;/<span class="name">use_uncompressed_cache</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">default</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">test1</span>&gt;</span> <span class="comment">&lt;!-- 自定义名称，默认角色 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">allow_experimental_live_view</span>&gt;</span>1<span class="tag">&lt;/<span class="name">allow_experimental_live_view</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ddistributed_product_mode</span>&gt;</span>allow<span class="tag">&lt;/<span class="name">ddistributed_product_mode</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">test1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure><p>另外 <code>profile</code> 配置支持继承，实现继承的方式是在定义中引用其他的 <code>profile</code> 名称：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">normal_inherit</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">profile</span>&gt;</span>test1<span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">profile</span>&gt;</span>test2<span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">distributed_product_mode</span>&gt;</span>deny<span class="tag">&lt;/<span class="name">distributed_product_mode</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">normal_inherit</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="配置约束"><a href="#配置约束" class="headerlink" title="配置约束"></a>配置约束</h4><p><code>constraints</code> 标签可以设置一组约束条件，以保障 <code>profile</code> 内的参数值不会被随意修改。约束条件有如下三种规则：</p><ul><li><code>Min</code>：最小值约束，在设置相应参数的时候，取值不能小于该阈值。</li><li><code>Max</code>：最大值约束，在设置相应参数的时候，取值不能大于该阈值。</li><li><code>Readonly</code>：只读约束，该参数值不允许被修改。</li></ul><p>示例如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span> <span class="comment">&lt;!-- 配置 profile --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">default</span>&gt;</span> <span class="comment">&lt;!-- 用户自定义角色 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">max_memory_usage</span>&gt;</span>100000000<span class="tag">&lt;/<span class="name">max_memory_usage</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">use_uncompressed_cache</span>&gt;</span>0<span class="tag">&lt;/<span class="name">use_uncompressed_cache</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">constraints</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">max_memory_usage</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">min</span>&gt;</span>5000000<span class="tag">&lt;/<span class="name">min</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">max</span>&gt;</span>10000000<span class="tag">&lt;/<span class="name">max</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">max_memory_usage</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">distributed_product_mode</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">readonly</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">distributed_product_mode</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">constraints</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">default</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="用户定义"><a href="#用户定义" class="headerlink" title="用户定义"></a>用户定义</h4><p>使用 <code>users</code> 标签可以配置自定义用户。如果打开 <code>user.xml</code> 配置文件，会发现已经默认配置 <code>default</code> 用户。定义一个新用户必须包含以下属性：</p><ul><li><code>username</code><br>  <code>username</code> 用于指定登录用户名，这是全局唯一属性。</li><li><code>password</code><br>  <code>password</code> 用于设置登录密码，支持明文、<code>SHA256</code> 加密、 <code>double_sha1</code> 加密三种方式，可以任选其中一种进行设置。<ul><li>明文密码：在使用明文密码时直接使用 <code>password</code> 标签定义密码。</li><li><code>SHA256</code> 加密：在使用 <code>SHA256</code> 加密算法时直接指定 <code>password_sha256_hex</code> 标签定义密码。</li><li><code>double_sha1</code> 加密：在使用 <code>double_sha1</code> 加密算法的时候，则需要通过 <code>double_sha1</code> 标签定义密码。</li></ul></li><li><code>networks</code><br>  <code>networks</code> 表示被允许登录的网络地址，用于限制用户登录的客户端地址。</li><li><code>profile</code><br>  用户所使用的 <code>profile</code> 配置，直接引用相应的名称即可。</li><li><code>quota</code><br>  用于设置该用户能够使用的资源限额，可以立即为一种熔断机制。</li></ul><p>示例如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">user_test</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">password</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">password</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">networks</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ip</span>&gt;</span>::/0<span class="tag">&lt;/<span class="name">ip</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">networks</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">profile</span>&gt;</span>default<span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">quota</span>&gt;</span>default<span class="tag">&lt;/<span class="name">quota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">user_test</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><h3 id="权限管理"><a href="#权限管理" class="headerlink" title="权限管理"></a>权限管理</h3><p>权限管理始终是一个的话题，<code>ClickHouse</code> 分别从访问、查询和数据等角度出发，层层递进提供了一个立体的权限体系。</p><h4 id="访问权限"><a href="#访问权限" class="headerlink" title="访问权限"></a>访问权限</h4><p>访问层控制是整个权限体系的第一层防护，它又可一步细分为两类权限。</p><ol><li><p>网络访问权限<br>网络访问权限使用 <code>networks</code> 标签设置，用于限制某个用户登录的客户端地址，有 <code>IP</code> 地址、<code>host</code> 主机名称以及正则匹配三种形式，可以任选其中一种进行设置：</p><ul><li><code>IP</code> 地址：直接使用 <code>IP</code> 地址进行设置。</li><li><code>host</code> 主机名称：通过 <code>host</code> 主机名称进行设置。</li><li>正则匹配：通过表达式来匹配 <code>host</code> 名称。</li></ul></li><li><p>数据库与字典访问权限<br>在客户端连入服务之后，可以进一步限制某个用户数据库和字典的访问权限，他们分别通过 <code>allow_databases</code> 和 <code>allow_dictionaries</code> 标签进行设置。如果不进行设置，则表示无任何限制。</p></li></ol><h4 id="查询权限"><a href="#查询权限" class="headerlink" title="查询权限"></a>查询权限</h4><p>查询权限是整个权限体系中的第二层防护，它决定了一个用户能够执行的查询语句。查询权限可以分为以下四类：</p><ul><li>读权限：包括 <code>SELECT</code> 、 <code>EXISTS</code> 、 <code>SHOW</code> 、 <code>DESCRIBE</code> 查询。</li><li>写权限：包括 <code>INSERT</code> 、 <code>OPTIMIZE</code> 查询。</li><li>设置权限：包括 <code>SET</code> 查询。</li><li><code>DDL</code> 权限：包括 <code>CREATE</code> 、 <code>DROP</code> 、 <code>ALTER</code> 、 <code>RENAME</code> 、 <code>ATTACH</code> 、 <code>DETACH</code> 和 <code>TRUNCATE</code> 查询。</li></ul><p>以上四类权限通过以下两种配置标签控制：</p><ul><li><code>readonly</code>：读权限、写权限和设置权限均由此标签控制，共有三种取值：</li><li>当取值为 <code>0</code> 时，不进行任何限制。</li><li>当取值为 <code>1</code> 时，只拥有读权限。</li><li>当取值为 <code>2</code> 时，拥有读权限和设置权限。</li><li><code>allow_ddl</code>：<code>DDL</code> 权限由此标签控制，共有两种取值：</li><li>当取值为 <code>0</code> 时，不允许 <code>DDL</code> 查询。</li><li>当取值为 <code>1</code> 时，允许 <code>DDL</code> 查询。</li></ul><h4 id="数据行级权限"><a href="#数据行级权限" class="headerlink" title="数据行级权限"></a>数据行级权限</h4><p>数据权限是整个权限体系中的第三层防护，它决定了一个用户能够看到什么数据。数据权限使用 <code>databases</code> 标签定义，他是用户定义中的一项选填参数，<code>databases</code> 通过定义用户级别的查询过滤器来实现数据的行级粒度查询，规则定义如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">databases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">database_name</span>&gt;</span> <span class="comment">&lt;!-- 数据库名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table_name</span>&gt;</span> <span class="comment">&lt;!-- 表名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span>&gt;</span> id &lt; 10 <span class="tag">&lt;/<span class="name">filter</span>&gt;</span> <span class="comment">&lt;!-- 数据过滤条件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table_name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">database_name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">databases</span>&gt;</span></span><br></pre></td></tr></table></figure><p><small>对于数据权限的使用有一点需要明确，在使用这项功能之后 <code>PREWHERE</code> 优化将不会生效。</small></p><hr><h3 id="熔断机制"><a href="#熔断机制" class="headerlink" title="熔断机制"></a>熔断机制</h3><p>熔断是限制资源被过度使用的一种自我保护机制，当使用的资源达到阈值时，那正在进行的操作都会被中断。</p><h4 id="时间周期的累计用量熔断"><a href="#时间周期的累计用量熔断" class="headerlink" title="时间周期的累计用量熔断"></a>时间周期的累计用量熔断</h4><p>这种方式下系统资源的用量是按照时间周期累积统计的，当累积量达到阈值，则直到下个计算周期开始之前，该用户无法继续进行操作。这种方式通过在 <code>user.xml</code> 中的 <code>quota</code> 标签进行配置，示例如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">quotas</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">default</span>&gt;</span> <span class="comment">&lt;!-- 自定义名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">internal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">duartion</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">duartion</span>&gt;</span> <span class="comment">&lt;!-- 时间周期 单位：秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">queries</span>&gt;</span>0<span class="tag">&lt;/<span class="name">queries</span>&gt;</span> <span class="comment">&lt;!-- 在周期内允许执行的查询次数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">errors</span>&gt;</span>0<span class="tag">&lt;/<span class="name">errors</span>&gt;</span> <span class="comment">&lt;!-- 在周期内允许发生异常的次数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result_rows</span>&gt;</span>0<span class="tag">&lt;/<span class="name">result_rows</span>&gt;</span> <span class="comment">&lt;!-- 在周期内允许查询返回的结果行数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">read_rows</span>&gt;</span>0<span class="tag">&lt;/<span class="name">read_rows</span>&gt;</span> <span class="comment">&lt;!-- 在周期内允许分布式查询节点读取的数据行数--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">execution_time</span>&gt;</span>0<span class="tag">&lt;/<span class="name">execution_time</span>&gt;</span> <span class="comment">&lt;!-- 在周期内允许执行的查询时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">internal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">default</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">quotas</span>&gt;</span></span><br></pre></td></tr></table></figure><p><small>上述配置的 <code>0</code> 表示不做限制。</small></p><h4 id="单次查询的用量熔断"><a href="#单次查询的用量熔断" class="headerlink" title="单次查询的用量熔断"></a>单次查询的用量熔断</h4><p>这种方式下系统资源的用量是按照单词查询统计的，而具体的熔断规则则是由许多不同配置项组成，这些配置项需要定义在用户 <code>profile</code> 中。</p><ol><li><p>针对普通查询的熔断配置</p><ul><li><code>max_memory_usage</code>：在单个 <code>ClickHouse</code> 服务进程中，运行一次查询限制使用的最大内存量，默认值为 <code>10GB</code>。</li><li><code>max_memory_usage_for_user</code>：以用户为单位进行统计，单个用户在运行查询时限制使用的最大内存量，默认值为 <code>0</code>，即不做限制。</li><li><code>max_memory_usage_for_all_queries</code>：所有运行的查询累加在一起所限制的最大内存量，默认值为 <code>0</code>，即不做限制。</li></ul></li><li><p>针对数据写入和聚合查询相关的熔断配置</p><ul><li><code>max_partitions_per_insert_block</code>：在单次写入时，限制创建的最大分区个数，默认值为 <code>100</code> 个。</li><li><code>max_rows_to_group_by</code>：在执行 <code>GROUP BY</code> 聚合查询时限制去重后聚合 <code>KEY</code> 的最大个数，默认值为 <code>0</code>，即不做限制。当超过阈值时，处理方式由 <code>group_by_overflow_mode</code> 参数指定。</li><li><code>group_by_overflow_mode</code> 当 <code>max_rows_to_group_by</code> 熔断规则触发时，<code>group_by_overflow_mode</code> 会提供三种处理方式：<ul><li><code>throw</code>：抛出异常，默认值。</li><li><code>break</code>：立即停止查询，并返回当前数据。</li><li><code>any</code>：仅根据当前已存在的聚合 <code>KEY</code> 继续完成聚合查询。</li></ul></li><li><code>max_bytes_before_external_group_by</code>：在执行 <code>GROUP BY</code> 聚合查询时限制使用的最大内存量，默认值为 <code>0</code>，即不做限制。当超过阈值时，聚合查询将会进一步借用本地磁盘。</li></ul></li></ol><hr><h3 id="数据备份"><a href="#数据备份" class="headerlink" title="数据备份"></a>数据备份</h3><p>前面我们学习了副本，那么还需要备份吗？当然是需要的，因为副本是不能解决误删除数据这类行为，因此 <code>ClickHouse</code> 提供如下几种方式。</p><h4 id="导出文件备份"><a href="#导出文件备份" class="headerlink" title="导出文件备份"></a>导出文件备份</h4><p>如果数据的体量较小，可以通过 <code>dump</code> 的形式将数据导出为本地文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clickhouse-client --query=&quot;select * from tast_backup&quot; &gt; /chbase/test_backup.csv</span><br></pre></td></tr></table></figure><p>将备份再次导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /chbase/test_backup.csv | clickhouse-client --query &quot;insert into test_backup format TSV&quot;</span><br></pre></td></tr></table></figure><p><small>上述 <code>dump</code> 形式的优势在于可以利用 <code>select</code> 查询并筛选数据，然后按需备份。</small></p><h4 id="通过快照备份"><a href="#通过快照备份" class="headerlink" title="通过快照备份"></a>通过快照备份</h4><p>快照表实质上就是普通的数据表，通常按照业务规定的备份频率创建。</p><h4 id="按分区备份"><a href="#按分区备份" class="headerlink" title="按分区备份"></a>按分区备份</h4><p>基于数据分区的备份，<code>ClickHouse</code> 目前提供了 <code>FREEZE</code> 和 <code>FETCH</code> 两种方式，使用方法可以参考之前的关键字。</p><hr><h3 id="服务监控"><a href="#服务监控" class="headerlink" title="服务监控"></a>服务监控</h3><p>基于原生 <code>ClickHouse</code> 进行监控，可以从<strong>系统表</strong>和<strong>查询日志</strong>两方面入手。</p><h4 id="系统表"><a href="#系统表" class="headerlink" title="系统表"></a>系统表</h4><p>在众多的 <code>SYSTEM</code> 系统表中，主要由以下三张表支撑对 <code>ClickHouse</code> 运行指标的查询，分别是 <code>metrics</code> 、 <code>events</code> 和 <code>asynchronous_metrics</code>。</p><ol><li><p><code>metrics</code><br><code>metrics</code> 表用于统计 <code>ClickHouse</code> 服务在运行时，当前正在运行的高层次的概要信息，包括正在执行的查询总次数、正在发生的合并操作总次数等。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> system.metrics limit <span class="number">5</span></span><br></pre></td></tr></table></figure></li><li><p><code>events</code><br><code>events</code> 用于统计 <code>ClickHouse</code> 服务在运行过程中已经执行过的高层次的累积概要信息，包括总的查询次数、总的 <code>SELECT</code> 查询次数等。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> event, <span class="keyword">value</span> <span class="keyword">from</span> system.events limit <span class="number">5</span></span><br></pre></td></tr></table></figure></li><li><p><code>asynchronous_metrics</code><br><code>asynchronous_metrics</code> 用于统计 <code>ClickHouse</code> 服务运行过程中，当前正在后台异步运行的高层次的概要信息，包括当前分配的内存、执行队列中的任务数量等。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> system.asynchronous_metrics limit <span class="number">5</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="查询日志"><a href="#查询日志" class="headerlink" title="查询日志"></a>查询日志</h4><p>查询日志目前主要分为六种类型，分别从不同的角度记录 <code>ClickHouse</code> 的操作行为。<br>所有查询日志在默认配置下都是关闭状态，需要在 <code>config.xml</code> 配置中进行更改，在配置开启之后会自动生成相应的系统表以供查询。</p><ol><li><p><code>query_log</code> 是最常用的查询日志，记录了 <code>ClickHouse</code> 服务中所有已执行的查询记录，定义方式如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">query_log</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">databases</span>&gt;</span>system<span class="tag">&lt;/<span class="name">databases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>query_log<span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">partition_by</span>&gt;</span>toYYYYMM(event_date)<span class="tag">&lt;/<span class="name">partition_by</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">flush_interval_milliseconds</span>&gt;</span>7500<span class="tag">&lt;/<span class="name">flush_interval_milliseconds</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">query_log</span>&gt;</span></span><br></pre></td></tr></table></figure><p><small>如果需要单独为某个用户开启该功能，可以在 <code>user.xml</code> 的 <code>profile</code> 配置中使用 <code>&lt;log_query&gt;1&lt;/log_query&gt;</code> 配置。</small></p></li><li><p><code>query_thread_log</code> 记录所有线程的执行查询信息，定义方式如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">query_thread_log</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">databases</span>&gt;</span>system<span class="tag">&lt;/<span class="name">databases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>query_thread_log<span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">partition_by</span>&gt;</span>toYYYYMM(event_date)<span class="tag">&lt;/<span class="name">partition_by</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">flush_interval_milliseconds</span>&gt;</span>7500<span class="tag">&lt;/<span class="name">flush_interval_milliseconds</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">query_thread_log</span>&gt;</span></span><br></pre></td></tr></table></figure><p><small>如果需要单独为某个用户开启该功能，可以在 <code>user.xml</code> 的 <code>profile</code> 配置中使用 <code>&lt;log_query_threads&gt;1&lt;/log_query_threads&gt;</code> 配置。</small></p></li><li><p><code>part_log</code> 日志记录 <code>MergeTree</code> 系列表引擎的分区操作日志，定义方式如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">part_log</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">databases</span>&gt;</span>system<span class="tag">&lt;/<span class="name">databases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>part_log<span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">flush_interval_milliseconds</span>&gt;</span>7500<span class="tag">&lt;/<span class="name">flush_interval_milliseconds</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">part_log</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>text_log</code> 日志记录 <code>ClickHouse</code> 运行过程中产生的一系列打印日志，包括 <code>INFO</code> 、 <code>DEBUG</code> 和 <code>Trace</code>，定义方式如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">text_log</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">databases</span>&gt;</span>system<span class="tag">&lt;/<span class="name">databases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>text_log<span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">flush_interval_milliseconds</span>&gt;</span>7500<span class="tag">&lt;/<span class="name">flush_interval_milliseconds</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">text_log</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><code>metric_log</code> 日志用于将 <code>system.metrics</code> 和 <code>system.events</code> 中的数据汇聚到一起，定义方式如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">metric_log</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">databases</span>&gt;</span>system<span class="tag">&lt;/<span class="name">databases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>metric_log<span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">flush_interval_milliseconds</span>&gt;</span>7500<span class="tag">&lt;/<span class="name">flush_interval_milliseconds</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">collect_interval_milliseconds</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">collect_interval_milliseconds</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">metric_log</span>&gt;</span></span><br></pre></td></tr></table></figure><p><small>其中 <code>collect_interval_milliseconds</code> 表示收集 <code>metrics</code> 和 <code>events</code> 数据的时间周期。</small></p></li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;本文将对 &lt;code&gt;ClickHouse&lt;/code&gt; 管理与运维相关的知识进行说明，主要包含&lt;strong&gt;用户&lt;/strong&gt;、&lt;strong&gt;权限&lt;/strong&gt;、&lt;strong&gt;熔断机制&lt;/strong&gt;、&lt;strong&gt;数据备份&lt;/strong&gt;和&lt;strong&gt;服务监控&lt;/strong&gt;等知识。&lt;/p&gt;</summary>
    
    
    
    
    <category term="ClickHouse" scheme="https://blog.vgbhfive.cn/tags/ClickHouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse-分布式</title>
    <link href="https://blog.vgbhfive.cn/ClickHouse-%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>https://blog.vgbhfive.cn/ClickHouse-%E5%88%86%E5%B8%83%E5%BC%8F/</id>
    <published>2022-09-10T12:18:11.000Z</published>
    <updated>2023-01-01T15:46:36.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>随着业务线数据量的突飞猛进、服务器的意外宕机，这些都是底层基础服务会遇到的问题，因此 <code>ClickHouse</code> 就设计了<strong>集群</strong>、<strong>副本</strong>和<strong>分片</strong>这三个帮手来帮忙。</p><span id="more"></span><p>集群是副本和分片的基础，他将 <code>ClickHouse</code>. 的服务拓扑由单节点延伸为多节点，但是他又不像 <code>hadoop</code> 那样的系统，要求所有的节点都组成一个大集群。<code>ClickHouse</code> 的集群配置非常灵活，用户既可以将所有节点组成一个大集群，也可以按照业务的诉求将节点划分为多个小集群。<br>在每个小集群区域之间，他们的节点、分区和副本数量可以各不相同。从总体来看，集群定义了多个节点的拓扑关系，这些节点在后续服务中会相互合作，而执行层面的具体内容则是由副本和分片来执行。</p><p>那么如何区分副本和分片呢？<br>在数据层面上，副本之间的数据完全相同，而分片之间数据是不同的。在功能层面上，副本的作用在于防止数据丢失，增加存储数据的冗余，而分片的目的在于实现数据的水平切分。</p><hr><h3 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h3><p>之前有说过 <code>ReplicatedMergeTree</code> 复制表引擎，该引擎可以实现应用副本的能力，他是在 <code>MergeTree</code> 表引擎的基础上实现了分布式协同的能力。<br>在 <code>MergeTree</code> 中，一个数据分区从开始创建到全部完成，会经历两类存储区域：</p><ul><li>内存，数据首先会被写入到内存缓冲区。</li><li>本地磁盘，数据接着会被写入 <code>tmp</code> 临时目录分区，待全部完成后再将临时目录重命名为正式分区。</li></ul><p>而 <code>ReplicatedMergeTree</code> 在上述的基础上增加了 <code>ZooKeeper</code> 的部分，他会进一步在 <code>ZooKeeper</code> 内部创建一系列的监听节点，并以此实现多个实例之间的通信，并且在整个通信过程中，<code>ZooKeeper</code> 不会涉及到任何的数据传输。</p><p>那么我们总结下副本的特点：</p><ul><li>依赖 <code>ZooKeeper</code>，在执行 <code>insert</code> 和 <code>alter</code> 查询时 <code>ReplicatedMergeTree</code> 需要借助 <code>ZooKeeper</code> 的分布式协同能力，以实现多个副本之间的同步，但是在 <code>select</code> 副本时并不需要使用。</li><li>表级别的副本，副本是在表级别定义的，所以每张表的副本配置都可以按照他的实际需求进行个性化定义，包括副本的数量、副本在集群中的分布位置等。</li><li>多主架构，可以在任意一个副本上执行 <code>insert</code> 和 <code>alter</code> 查询，他们的效果都是相同的。</li><li><code>Block</code> 数据块，在执行 <code>insert</code> 命令时，会依据 <code>max_insert_block_size</code> 的大小将数据切分为若干个 <code>Block</code> 数据块，因此 <code>Block</code> 数据块是写入的基本单元，并且具有写入的唯一性和原子行。</li><li>原子性，在数据写入时，一个 <code>Block</code> 块内的数据要么全部写入成功，要么全部失败。</li><li>唯一性，在写入一个 <code>Block</code> 数据块时，会按照当前 <code>Block</code> 数据块的数据顺序、数据行和数据大小等指标计算 <code>Hash</code> 信息摘要并记录在案。如果后续遇到相同的 <code>Hash</code> 摘要则该数据块会被忽略。</li></ul><h4 id="ZooKeeper-配置方式"><a href="#ZooKeeper-配置方式" class="headerlink" title="ZooKeeper 配置方式"></a><code>ZooKeeper</code> 配置方式</h4><p>首先新建一个 <code>metrika.xml</code> 的配置文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span> <span class="comment">&lt;!-- ZooKeeper 配置，名称自定义 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;1&quot;</span>&gt;</span> <span class="comment">&lt;!-- 节点配置，可以配置多个地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">host</span>&gt;</span>host1.vgbhfive.cn<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure><p>接着在全局配置 <code>config.xml</code> 中使用 <code>&lt;include_from&gt;</code> 标签导入刚才定义的配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">include_from</span>&gt;</span>/etc/clickhouse-server/cpnfig.d/metrika.xml<span class="tag">&lt;/<span class="name">include_from</span>&gt;</span></span><br></pre></td></tr></table></figure><p><small><code>incl</code> 与 <code>metrika.xml</code> 配置文件中的节点名称彼此要相互对应。</small></p><p>另外 <code>ClickHouse</code> 还在系统表中提供了一张 <code>zookeeper</code> 的代理表，通过这个表可以使用 <code>SQL</code> 查询读取远端 <code>ZooKeeper</code> 内的数据，不过查询时需要指定 <code>path</code> 条件才能查询到数据。</p><h4 id="副本定义形式"><a href="#副本定义形式" class="headerlink" title="副本定义形式"></a>副本定义形式</h4><p>首先由于增加了数据的冗余存储，所以降低了数据丢失的风险；其次由于副本采用多主架构，所以每个副本实例都可以作为数据读、写的入口，但这都增加了节点的负载。</p><p><code>ReplicatedMergeTree</code> 定义方式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = ReplicatedMergeTree(&#x27;zk_path&#x27;, &#x27;replica_name&#x27;)</span><br></pre></td></tr></table></figure><p> 在上述配置中有 <code>zk_path</code> 和 <code>replica_name</code> 两项配置：</p><ul><li><code>zk_path</code> 用于指定在 <code>ZooKeeper</code> 中创建的数据表的路径，路径名称是自定义的，可以设置成自己希望的任何路径。<br>当然也有一些约定俗成的配置：<ul><li><code>/clickhouse/tables/</code> 是约定的路径固定前缀，表示存放数据表的根路径。</li><li><code>&#123;shard&#125;</code> 表示分片编号，通常使用数字来替代。</li><li><code>table_name</code> 表示数据表的名称，为了维护方便通常采用与物理表相同的名字。</li></ul></li><li><code>replica_name</code> 是定义副本名称，该名称是区分不同副本实例的唯一标识。</li></ul><p><small>对于 <code>zk_path</code> 而言，同一张数据表的同一个分片的不同副本，应该定义相同的路径。而对于 <code>replica_name</code> 而言，同一张数据表的同一个分片的不同副本，应该定义不同的名称。</small></p><h4 id="ReplicatedMergeTree-原理解析"><a href="#ReplicatedMergeTree-原理解析" class="headerlink" title="ReplicatedMergeTree 原理解析"></a><code>ReplicatedMergeTree</code> 原理解析</h4><p>在 <code>ReplicatedMergeTree</code> 的核心逻辑中，大量运用了 <code>ZooKeeper</code> 的能力，以实现多个 <code>ReplicatedMergeTree</code> 副本实例之间的协同，包括主副本选举、副本状态感知、操作日志分发、任务队列和 <code>BlockID</code> 去重判断等。<br>在执行 <code>INSERT</code> 数据写入、<code>MERGE</code> 分区和 <code>MUTATION</code> 操作的时候都会涉及到 <code>ZooKeeper</code> 的通信，但是在通信的过程中，并不会涉及到任何表数据的传输，在查询数据时也不会访问 <code>ZooKeeper</code>。</p><h5 id="ZooKeeper-节点结构"><a href="#ZooKeeper-节点结构" class="headerlink" title="ZooKeeper 节点结构"></a><code>ZooKeeper</code> 节点结构</h5><p><code>ReplicatedMergeTree</code> 依赖 <code>ZooKeeper</code> 的事件监听机制以实现各个副本之间的协同。因此在每个 <code>ReplicatedMergeTree</code> 表的创建过程中，会以 <code>zk_path</code> 为根路径创建一组监听节点，按照作用不同，监听节点可以大致分为一下几点：</p><ul><li>元数据<ul><li><code>/metadata</code> 保存元数据信息，包括主键、分区键、采样表达式等。</li><li><code>/columns</code> 保存列字段信息，包括列名称和数据类型。</li><li><code>/replicas</code> 保存副本名称，对应设置参数中的 <code>replica_name</code>。</li></ul></li><li>判断标识<ul><li><code>/leader_election</code> 用于主副本的选举工作，主副本会主导 <code>MERGE</code> 和 <code>MUTATION</code> 操作，这些任务都是在主副本完成之后再借助 <code>ZooKeeper</code> 将消息事件分发到其他副本。</li><li><code>/blocks</code> 记录 <code>Block</code> 数据块的 <code>Hash</code> 信息摘要，以及对应的 <code>partition_id</code>。通过 <code>Hash</code> 摘要能够判断 <code>Block</code> 数据块是否重复；通过 <code>partition_id</code> 则能找到需要同步的数据分区。</li><li><code>block_numbers</code> 按照分区的写入顺序，以相同的顺序记录 <code>partition_id</code>，各个副本在本地进行 <code>MERGE</code> 时都会依照相同的 <code>block_numbers</code> 顺序进行。</li><li><code>quorum</code> 记录 <code>quorum</code> 的数量，当至少有 <code>quorum</code> 数量的副本写入成功后，整个写入操作才算成功。<code>quorum</code> 的数量有 <code>insert_quorum</code> 参数控制，默认值为 <code>0</code>。</li></ul></li><li>操作日志<ul><li><code>/log</code> 常规操作日志，保存副本需要执行的任务指令。</li><li><code>/mutations</code> 操作日志，作用与 <code>/log</code> 日志类似，当执行 <code>ALERT DELETE</code> 或 <code>ALERT UPDATE</code> 查询时，操作指令会被添加到这个节点。</li><li><code>/replicas/&#123;replica_name&#125;/*</code> 每个副本各自的节点下的一组监听节点，用于指导副本在本地执行具体的任务指令<ul><li><code>/queue</code> 任务队列节点，用于执行具体的操作任务。</li><li><code>/log_pointer</code> <code>log</code> 日志指针节点，记录最后一次执行的 <code>log</code> 日志下标信息。</li><li><code>/mutation_pointer</code> <code>mutations</code> 日志指针节点，记录了最后一次执行的 <code>mutatutions</code> 日志名称。</li></ul></li></ul></li></ul><h5 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h5><p><code>/log</code> 和 <code>/mutations</code> 他们犹如通信路由器，是分发操作指令的信息通道，而发送指令的方式则是为这些父节点添加子节点。所有的副本实例都会监听父节点的变化，当有子节点被添加时都会被其他副本实时感知。</p><p>被添加的子节点统一被抽象为 <code>Entry</code> 对象，而具体实现则是 <code>LogEntry</code> 和 <code>MutationEntry</code> 对象承载，分别对应 <code>/log</code> 和 <code>/mutations</code> 节点：</p><ul><li><code>LogEntry</code> 用于封装 <code>/log</code> 子节点信息，核心属性如下：<ul><li><code>source replica</code> 发送这条 <code>Log</code> 指令的副本来源，对应 <code>replica_name</code>。</li><li><code>type</code> 操作指令类型，主要有 <code>get</code>、 <code>merge</code> 和 <code>mutate</code> 三种，分别对应从远程副本下载分区、合并分区和 <code>MUTATION</code> 操作。</li><li><code>block_id</code> 当前分区的 <code>BlockID</code>，对应 <code>/blocks</code> 路径下子节点的名称。</li><li><code>partition_name</code> 当前分区目录的名称。</li></ul></li><li><code>MutationEntry</code>用于封装 <code>/mutations</code> 子节点信息，核心属性如下：<ul><li><code>source replica</code> 发送这条 <code>MUTATION</code> 指令的副本来源，对应 <code>replica_name</code>。</li><li><code>commands</code> 操作指令，主要有 <code>ALERT DELETE</code> 和 <code>ALERT UPDATE</code>。</li><li><code>mutation_id</code> <code>MUTATION</code> 操作的版本号。</li><li><code>partition_id</code> 当权分区目录的 <code>ID</code>。</li></ul></li></ul><h5 id="副本协同流程"><a href="#副本协同流程" class="headerlink" title="副本协同流程"></a>副本协同流程</h5><h6 id="写入执行流程"><a href="#写入执行流程" class="headerlink" title="写入执行流程"></a>写入执行流程</h6><p>当需要在 <code>ReplicatedMergeTree</code> 中执行 <code>INSERT</code> 查询以写入数据时，即会进入 <code>INSERT</code> 核心流程，整体流程从上至下按照时间顺序进行，大致可以分为八个步骤。</p><ol><li><p>创建第一个副本实例</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> replica_sales_1 &#123;</span><br><span class="line">    id String,</span><br><span class="line">    price Float64,</span><br><span class="line">    create_time DateTime</span><br><span class="line">&#125; ENGINW <span class="operator">=</span> ReplicatedMergeTree(<span class="string">&#x27;/clickhouse/tables/01/replicated_sales_1&#x27;</span>, <span class="string">&#x27;ch5.vgbhfive.cn&#x27;</span>)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(create_time)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> id</span><br></pre></td></tr></table></figure><p>在创建的过程中 <code>ReplicatedMergeTree</code> 会进行一些初始化操作：</p><ul><li>根据 <code>zk_path</code> 初始化所有的 <code>ZooKeeper</code> 节点。</li><li>在 <code>/replicas/</code> 节点下注册自己的副本实例 <code>ch5.vgbhfive.cn</code>。</li><li>启动监听任务，监听 <code>/log</code> 日志节点。</li><li>参与副本选举，选举出主副本，选举的方式是向 <code>/leader_election/</code> 插入子节点，第一个插入成功的副本就是主副本。</li></ul></li><li><p>创建第二个副本实例<br>与上述第一个创建副本实例类似，不同之处在于实例名称。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> replica_sales_1 &#123;</span><br><span class="line">    id String,</span><br><span class="line">    price Float64,</span><br><span class="line">    create_time DateTime</span><br><span class="line">&#125; ENGINW <span class="operator">=</span> ReplicatedMergeTree(<span class="string">&#x27;/clickhouse/tables/01/replicated_sales_1&#x27;</span>, <span class="string">&#x27;ch6.vgbhfive.cn&#x27;</span>)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(create_time)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> id</span><br></pre></td></tr></table></figure><p>在创建过程中，第二个 <code>ReplicatedMergeTree</code> 同样会进行一些初始化操作：</p><ul><li>在 <code>/replicas/</code> 节点下注册自己的副本实例 <code>ch6.vgbhfive.cn</code>。</li><li>启动监听任务，监听 <code>/log</code> 日志节点。</li><li>参数副本选举，选举出主副本。</li></ul></li><li><p>向第一个实例中写入数据<br>现在尝试向第一个副本 <code>ch5</code> 写入数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO TABLE replicated_sales_1 VALUES(&#x27;A001&#x27;, 100, &#x27;2022-05-15 00:00:00&#x27;)</span><br></pre></td></tr></table></figure><p>上述命令执行完毕后，首先会在本地完成分区目录的写入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Renaming temporary part tmp_insert_202205_1_1_0 to 202205_0_0_0</span><br></pre></td></tr></table></figure><p>接着向 <code>/blocks</code> 节点写入该数据分区的 <code>block_id</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Wrote block with ID &#x27;202205_xxxxx_yyyy&#x27;</span><br></pre></td></tr></table></figure><p>该 <code>block_id</code> 将作为后续去重操作的判断依据。如果此时再重复执行刚才的语句，试图写入重复数据，则会抛出异常，即副本会自动忽略 <code>block_id</code> 重复的待写入数据。<br>此外如果设置了 <code>insert_quorum</code> （默认参数为 <code>0</code>），并且 <code>insert_quorum &gt;= 2</code>，则 <code>ch5  </code> 会进一步监控已完成写入操作的副本个数，只有当写入副本个数大于或等于 <code>insert_quorum</code> 时，整个写入操作才会成功。</p></li><li><p>由第一个副本实例推送 <code>Log</code> 日志<br>在 <code>3</code> 步骤完成之后，会继续执行 <code>insert</code> 的副本向 <code>/log</code> 节点推送操作日志。日志的编号是 <code>/log/log-00000000</code>，而 <code>LogEntry</code> 的核心属性如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/log/log-00000000</span><br><span class="line">source replica: ch5.vgbhfive.cn</span><br><span class="line">block_id: 202205_xxxxx</span><br><span class="line">type: get</span><br><span class="line">partition_name: 202205_0_0_0</span><br></pre></td></tr></table></figure><p>从日志内容中可以看出，操作类型为 <code>get</code> 下载，而需要下载的分区时 <code>202205_0_0_0</code>。其余所有副本都会基于 <code>Log</code> 日志以相同的顺序执行命令。</p></li><li><p>第二个副本实例拉取 <code>Log</code> 日志<br><code>ch6</code> 副本会一直监听 <code>/log</code> 节点变化，当 <code>ch5</code> 推送日志之后，<code>ch6</code>  便会触发日志的拉取任务并更新 <code>log_pointer</code>，将其指向最新日志下标：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/replicas/ch6.vgbhfive.cn/log_pointer: 0</span><br></pre></td></tr></table></figure><p>在拉取 <code>LogEntry</code> 之后，并不会直接执行，而是将其转为任务对象放至队列：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/replicas/ch6.vgbhfive.cn/queues/</span><br><span class="line">Pulling 1 entries to queue: log-00000000 to log-00000000</span><br></pre></td></tr></table></figure><p><small>上述 <code>LogEntry</code> 放入队列是因为在复杂的情况下，会连续收到许多个 <code>LogEntry  </code> ，所以使用队列消化任务也是一种合理的设计。</small></p></li><li><p>第二个副本实例向其他副本发起下载请求<br><code>ch6</code> 基于 <code>/queue</code> 队列开始执行任务，当看到 <code>type</code> 为 <code>get</code>    时，<code>ReplicatedMergeTree</code> 会明白此时在远端的其他副本已经成功写入数据分区，而自己需要同步这些数据。<br><code>ch6</code> 上的第二个副本实例会开始选择一个远端的其他副本作为数据的下载来源。远端副本的选择算法大致是这样的：</p><ul><li>从 <code>/replicas</code> 节点拿到所有的副本节点。</li><li>遍历这些副本选取其中一个。选取的副本需要拥有最大的 <code>log_pointer</code> 下标，并且 <code>/queue</code> 子节点数量最少。<code>log_pointer</code> 下标最大，则意味该副本执行的日志最多，数据应该更加完整；而 <code>/queue</code> 最小，则意味着该副本目前的任务执行负担最小。</li></ul><p> <small>在这个实例中，算法选择的副本实例是 <code>ch5</code>。</small></p></li><li><p>第一个副本实例响应数据下载<br><code>ch5</code> 的 <code>DataPartsExchange</code> 端口服务接收到调用请求，在得知对方来意之后，根据参数做出响应，将本地分区 <code>202205_0_0_0</code> 基于 <code>DataPartsExchange</code> 的服务响应发送回 <code>ch6</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sending part 202205_0_0_0</span><br></pre></td></tr></table></figure></li><li><p>第二个实例下载数据并完成本地写入<br><code>ch6</code> 副本在接收到 <code>ch5</code> 的分区数据后，首先将其写入到临时目录中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmp_fetch_202205_0_0_0</span><br></pre></td></tr></table></figure><p>待全部数据接收完成之后，重命名该目录：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Renaming temporary part tmp_fetch_202205_0_0_0 to 202205_0_0_0</span><br></pre></td></tr></table></figure><p>至此，整个写入结束。</p></li></ol><p>在整个 <code>insert</code> 的写入过程中，<code>ZooKeeper</code> 不会进行任何实质性的数据传输。本着谁执行谁负责的原则，由写入数据的实例负责发送 <code>Log</code> 日志、通知其他实例、监控是否完成、返回下载数据。</p><h6 id="MERGE-执行流程"><a href="#MERGE-执行流程" class="headerlink" title="MERGE 执行流程"></a><code>MERGE</code> 执行流程</h6><p>当需要在 <code>ReplicatedMergeTree</code> 中触发分区合并动作时，即会进入这个部分的流程，无论 <code>MERGE</code> 操作从哪个副本发起，其合并计划都会交由主副本来制定。</p><ol><li><p>创建远程连接，尝试与主副本通信。<br>首先在 <code>ch6</code> 节点执行 <code>OPTIMIZE</code> 强制执行 <code>MERGE</code> 合并，此时 <code>ch6</code> 通过 <code>replicas</code> 找到主副本 <code>ch5</code>，并尝试建立与它的远程连接。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimize table replicated_sales_1</span><br><span class="line">Connection (ch5.vgbhfive.cn:9000): Connecting. Database: default. User: default</span><br></pre></td></tr></table></figure></li><li><p>主副本接收通信<br>主副本 <code>ch5</code> 接收并建立来自远端副本 <code>ch6</code> 的连接。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Connected ClickHouse Follower replica version 19.17.0, revision: 54428, database: default, user: default</span><br></pre></td></tr></table></figure></li><li><p>由主副本制定 <code>MERGE</code> 计划并推送 <code>Log</code> 日志<br>由主副本 <code>ch5   </code> 制定 <code>MERGE</code> 计划，并判断哪些分区需要被合并。在选定之后 <code>ch5</code> 将合并计划转换为 <code>Log</code> 日志对象并推送 <code>Log</code> 日志，以通知所有副本开始合并。日志的核心信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type: merge</span><br><span class="line">202205_0_0_0</span><br><span class="line">202205_1_1_0</span><br><span class="line">into</span><br><span class="line">202205_0_1_1</span><br></pre></td></tr></table></figure><p>从日志内容可以看出操作类型为 <code>MERGE</code> 合并，而这次需要合并的分区目录是 <code>202205_0_0_0</code> 和 <code>202205_0_1_1</code>。与此同时主副本还会锁住执行线程，对日志的接收情况进行监听：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Waiting for queue-0000000002 to disapper from ch5.vgbhfive.cn queue</span><br></pre></td></tr></table></figure><p>其监听行为由 <code>replication_alter_partitions_sync</code> 参数控制，默认值为 <code>1</code>。</p><ul><li>参数为 <code>0</code> 时，不做任何等待。</li><li>参数为 <code>1</code> 时，只等主副本自身完成。</li><li>参数为 <code>0</code> 时，等待所有副本拉取完成。</li></ul></li><li><p>各个副本分别拉取 <code>Log</code> 日志<br><code>ch5</code> 和 <code>ch6</code> 两个副本实例将分别监听 <code>/log/log-00000002</code> 日志的推送，他们分别会拉取日志到本地，并推送到各自的 <code>/queue</code> 任务队列：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pulling 1 entries to queue : log-00000002 - log-000000002</span><br></pre></td></tr></table></figure></li><li><p>各个副本分别在本地执行 <code>MERGE</code><br><code>ch5</code> 和 <code>ch6</code> 基于各自的 <code>/queue</code> 队列开始执行任务：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Executing log entry to merge parts 202205_0_0_0, 202205_1_1_0 to 202205_0_1_1</span><br></pre></td></tr></table></figure><p>各个副本开始在本地执行 <code>MERGE</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Merged 2 parts: from 202205_0_0_0 to 202205_1_1_0</span><br></pre></td></tr></table></figure></li></ol><p>至此 <code>MERGE</code> 的合并过程 <code>ZooKeeper</code> 不会进行任何实质性的数据传输，所有的合并操作最终都是由各个副本在本地完成的。而无论合并动作在哪个副本被触发，最终都会交由主副本负责合并计划的制定、消息日志的推送以及日志接收情况的监控。</p><h6 id="MUTATION-执行流程"><a href="#MUTATION-执行流程" class="headerlink" title="MUTATION 执行流程"></a><code>MUTATION</code> 执行流程</h6><p>当对 <code>ReplicatedMergeTree</code> 执行 <code>ALTER DELTE</code> 或者 <code>ALTER UPDATE</code> 操作的时候，即会进入 <code>MUTATION</code> 部分的逻辑，与 <code>MERGE</code> 类似，无论 <code>MUTATION</code> 操作从哪个副本发起，首先都会由主副本进行响应。</p><ol><li>推送 <code>MUTATION</code> 日志<br>在 <code>ch6</code> 节点尝试通过 <code>DELETE</code> 来删除一行数据，执行如下命令：<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> replicated_sales_1 <span class="keyword">DELETE</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>上述命令执行之后该副本会接着执行两个重要事项：</li></ol><ul><li>创建 <code>MUTATION ID</code>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Created mutation with ID 000000000</span><br></pre></td></tr></table></figure></li><li>将 <code>MUTATION</code> 操作转换为 <code>MutatutionEntry</code> 日志，并推送到 <code>/mutations/00000000</code>。<code>Mutation</code> 的核心属性如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/mutations/000000000</span><br><span class="line">source replica: ch6.vgbhfive.cn</span><br><span class="line">mutation_id: 2</span><br><span class="line">partition_id: 202205</span><br><span class="line">commands: DELETE where id = \&#x27;1\&#x27;</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li><p>所有副本实例各自监听 <code>MUTATION</code> 日志<br><code>ch5</code> 和 <code>ch6</code> 都会监听 <code>/mutations</code> 节点，因此有新的日志子节点加入都会被实时感知：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Loading 1 mutation entries: 000000000 - 000000000</span><br></pre></td></tr></table></figure><p>当监听到有新的 <code>MUTATION</code> 日志加入时，并不是所有副本都会直接做出响应，他们首先会判断自己是否为主副本。</p></li><li><p>由主副本实例响应 <code>MUTATION</code> 日志并推送 <code>Log</code> 日志<br>只有主副本才会响应 <code>MUTATION</code> 日志，主副本会将 <code>MUTATION</code> 日志转换为 <code>LogEntry</code> 日志并推送到 <code>/log</code> 节点，以通知各个副本执行具体的操作。日志的核心信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/log/log-000000003</span><br><span class="line">source replica: ch5.vgbhfive.cn</span><br><span class="line">block_id:</span><br><span class="line">type: mutate</span><br><span class="line">202005_0_1_1 to 202205_0_1_1_2</span><br></pre></td></tr></table></figure></li><li><p>各个副本实例分别拉取 <code>Log</code> 日志<br><code>ch5</code> 和 <code>ch6</code> 两个副本分别监听 <code>/log/log-000000002</code> 日志的推送，他们也会分别拉取日志到本地，并推送到各自的 <code>/queue</code> 任务队列：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pulling 1 entries to queue: log-0000000002 - log-0000000002</span><br></pre></td></tr></table></figure></li><li><p>各个副本实例分别在本地执行 <code>MUTATION</code><br><code>ch5</code> 和 <code>ch6</code> 基于各自的 <code>/queue</code> 队列开始执行任务：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Executing log entry to mutate part 202205_0_1_1 to 202205_0_1_1_2</span><br></pre></td></tr></table></figure><p>各个副本开始在本地执行 <code>MUTATION</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Cloning part 202205_0_1_1 to tmp_clone_202205_0_1_1_2</span><br><span class="line">Renaming temporary part tmp_clone_202205_0_1_1_2 to 202205_0_1_1_2.</span><br></pre></td></tr></table></figure></li></ol><p>至此在 <code>MUTATION</code> 的整个过程中 <code>ZooKeeper</code> 同样不会进行任何实质性的数据传输。所有的 <code>MUTATION</code> 操作，最终都是由各个副本在本地完成的，而 <code>MUTATION</code> 操作是经过 <code>/mutations</code> 节点实现分发的。本着谁执行谁负责的原则，执行命令的副本负责消息推送，但是无论在哪个副本执行最终都会被交由主副本，再由主副本负责推送 <code>Log</code> 日志，以通知各个副本最终的 <code>MUTATION</code> 逻辑，同时也由主副本对日志接收的情况进行监控。</p><h6 id="ALTER-执行流程"><a href="#ALTER-执行流程" class="headerlink" title="ALTER 执行流程"></a><code>ALTER</code> 执行流程</h6><p>当对 <code>ReplicatedMergeTree</code> 执行 <code>ALTER</code> 操作的时候，即会进入 <code>ALTER</code> 部分的逻辑，与前几个类似 <code>ALTER</code> 流程会简单许多，其执行流程并不会涉及 <code>/log</code> 日志的分发。</p><ol><li><p>修改共享元数据<br>在 <code>ch6</code> 节点尝试增加一个列字段，执行语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> replicated_sales_1 <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> id2 String</span><br></pre></td></tr></table></figure><p>执行之后，<code>ch6</code> 会修改 <code>ZooKeeper</code> 内的共享元数据节点：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/metadata, /columns</span><br><span class="line">Updated shared metadata nodes in ZooKeeper. Waiting for replicas to apply changes.</span><br></pre></td></tr></table></figure><p>数据修改之后，节点的版本号也会同时提升：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Version of metadata nodes in ZooKeeper changed. Waiting for structure write lock.</span><br></pre></td></tr></table></figure><p>与此同时，<code>ch6</code> 还会负责监听所有副本的修改完成情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Waiting for ch5.vgbhfive.cn to apply changes.</span><br><span class="line">Waiting for ch6.vgbhfive.cn to apply changes.</span><br></pre></td></tr></table></figure></li><li><p>监听共享元数据变更并各自执行本地修改<br><code>ch5</code> 和 <code>ch6</code> 两个副本分别监听共享元数据的变更，之后会分别对本地的元数据版本号与共享版本号进行对比。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Metadata changed in ZooKeeper. Applying changes locally.</span><br><span class="line">Applied changes to the metadata of the table.</span><br></pre></td></tr></table></figure></li><li><p>确认所有副本完成修改<br>确认所有副本均已完成修改：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER finished.</span><br><span class="line">Done processing query.</span><br></pre></td></tr></table></figure></li></ol><p>至此整个 <code>ALTER</code> 流程结束。在执行过程中，<code>ZooKeeper</code> 没有参与实质性的数据传输，所有的 <code>ALTER</code> 都是各个副本在本地完成的。</p><hr><h3 id="分片"><a href="#分片" class="headerlink" title="分片"></a>分片</h3><p>通过引入数据副本可以降低数据丢失的风险，并提升查询的性能，但是仍然有一个问题没有解决，那就是数据表的容量问题，到目前为止每个副本都是保存全量的数据。</p><p><code>ClickHouse</code> 中的每个节点都可以称为一个 <code>shard</code> （分片），对于一个完整的方案来说，还需要考虑数据在写入时数据如何被均匀地被写入到各个分片中，以及在数据查询时如何路由到每个分片并组合成结果集，所以 <code>ClickHouse</code> 地数据分片需要结合 <code>Distributed</code> 表引擎一同使用。</p><p><code>Distributed</code> 表引擎自身不存储任何数据，它能够作为分布式表的一层透明代理，在集群内部自动开展数据的写入、分发、查询、路由等工作。</p><h4 id="集群的配置方式"><a href="#集群的配置方式" class="headerlink" title="集群的配置方式"></a>集群的配置方式</h4><p>在 <code>ClickHouse</code> 中集群配置用 <code>shard</code> 代表分片，用 <code>replica</code> 代表副本。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 1 分片 0 副本</span><br><span class="line"><span class="tag">&lt;<span class="name">shard</span>&gt;</span><span class="comment">&lt;!-- 分片 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">replica</span>&gt;</span><span class="comment">&lt;!-- 副本 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line"># 1 分片 1 副本</span><br><span class="line"><span class="tag">&lt;<span class="name">shard</span>&gt;</span><span class="comment">&lt;!-- 分片 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">replica</span>&gt;</span><span class="comment">&lt;!-- 副本 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这样的配置貌似有点问题，其实分片更像是逻辑层面的分组，而无论是分片或时副本，承载他们的都是 <code>replica</code>，所以从某种角度来看，副本也是分片。</p><h4 id="集群分布式-DDL"><a href="#集群分布式-DDL" class="headerlink" title="集群分布式 DDL"></a>集群分布式 <code>DDL</code></h4><p>在默认情况下 <code>CREATE</code> 、 <code>DROP</code> 、 <code>RENAME</code> 、 <code>ALTER</code> 等 <code>DDL</code> 语句并不支持分布式执行，但是在加入集群配置后使用新的语法实现分布式 <code>DDL</code> 执行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span><span class="operator">/</span><span class="keyword">DROP</span><span class="operator">/</span>RENAME<span class="operator">/</span><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> OM CLUSTER cluster_name</span><br></pre></td></tr></table></figure><p><small><code>cluster_name</code> 对应了配置文件中的集群名称，<code>ClickHouse</code> 会根据集群配置信息分别去各个节点执行 <code>DDL</code> 语句。</small></p><h5 id="数据结构-1"><a href="#数据结构-1" class="headerlink" title="数据结构"></a>数据结构</h5><p>与 <code>ReplicatedMergeTree</code> 类似，分布式 <code>DDL</code> 语句在执行的过程中也需要借助 <code>ZooKeeper</code> 的协同能力，以实现日志分发。</p><ol><li><p><code>ZooKeeper</code> 内的节点结构<br>默认情况下分布式 <code>DDL</code> 在 <code>ZooKeeper</code> 内使用的根路径为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/clickhouse/task_queue/ddl</span><br></pre></td></tr></table></figure><p><small>该地址可以在 <code>config.xml</code> 中的 <code>distributed_ddl</code> 配置指定。</small><br>当然在此节点之下还有其他的监听节点，包含 <code>/query-[seq]</code>，其内包含 <code>DDL</code> 操作日志，每执行一次分布式 <code>DDL</code> 查询，在该节点下新增一条操作日志。<code>DDL</code> 操作日志使用 <code>ZooKeeper</code> 的持久顺序型节点，每条指令的名称以 <code>query-</code> 为前缀，后面的序号递增。在该操作日志下，还有两条状态节点：</p><ul><li><code>/query-[seq]/active</code> 用于状态监控等用途，在任务的执行过程中，在该节点下会临时保存当前集群内状态为 <code>active</code> 的节点。</li><li><code>/query-[seq]/finished</code> 用于检查任务完成情况，在任务的执行过程中，每当集群内的某个 <code>host</code> 执行完毕之后，就会在该节点下写入记录。</li></ul></li><li><p><code>DDLLongEntry</code> 日志对象的数据结构<br>在 <code>/query-[seq]</code> 下记录的日志信息由 <code>DDLLogEntry</code> 承载，拥有以下几个核心属性：</p><ul><li><code>query</code> 记录 <code>DDL</code> 查询的执行语句  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query: DROP TABLE default.test_1_local ON CLUSTER shard_2</span><br></pre></td></tr></table></figure></li><li><code>hosts</code> 记录指定集群的主机列表，集群由分布式 <code>DDL</code> 语句中的 <code>ON CLUSTER</code> 指定  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hosts: [&#x27;ch5.vgbhfive.cn:9000&#x27;, &#x27;ch6.vgbhfive.cn:9000&#x27;]</span><br></pre></td></tr></table></figure></li><li><code>initiator</code> 记录初始化 <code>host</code> 主机的名称，主机列表来源于初始化 <code>host</code> 节点上的集群  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initiator: ch5.vgbhfive.cn</span><br></pre></td></tr></table></figure></li></ul></li><li><p>分布式 <code>DDL</code> 的核心执行流程<br><img src="https://s2.loli.net/2022/09/10/xETJYlW2Ic4dft3.png" alt="click-4-1.png"><br>整个流程从上到下按照时间顺序进行，其大致分为三个步骤：</p><ul><li>推送 <code>DDL</code> 日志，在节点执行语句，本着谁执行谁负责原则，会由这个节点创建 <code>DDLLogEntry</code> 日志并将日志推送到 <code>ZooKeeper</code> ，同时也会由这个节点负责监控任务的执行进度。</li><li>拉取日志并执行，其余节点监听到日志推送，于是拉取日志到本地。首先判断自身 <code>host</code> 是否被包含在 <code>DDLLogEntry</code> 的 <code>hosts</code> 列表中，如果包含在内则进入执行流程，执行完毕后将状态写入 <code>finished</code> 节点；如果不包含则忽略这次日子的推送。</li><li>确认执行进度，在执行 <code>DDL</code> 语句之后，客户端会阻塞等待 <code>180</code> 秒后，以期望所有 <code>host</code> 执行完毕。如果等待时间超过 <code>180</code> 秒，则进入后台线程继续等待（等待时间由 <code>distributed_ddl_task_timeout</code> 参数执行，默认 <code>180</code> 秒）。</li></ul></li></ol><h4 id="Distributed-原理解析"><a href="#Distributed-原理解析" class="headerlink" title="Distributed 原理解析"></a><code>Distributed</code> 原理解析</h4><p><code>Distributed</code> 表引擎是分布式表的代名词，它自身不存储任何数据，而是作为数据分片的透明代理，能够自动路由数据至集群中的各个节点，所以 <code>Distributed</code> 表引擎需要和其他数据表引擎一起协同工作。<br>从实体表层面来看，一张分片由两部分组成：</p><ul><li>本地表：通常以 <code>_local</code> 为后缀进行命名。本地表是承接数据的载体，可以使用非 <code>Distributed</code> 的任意表引擎，一张本地表对应了一个数据分片。</li><li>分布式表：通常以 <code>_all</code> 为后缀进行命名。分布式表是能使用 <code>Distributed</code> 表引擎，他与本地表形成一对多的映射关系，日后将通过分布式表代理操作多张本地表。</li></ul><p><small><code>Distirbuted</code> 表引擎采用读时检查，即在查询时才会抛出错误，而不会在创建表时检查。</small> </p><h5 id="定义形式"><a href="#定义形式" class="headerlink" title="定义形式"></a>定义形式</h5><p><code>Distributed</code> 表引擎的定义形式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = Distributed(cluster, database, table [, sharding_key])</span><br></pre></td></tr></table></figure><p>其中各个参数的含义如下：</p><ul><li><code>cluster</code>：集群名称，与集群配置中的自定义名称相对应。在对分布式表执行写入和查询的过程中，他会使用集群的配置信息来找到相应的 <code>host</code> 节点。</li><li><code>dataabse</code> 和 <code>table</code>：分别对应数据库和表的名称，分布式表使用这组配置映射到本地表。</li><li><code>sharding_key</code>：分片键，选填参数。在数据写入的过程中，分布式表会依据分片键的规则，将数据分布到各个 <code>host</code> 节点的本地表。</li></ul><h5 id="查询分类"><a href="#查询分类" class="headerlink" title="查询分类"></a>查询分类</h5><p><code>Distributed</code> 表引擎的查询操作分类如下：</p><ul><li>会作用于本地表的查询，对于 <code>INSERT</code> 和 <code>SELECT</code> 查询，<code>Distributed</code> 将会以分布式的方式作用于 <code>local</code> 本地表。</li><li>只会影响 <code>Distributed</code> 自身，不会作用于本地表的查询： <code>Distributed</code> 支持部分元数据操作，包括 <code>CREATE</code> 、 <code>DROP</code> 、 <code>REANME</code> 、 <code>ALTER</code>，其中 <code>ALTER</code> 并不包括分区的操作。</li><li>不支持的查询：<code>Distributed</code> 表不支持任何 <code>MUTATION</code> 类型的操作，包括 <code>ALTER DELETE</code> 和 <code>ALTER UPDATE</code>。</li></ul><h5 id="分片规则"><a href="#分片规则" class="headerlink" title="分片规则"></a>分片规则</h5><p>分片键要求返回一个整数类型的取值，包括 <code>Int</code> 类型和 <code>UInt</code> 类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Distributed(cluster, database, table, userid) # 按照用户 id 的余数划分</span><br></pre></td></tr></table></figure><p><small>如果不声明分片键，那么分布式表则只会有一个分片，意味者只能映射一张本地表。当然如果分布式表只包含一个分片，那也就失去了使用的意义。</small></p><p>关于数据如何被具体的划分，需要明确以下几个概念：</p><ul><li>分片权重<br>  在集群的配置中，还有一项分片权重（<code>weight</code>）的设置：  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">weight</span>&gt;</span>10<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">weight</span>&gt;</span>20<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br></pre></td></tr></table></figure>  分片权重会影响数据在分片中的倾斜程度，一个分片权重值越大，那么被写入的数据就会越多。</li><li><code>slot</code>（槽）<br>  <code>slot</code> 可以理解成许多个小水槽，如果把数据比作成水的话，那么数据之水会顺着这些水槽流进每个数据分片。<code>slot</code> 的数量等于所有分片的权重之和。</li><li>选择函数<br>  选择函数用于判断一行待写入的数据应该被写入到哪个分片，那整个步骤会被分为两个步骤：<ul><li>找出 <code>slot</code> 的取值，计算公式如下：   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slot = shard_value - sum_weight</span><br></pre></td></tr></table></figure>   其中 <code>shard_value</code> 是分片键的取值，<code>sum_weight</code> 是所有分片的权重之和。</li><li>基于 <code>slot</code> 值找到对应的数据分片</li></ul></li></ul><h5 id="分布式核心流程"><a href="#分布式核心流程" class="headerlink" title="分布式核心流程"></a>分布式核心流程</h5><h6 id="分布式写入流程"><a href="#分布式写入流程" class="headerlink" title="分布式写入流程"></a>分布式写入流程</h6><p>在向集群内的分片写入数据时，通常有两种思路：一种是借助外部计算系统，事先将数据均匀分片，再借由计算系统直接写入 <code>ClickHouse</code>集群的各个本地表。第二种则是通过 <code>Distributed</code> 表引擎代理写入分片数据的。</p><h6 id="将数据写入分片的核心流程"><a href="#将数据写入分片的核心流程" class="headerlink" title="将数据写入分片的核心流程"></a>将数据写入分片的核心流程</h6><p>在对 <code>Distributed</code> 表执行 <code>INSERT</code> 查询的时候，会进入数据写入分片的执行逻辑，可以分为五个步骤，整体流程如下：</p><ol><li><p>在第一个分片节点写入本地分片数据<br>在 <code>ch5</code> 节点对本地表 <code>test_shard_2_all</code> 执行 <code>INSERT</code> 查询，尝试写入 <code>10, 30, 200, 50</code> 四行数据。执行之后分布式表主要分做两件事情：其一，根据分片规则划分数据；第二，将属于当前分片的数据直接写入本地表 <code>test_shard_2_all</code>。</p></li><li><p>第一个分片建立远端连接，准备发送远端分片数据<br>将归至远端分片的数据以分区为单位，分别写入 <code>test_shard_2_all</code> 存储目录下的临时 <code>bin</code> 文件，数据文件的命名规则如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/database@host:port/[increase_num].bin</span><br></pre></td></tr></table></figure><p>由于在这个示例中只有一个远端分片 <code>ch6</code>，所以临时数据文件如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/test_shard_2_all/default@ch6.vgbhfive.cn:9000/1.bin</span><br></pre></td></tr></table></figure><p><code>10, 200, 50</code> 三行数据会被写入上述临时数据文件。接着会尝试与远端 <code>ch6</code> 分片建立连接：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Connection (ch6.vgbhfive.cn) : Connected to ClickHouse server</span><br></pre></td></tr></table></figure></li><li><p>第一个分片向远端分片发送数据<br>此时会有另一组监听任务负责监听 <code>/test_shard_2_all</code> 目录下的文件变化，这些任务负责将目录数据发送至远端分片：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_shard_2_all.Distributed.DirectoryMonitor:</span><br><span class="line">Started processing /test_shard_2_all/default@ch6.vgbhfive.cn:9000/1.bin</span><br></pre></td></tr></table></figure><p>其中，每份目录将会由独立的线程负责发送，数据在传输之前会被压缩。</p></li><li><p>第二个分片接收数据并写入本地<br><code>ch6</code> 分片节点确认建立与 <code>ch5</code> 的连接：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TCPHandlerFactory: TCP Request. Address: ch5:459127</span><br><span class="line">TCPHandler: Connected ClickHouse server</span><br></pre></td></tr></table></figure><p>在接收到 <code>ch5</code> 发送的数据后，将他们写入本地表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">executeQuery: (from ch5) INSERT INTO default.test_shard_2_local</span><br><span class="line">-- 第一个分区</span><br><span class="line">Reserving 1.00 MB on disk &#x27;default&#x27;</span><br><span class="line">Renaming temporary part tmp_insert_10_1_1_0 to 10_1_1_0.</span><br><span class="line">-- 第二个分区</span><br><span class="line">Reserving 1.00 MB on disk &#x27;default&#x27;</span><br><span class="line">Renaming temporary part tmp_insert_200_1_1_0 to 200_1_1_0.</span><br><span class="line">-- 第三个分区</span><br><span class="line">Reserving 1.00 MB on disk &#x27;default&#x27;</span><br><span class="line">Renaming temporary part tmp_insert_50_1_1_0 to 50_1_1_0.</span><br></pre></td></tr></table></figure></li><li><p>由第一个分片确认完成写入<br>最后由 <code>ch5</code> 分片确认所有的数据发送完毕：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Finished processing /test_shard_2_all/default@ch6.vgbhfive.cn:9000/1.bin</span><br></pre></td></tr></table></figure></li></ol><p>至此整个流程结束，<code>Distributed</code> 表负责所有分片的写入工作。在由 <code>Distirbuted</code> 表负责向远端分片发送数据时，有异步和同步两种写模式：如果是异步写入则在 <code>Distributed</code> 表写完本地分片之后，<code>INSERT</code> 查询就会返回成功写入的消息；如果是同步写入则在 <code>INSERT</code> 查询之后，会等待所有分片完成写入。使用何种模式由 <code>insert_distributed_sync</code> 参数控制，默认为 <code>false</code>，即异步写入；如果将其设置为 <code>true</code>，则可以进一步通过 <code>insert_distributed_timeout</code> 参数控制同步等待的超时时间。</p><h6 id="副本复制数据流程"><a href="#副本复制数据流程" class="headerlink" title="副本复制数据流程"></a>副本复制数据流程</h6><p>除了刚才的分片写入流程之外，还会触发副本数据的复制流程。数据在多个副本之间，有两种复制实现方式：一种是继续借助 <code>Distributed</code> 表引擎，由它将数据写入副本。另一种则是借助 <code>ReplicatedMergeTree</code> 表引擎实现副本数据的分发。</p><h6 id="分布式查询流程"><a href="#分布式查询流程" class="headerlink" title="分布式查询流程"></a>分布式查询流程</h6><p>与数据写入有所不同，在面向集群查询数据的时候，只能通过 <code>Distributed</code> 表引擎实现。当 <code>Distributed</code> 表接收到 <code>SELECT</code> 查询的时候，他会依次查询每个分片的数据，再合并汇总返回。</p><ol><li><p>多副本的路由规则<br>在查询数据的时候，如果集群中的一个 <code>shard</code>，拥有多个 <code>replica</code>，那么 <code>Distributed</code> 表引擎需要面临副本选择的问题。他会使用负载均衡算法从众多 <code>replica</code> 中选择一个，而具体使用何种负载均衡算法，则由 <code>load_balancing</code> 参数控制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load_balancing = random / nearest_hostname / in_order / first_or_random</span><br></pre></td></tr></table></figure><p>有四种负载均衡算法：</p><ul><li><code>random</code></li><li><code>nearest_hostname</code></li><li><code>in_order</code></li><li><code>first_or_random</code></li></ul></li><li><p>多分片查询<br>分布式查询与分布式写入类似，同样本着谁执行谁负责的原则，他会接收 <code>SELECT</code> 查询的 <code>Distributed</code> 表并负责串联起整个过程。首先他会针对分布式表的 <code>SQL</code> 语句，按照分片数量将查询拆分为若干个本地表的子查询，然后向各个分片发起查询，然后再汇总各个分片的返回结果。</p></li></ol><hr><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><hr><h3 id="个人备注"><a href="#个人备注" class="headerlink" title="个人备注"></a>个人备注</h3><p><strong>此博客内容均为作者学习所做笔记，侵删！</strong><br><strong>若转作其他用途，请注明来源！</strong></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;随着业务线数据量的突飞猛进、服务器的意外宕机，这些都是底层基础服务会遇到的问题，因此 &lt;code&gt;ClickHouse&lt;/code&gt; 就设计了&lt;strong&gt;集群&lt;/strong&gt;、&lt;strong&gt;副本&lt;/strong&gt;和&lt;strong&gt;分片&lt;/strong&gt;这三个帮手来帮忙。&lt;/p&gt;</summary>
    
    
    
    
    <category term="ClickHouse" scheme="https://blog.vgbhfive.cn/tags/ClickHouse/"/>
    
  </entry>
  
</feed>
